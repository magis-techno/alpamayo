<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</title>
<!--Generated on Wed Jan  7 09:04:59 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2511.00088v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S1" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS1" title="In 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>VLMs and VLAs in Autonomous Driving</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS2" title="In 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Reasoning VLAs in Autonomous Driving</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS3" title="In 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Post-training Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS4" title="In 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Vision-Language Datasets for Autonomous Driving</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Building a Reasoning VLA Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS1" title="In 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>VLM Backbone: Cosmos-Reason</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS2" title="In 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Domain-Specific Adaptations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS2.SSS1" title="In 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Vision Encoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS2.SSS2" title="In 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Trajectory Decoding</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS1" title="In 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Structured Chain of Causation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS2" title="In 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Data Curation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS3" title="In 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Hybrid Labeling Procedure</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS3.SSS1" title="In 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Human Labeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS3.SSS2" title="In 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Auto-Labeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS3.SSS3" title="In 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Evaluation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Training Strategy</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="In 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Action Modality Injection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS2" title="In 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Eliciting Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="In 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>RL-based Post-Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3.SSS1" title="In 5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.1 </span>Post-Training Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3.SSS2" title="In 5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.2 </span>Reward Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3.SSS3" title="In 5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.3 </span>Post-Training Data Curation for Cost-Effective Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3.SSS4" title="In 5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.4 </span>Post-Training Infrastructure</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS1" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Evaluation Protocol</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS2" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Policy Improvements from Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS3" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Improvements of Reasoning, Consistency, and Safety via RL Post-Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS4" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Public Benchmark Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS5" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Ablation: VLM Backbone Selection</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS5.SSS1" title="In 6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.1 </span>Model Size Ablation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS5.SSS2" title="In 6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.2 </span>Data Scaling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS5.SSS3" title="In 6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.3 </span>Cosmos-Reason Physical AI Capabilities</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS6" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Ablation: Action Modality Injection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS7" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.7 </span>Ablation: Efficient Vision Encoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS8" title="In 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.8 </span>On-Vehicle Road Tests</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S7" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#A1" title="In Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Contributors and Acknowledgments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#A1.SS1" title="In Appendix A Contributors and Acknowledgments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Core Contributors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#A1.SS2" title="In Appendix A Contributors and Acknowledgments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Contributors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#A1.SS3" title="In Appendix A Contributors and Acknowledgments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Acknowledgments</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" lang="en">Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">NVIDIA<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>A detailed list of contributors and acknowledgments can be found in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#A1" title="Appendix A Contributors and Acknowledgments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">A</span></a> of this paper.
  Following the release of NVIDIA Alpamayo at CES 2026 <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib65" title="">2026</a>)</cite>, Alpamayo-R1 is also referred to as Alpamayo 1.</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="1.1">End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. We introduce Alpamayo-R1 (AR1), a vision–language–action model (VLA) that integrates Chain of Causation reasoning with trajectory planning for complex driving scenarios.
Our approach features three key innovations:
(1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors;
(2) a modular VLA architecture combining Cosmos-Reason, a vision-language model pre-trained for Physical AI, with a diffusion-based trajectory decoder that generates dynamically feasible trajectories in real time;
(3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to enforce reasoning-action consistency and optimize reasoning quality.
AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. Model weights are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/nvidia/Alpamayo-R1-10B" title="">https://huggingface.co/nvidia/Alpamayo-R1-10B</a> with inference code at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVlabs/alpamayo" title="">https://github.com/NVlabs/alpamayo</a>.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1" lang="en">\abscontent</span>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The evolution of autonomous driving systems has witnessed a paradigm shift from traditional modular architectures <cite class="ltx_cite ltx_citemacro_citep">(Urmson et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib89" title="">2008</a>; Paden et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib71" title="">2016</a>; Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib21" title="">2018</a>; Lefèvre et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib42" title="">2014</a>)</cite> to end-to-end (E2E) driving frameworks <cite class="ltx_cite ltx_citemacro_citep">(Bojarski et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib6" title="">2016</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib26" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib33" title="">2023a</a>; Weng et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib96" title="">2024</a>; Wu, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib99" title="">2025</a>)</cite>, a transition increasingly embraced by industry.
In contrast to modular designs that explicitly separate perception, prediction, and planning with hand-crafted intermediate representations, E2E approaches map raw sensor inputs directly to vehicle motion through jointly trained neural networks. This unified formulation eliminates manually engineered interfaces, enabling joint optimization and data-driven policy learning at scale. Recent advances in transformer-based architectures, coupled with large-scale driving datasets have further improved the overall performance and generalization of the E2E driving paradigm.
Despite these successes, current E2E approaches remain fragile in handling long-tail and safety-critical situations, where sparse supervision and the need for high-level reasoning pose significant challenges.. Consequently, a significant gap persists between the capabilities of existing E2E models and the requirements for achieving robust Level-4 autonomy with driving-specific reasoning capabilities.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent advances in large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib1" title="">2023</a>; Comanici et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib11" title="">2025</a>)</cite> offer a promising direction to address this <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">reasoning</span> gap. LLMs have transformed artificial intelligence,
with scaling laws <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib36" title="">2020</a>)</cite> demonstrating that model performance improves predictably as compute and data increase.
Beyond training-time scaling, recent frontier models such as OpenAI’s o1 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib68" title="">2024</a>)</cite>, DeepSeek-R1 <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib14" title="">2025</a>)</cite>, and similar systems have introduced a new paradigm: <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">inference-time reasoning</em>.
Unlike traditional single-step answer generation, these models generate intermediate reasoning traces, denoted <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">chains of thought</span> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib94" title="">2022</a>)</cite>,
that mimic human problem-solving strategies.
This shift makes inference time a tunable resource: allocating more compute to deliberative reasoning often yields more accurate, robust, and verifiable decisions <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib104" title="">2023</a>)</cite>. This reasoning capability is particularly important for autonomous driving, where decision-making is inherently uncertain and safety-critical. Text-based reasoning further enables models to explore alternative outcomes in language space before committing to actions,
offering several key advantages:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">improved safety</span> through explicit counterfactual reasoning and the potential for runtime safety cross-checks and monitoring;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">better interpretability</span> via human-readable decision rationales;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">richer training signals</span> that can be used as verifiable rewards to boost long-tail performance.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">VLMs/VLAs have been widely applied to autonomous driving <cite class="ltx_cite ltx_citemacro_citep">(Mao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib57" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib58" title="">2024</a>; Hwang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib27" title="">2024</a>; Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib110" title="">2025a</a>; Renz et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib78" title="">2025</a>)</cite>,
however, most approaches either lack explicit reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wu, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib99" title="">2025</a>; Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib111" title="">2025b</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib32" title="">2025</a>)</cite>
or perform reasoning in a free-form, unstructured manner <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib53" title="">2025a</a>; Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib105" title="">2025</a>; Rowe et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib79" title="">2025</a>)</cite>.
Such approaches struggle to generalize beyond training distributions,
especially in ambiguous or compositional long-tail scenarios where strong domain priors are essential.
Moreover, treating autonomous vehicle (AV) reasoning as a pure natural language processing (NLP) problem
overlooks the rich structural knowledge inherent to driving:
lane geometry, traffic rules, map priors, agent interactions, and dynamic constraints.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We argue that effective reasoning for autonomous driving must be <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">causally grounded</em> and <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">structurally aligned</em> with the task of driving.
Instead of generating verbose, unstructured narratives,
reasoning traces should explicitly link observed scene evidence to concrete driving decisions through causal chains,
and these decisions should directly condition or control low-level trajectory generation.
The above design principle ensures that reasoning is not only an interpretability-enhancing addition,
but rather a functional component that improves both training efficiency and closed-loop driving performance,
particularly in safety-critical long-tail events.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we introduce <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Alpamayo-R1</span>, a VLA that extends the vision-action (VA) model Alpamayo-VA <cite class="ltx_cite ltx_citemacro_citep">(Wu, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib99" title="">2025</a>)</cite> with structured reasoning capabilities, bridging reasoning and action prediction for generalizable autonomous driving.
It addresses the challenges stated above through three key innovations:</p>
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We develop a structured <span class="ltx_text ltx_font_bold" id="S1.I2.i1.p1.1.1">Chain of Causation (CoC)</span> labeling framework that produces decision-grounded, causally-linked reasoning traces aligned with driving scenarios, supported by a hybrid human-in-the-loop and auto-labeling pipeline for scalable high-quality data generation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We employ a <span class="ltx_text ltx_font_bold" id="S1.I2.i2.p1.1.1">diffusion-based action-expert trajectory decoder</span> built on flow matching <cite class="ltx_cite ltx_citemacro_citep">(Lipman et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib48" title="">2023</a>; Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib18" title="">2025</a>)</cite> to efficiently generate continuous, multi-modal trajectory plans that align with the language reasoning outputs while meeting real-time inference requirements.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">We adopt a <span class="ltx_text ltx_font_bold" id="S1.I2.i3.p1.1.1">multi-stage training strategy</span> that builds upon the Cosmos-Reason VLM backbone, injects action modality for trajectory prediction, elicits reasoning via supervised fine-tuning on CoC data, and employs reinforcement learning (RL) to boost the reasoning quality, reasoning-action consistency and trajectory quality.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S1.p5.2">Through extensive open-loop and closed-loop (simulation and onboard) evaluations, we demonstrate that AR1 achieves substantial improvements over end-to-end baselines, with the largest gains in rare, safety-critical scenarios, while maintaining real-time inference performance (99ms end-to-end latency).</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In the following sections, we present the detailed components of our framework. <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2" title="2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">2</span></a> reviews related work. <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3" title="3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a> presents the model architecture and key design choices. <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4" title="4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">4</span></a> describes the proposed hybrid labeling pipeline and the resulting CoC dataset, specifically developed for reasoning-based VLA tasks in autonomous driving. <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5" title="5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a> outlines our multi-stage training strategy, where each stage progressively enhances the model’s capabilities from improving general visual-language understanding in the AV domain, to generating action modalities, to strengthening reasoning ability and output alignment. Finally, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6" title="6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6</span></a> reports extensive evaluation results, demonstrating the effectiveness of our approach in both open-loop and closed-loop environments.</p>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our work builds upon recent advances in VLMs for autonomous driving, reasoning-augmented action models, and post-training alignment techniques. We organize our review around four key areas. First, we discuss the evolution from general-purpose VLMs <cite class="ltx_cite ltx_citemacro_citep">(Hwang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib27" title="">2024</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib101" title="">2024a</a>)</cite> to action-oriented VLAs <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib110" title="">2025a</a>; Renz et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib78" title="">2025</a>)</cite> in autonomous driving (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS1" title="2.1 VLMs and VLAs in Autonomous Driving ‣ 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">2.1</span></a>), highlighting the shift toward embodied action prediction. Second, we examine reasoning VLAs (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS2" title="2.2 Reasoning VLAs in Autonomous Driving ‣ 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">2.2</span></a>) that incorporate explicit chain-of-thought processes <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib94" title="">2022</a>; Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib53" title="">2025a</a>; Rowe et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib79" title="">2025</a>)</cite> for interpretable decision-making. Third, we review post-training alignment methods (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS3" title="2.3 Post-training Alignment ‣ 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">2.3</span></a>), particularly RL from human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_citep">(Christiano et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib10" title="">2017</a>)</cite> and RL with verifiable rewards (RLVR) <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib14" title="">2025</a>)</cite>, which form the foundation of our reasoning alignment approach. Finally, we review vision-language datasets in autonomous driving (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S2.SS4" title="2.4 Vision-Language Datasets for Autonomous Driving ‣ 2 Related Work ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">2.4</span></a>), identifying key limitations in existing reasoning datasets <cite class="ltx_cite ltx_citemacro_citep">(Sima et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib81" title="">2024</a>; Nie et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib61" title="">2024</a>)</cite> that motivate our data construction methodology.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>VLMs and VLAs in Autonomous Driving</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Early work explored leveraging LLMs’ general knowledge for driving. Drive-GPT <cite class="ltx_cite ltx_citemacro_citep">(Mao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib57" title="">2023</a>)</cite>, Wolf <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib43" title="">2025a</a>)</cite>, and AgentDriver <cite class="ltx_cite ltx_citemacro_citep">(Mao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib58" title="">2024</a>)</cite> treat planning as text generation or language-based tool use, achieving competitive open-loop performance. Cube-LLM <cite class="ltx_cite ltx_citemacro_citep">(Cho et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib9" title="">2025</a>)</cite>, TOKEN <cite class="ltx_cite ltx_citemacro_citep">(Tian et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib86" title="">2024a</a>)</cite>, and EMMA <cite class="ltx_cite ltx_citemacro_citep">(Hwang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib27" title="">2024</a>)</cite> scale multimodal LLMs to multi-task scene understanding and trajectory prediction. VLM-AD <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib101" title="">2024a</a>)</cite> uses VLMs as training-time supervisors, while ReAL-AD <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib52" title="">2025</a>)</cite> models hierarchical reasoning, and DiMA <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib24" title="">2025</a>)</cite> distills VLM knowledge into efficient, LLM-free planners.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">A complementary line of work couples language with explicit action representation to create VLA models. OpenDriveVLA <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib110" title="">2025a</a>)</cite> autoregressively produces trajectory waypoints from structured vision-language tokens. AutoVLA <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib111" title="">2025b</a>)</cite> unifies reasoning and action with adaptive “think vs. act” control. IRL-VLA <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib32" title="">2025</a>)</cite> incorporates inverse RL for safety-efficiency balance, CoReVLA <cite class="ltx_cite ltx_citemacro_citep">(Fang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib22" title="">2025</a>)</cite> targets long-tail scenarios, and SimLingo <cite class="ltx_cite ltx_citemacro_citep">(Renz et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib78" title="">2025</a>)</cite> achieves state-of-the-art closed-loop results in Bench2Drive <cite class="ltx_cite ltx_citemacro_citep">(Jia et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib31" title="">2024</a>)</cite>. However, these approaches largely operate reactively without explicit reasoning, struggling to generalize beyond training distributions in ambiguous or long-horizon scenarios requiring counterfactual reasoning.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Reasoning VLAs in Autonomous Driving</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.2">Explicit reasoning methods such as chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib94" title="">2022</a>)</cite> and tree-of-thought (ToT) <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib104" title="">2023</a>)</cite> have demonstrated that intermediate reasoning traces can substantially improve performance in complex language tasks. In the domain of autonomous driving, many recent works on VLA adopt this insight by integrating structured reasoning into vision-to-action pipelines.
One line of work focuses on adaptive or efficient invocation of reasoning. For example, AdaThinkDrive <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib53" title="">2025a</a>)</cite> uses a fast-and-slow thinking mechanism, trained with RL, to invoke CoT only when needed, reducing inference overhead while maintaining performance. AutoDrive-R<sup class="ltx_sup" id="S2.SS2.p1.2.1">2</sup> <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib105" title="">2025</a>)</cite> builds an explicit CoT and self-reflection dataset (nuScenesR<sup class="ltx_sup" id="S2.SS2.p1.2.2">2</sup>-6K), leveraging GRPO <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib80" title="">2024</a>)</cite> with physics-grounded rewards to refine reasoning-augmented trajectories while ensuring physical feasibility.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Other approaches explore diverse reasoning strategies: RIV-CoT <cite class="ltx_cite ltx_citemacro_citep">(Corbière et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib12" title="">2025</a>)</cite> augments CoT with retrieval, FutureSightDrive <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib106" title="">2025</a>)</cite> performs spatio-temporal reasoning, and CoT-Drive <cite class="ltx_cite ltx_citemacro_citep">(Liao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib47" title="">2025</a>)</cite> distills reasoning into lightweight models. ReCogDrive <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib45" title="">2025b</a>)</cite>, ReasonPlan <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib50" title="">2025b</a>)</cite>, MTRDrive <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib54" title="">2025b</a>)</cite>, Drive-R1 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib46" title="">2025c</a>)</cite>, AgentThink <cite class="ltx_cite ltx_citemacro_citep">(Qian et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib74" title="">2025</a>)</cite>, DriveAgent <cite class="ltx_cite ltx_citemacro_citep">(Hou et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib25" title="">2025</a>)</cite>, and DSDrive <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib49" title="">2025a</a>)</cite> combine memory, tool invocation, multi-agent reasoning, or compression. Notably, Poutine <cite class="ltx_cite ltx_citemacro_citep">(Rowe et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib79" title="">2025</a>)</cite> topped the 2025 Waymo Vision-Based End-to-End Driving Challenge, demonstrating that reasoning-enhanced VLAs with RL finetuning excel in long-tail scenarios. This work demonstrates that reasoning serves as a functional core of driving decisions, with trade-offs among interpretability, runtime cost, and performance. However, most existing approaches rely on free-form reasoning that lacks explicit causal grounding and consistency between reasoning and actions. In contrast, our work introduces a structured CoC framework that ties reasoning to concrete driving decisions, and employs post-training RL to simultaneously optimize reasoning quality, reasoning-action consistency, and trajectory safety.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Post-training Alignment</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Generative models (e.g., LLMs and text-to-image generators) are predominantly trained with an imitative objective, such as next-token prediction. While this objective enables efficient learning from Internet-scale data, it remains only a proxy for the true training goal: optimizing for the expert’s internal reward function that motivated the demonstrated behavior. Consequently, generative models may deviate from end-user intent and, in some cases, exhibit safety-critical failures, such as producing harmful text outputs <cite class="ltx_cite ltx_citemacro_citep">(Mu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib60" title="">2024</a>)</cite>, unsafe visual generations <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib41" title="">2023b</a>)</cite>, or hazardous robot motions <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib51" title="">2023</a>)</cite>.
To mitigate such misalignment, post-training alignment—particularly through RLHF <cite class="ltx_cite ltx_citemacro_citep">(Christiano et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib10" title="">2017</a>)</cite> has emerged as a central strategy for aligning generative models with human preferences. For reasoning models specifically, DeepSeek-R1 <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib14" title="">2025</a>)</cite> employs Group Relative Policy Optimization (GRPO) <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib80" title="">2024</a>)</cite> to directly improve reasoning quality by rewarding verifiable solutions rather than intermediate token likelihood, while OpenAI o1 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib68" title="">2024</a>)</cite> similarly demonstrates that outcome-based RL substantially enhances chain-of-thought (CoT) quality. In the embodied AI domain, these alignment techniques have been extended to VLAs to generate actions that better reflect human intent across diverse embodiments, including autonomous driving <cite class="ltx_cite ltx_citemacro_citep">(Tian and Goel, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib85" title="">2025</a>)</cite> and assistive robots <cite class="ltx_cite ltx_citemacro_citep">(Tian et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib87" title="">2024b</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib108" title="">2025</a>)</cite>.
While these methods focus on improving action outcomes, our work addresses a complementary dimension: improving the reasoning process itself and ensuring that the model’s internal decision rationale remains causally consistent and contextually grounded in the context of safety-critical autonomous driving.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Vision-Language Datasets for Autonomous Driving</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Building upon the open-source nuScenes <cite class="ltx_cite ltx_citemacro_citep">(Caesar et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib7" title="">2020</a>)</cite> dataset, early work <cite class="ltx_cite ltx_citemacro_citep">(Qian et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib75" title="">2024</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib97" title="">2025a</a>; Tian et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib84" title="">2025</a>)</cite> primarily focuses on object-centric perception tasks, enabling VLMs to acquire general perception knowledge and improve object grounding in driving scenes.
Beyond nuScenes, datasets such as WOMD-reasoning <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib44" title="">2024</a>)</cite> and DriveQA <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib95" title="">2025</a>)</cite> extend vision-language annotations to large-scale motion datasets such as the Waymo Open Motion Dataset <cite class="ltx_cite ltx_citemacro_citep">(Ettinger et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib20" title="">2021</a>)</cite> and the CARLA simulator <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib16" title="">2017</a>)</cite>, focusing on describing interactions between agents, traffic rules, and right of way principles.
While these datasets serve as valuable resources for VLM pre-training, their language annotations are not explicitly linked to the ego-vehicle’s actions. As a result, they provide limited supervision for <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.1">planning-oriented</span> reasoning, a key capability required by VLAs. To bridge this gap, prior work has focused on constructing language datasets tailored for motion planning. For instance, Drama <cite class="ltx_cite ltx_citemacro_citep">(Malla et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib56" title="">2023</a>)</cite> annotates important objects that may influence the ego vehicle’s behavior. Subsequent works such as DriveAction <cite class="ltx_cite ltx_citemacro_citep">(Hao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib23" title="">2025</a>)</cite> and DriveBench <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib100" title="">2025</a>)</cite> develop comprehensive QA pairs for VLA training, emphasizing not only the identification of critical objects for planning, but also covering QA pairs for motion prediction, traffic signs, road markings, navigation following, etc.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Motivated by the development of reasoning VLAs, recent research has shifted from general VLA datasets to reasoning-oriented ones, where explicit explanations are provided for the ego vehicle’s actions.
As an early effort, BDD-X <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib38" title="">2018</a>)</cite> provides a small set of human-written explanations describing driver behaviors. With the significant advancement of LLMs/VLMs, subsequent works such as DriveGPT4 <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib102" title="">2024b</a>)</cite>, CoVLA <cite class="ltx_cite ltx_citemacro_citep">(Arai et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib2" title="">2025</a>)</cite>, and LingoQA <cite class="ltx_cite ltx_citemacro_citep">(Marcu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib59" title="">2024</a>)</cite> introduce automated or human-in-the-loop pipelines to enrich the linguistic expressiveness of reasoning data.
To capture the full reasoning process across perception, prediction and planning, DriveCoT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib93" title="">2024</a>)</cite>, Nuinstruct <cite class="ltx_cite ltx_citemacro_citep">(Ding et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib15" title="">2024</a>)</cite>, Reason2drive <cite class="ltx_cite ltx_citemacro_citep">(Nie et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib61" title="">2024</a>)</cite>, DriveLM <cite class="ltx_cite ltx_citemacro_citep">(Sima et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib81" title="">2024</a>)</cite>, DriveLMM-o1 <cite class="ltx_cite ltx_citemacro_citep">(Ishaq et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib28" title="">2025</a>)</cite>, and Senna <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib34" title="">2024</a>)</cite> develop explicit chain-based reasoning pipelines for data construction.
In parallel, Impromptu VLA <cite class="ltx_cite ltx_citemacro_citep">(Chi et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib8" title="">2025</a>)</cite> focuses on curating reasoning data in unstructured road scenarios.
However, these datasets still exhibit key limitations in enforcing the causal relationship between observations and actions in their reasoning traces. For example, free-form reasoning traces tend to use vague descriptions such as <span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.1">“the ego vehicle should be cautious and watch out for …”</span> rather than specifying actionable driving decisions. Additionally, many reasoning traces contain superficial causal factors such as <span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.2">“sunny weather”, “wide roads”, “… due to traffic rules”</span>, or introduce causal confusion by exposing the entire video clip in the labeling process and referencing future events that are not observable. These issues underscore the need for a dataset with explicit, decision-grounded, and causally linked reasoning traces, motivating our proposed CoC data pipeline.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Building a Reasoning VLA Architecture</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="383" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Overview of Alpamayo-R1 architecture. Multi-camera images and egomotion are processed by a vision encoder to produce visual tokens, which are fed into a VLM backbone (Cosmos-Reason) along with textual inputs. The model autoregressively generates chain-of-thought reasoning and discrete trajectory tokens. At inference, an action-expert decoder using flow matching converts the discrete trajectory tokens into continuous, kinematically feasible waypoints conditioned on the reasoning output.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Building an effective and reasoning-capable VLA for autonomous driving requires enabling several new capabilities beyond what general-purpose VLMs <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib1" title="">2023</a>; Comanici et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib11" title="">2025</a>)</cite> currently offer.
First, autonomous vehicles rely on <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">multi-camera, multi-timestep observations</em> to achieve 360-degree situational awareness, yet standard VLMs typically process images or video frames <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">independently</em> without explicit temporal or cross-view reasoning, leading to prohibitive token counts that preclude real-time inference when handling multi-camera inputs.
Second, driving decisions must be grounded in <em class="ltx_emph ltx_font_italic" id="S3.p1.1.3">causally structured reasoning</em> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib94" title="">2022</a>)</cite> rather than free-form narratives; the model must explain <em class="ltx_emph ltx_font_italic" id="S3.p1.1.4">why</em> a maneuver is safe and legal based on observable evidence in the history window.
Third, the model must generate <em class="ltx_emph ltx_font_italic" id="S3.p1.1.5">precise, multi-modal trajectory predictions</em> in real-time; autoregressively decoding waypoints as text tokens is inefficient and lacks the geometric and kinematic constraints essential for safe vehicle control <cite class="ltx_cite ltx_citemacro_citep">(Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib18" title="">2025</a>)</cite>.
Finally, to ensure safety in long-tail scenarios, reasoning traces must be <em class="ltx_emph ltx_font_italic" id="S3.p1.1.6">aligned with executed actions</em>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To address these challenges, we introduce <span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Alpamayo-R1 (AR1)</span>, a modular VLA architecture that extends Alpamayo-VA <cite class="ltx_cite ltx_citemacro_citep">(Wu, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib99" title="">2025</a>)</cite> to integrate reasoning with action prediction for autonomous driving.
Our design philosophy emphasizes <em class="ltx_emph ltx_font_italic" id="S3.p2.1.2">flexibility</em> and <em class="ltx_emph ltx_font_italic" id="S3.p2.1.3">modularity</em>: the architecture can adopt any off-the-shelf VLM backbone while incorporating domain-specific components for efficient vision encoding and real-time action decoding.
This modularity enables us to leverage advances in vision-language pretraining <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib66" title="">2025a</a>; Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib3" title="">2025</a>)</cite> while efficiently bridging high-level reasoning with low-level control for autonomous driving.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.5"><span class="ltx_text ltx_font_bold" id="S3.p3.5.1">Problem Formulation.</span> Given a sequence of past observations <math alttext="\bm{o}" class="ltx_Math" display="inline" id="S3.p3.1.m1" intent=":literal"><semantics><mi>𝒐</mi><annotation encoding="application/x-tex">\bm{o}</annotation></semantics></math> up to timestamp <math alttext="T" class="ltx_Math" display="inline" id="S3.p3.2.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> (omitted below), including multi-camera images <math alttext="\bm{o}_{\text{image}}" class="ltx_Math" display="inline" id="S3.p3.3.m3" intent=":literal"><semantics><msub><mi>𝒐</mi><mtext>image</mtext></msub><annotation encoding="application/x-tex">\bm{o}_{\text{image}}</annotation></semantics></math> and egomotion history <math alttext="\bm{o}_{\text{egomotion}}" class="ltx_Math" display="inline" id="S3.p3.4.m4" intent=":literal"><semantics><msub><mi>𝒐</mi><mtext>egomotion</mtext></msub><annotation encoding="application/x-tex">\bm{o}_{\text{egomotion}}</annotation></semantics></math>, AR1 is trained to perform reasoning, denoted as <span class="ltx_text ltx_font_smallcaps" id="S3.p3.5.2">Reason</span>, and to predict the future trajectory of the ego vehicle <math alttext="\bm{\tau}" class="ltx_Math" display="inline" id="S3.p3.5.m5" intent=":literal"><semantics><mi>𝝉</mi><annotation encoding="application/x-tex">\bm{\tau}</annotation></semantics></math>.
We formulate this task as a sequential prediction problem,
where the entire sequence is constructed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="[\bm{o}_{\text{image}},\bm{o}_{\text{egomotion}},\textsc{Reason},\bm{\tau}]," class="ltx_Math" display="block" id="S3.E1.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">[</mo><msub><mi>𝒐</mi><mtext>image</mtext></msub><mo>,</mo><msub><mi>𝒐</mi><mtext>egomotion</mtext></msub><mo>,</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo>,</mo><mi>𝝉</mi><mo stretchy="false">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">[\bm{o}_{\text{image}},\bm{o}_{\text{egomotion}},\textsc{Reason},\bm{\tau}],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p3.13">with each component conditioned on all previous ones.
By default, the model is trained to predict the entire 6.4s-long future trajectory sequence</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{\tau}=\{(x^{i},y^{i},\theta_{\text{yaw}}^{i})\}_{i=1}^{64}," class="ltx_Math" display="block" id="S3.E2.m1" intent=":literal"><semantics><mrow><mrow><mi>𝝉</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo>,</mo><msup><mi>y</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>θ</mi><mtext>yaw</mtext><mi>i</mi></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>64</mn></msubsup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{\tau}=\{(x^{i},y^{i},\theta_{\text{yaw}}^{i})\}_{i=1}^{64},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p3.8">where <math alttext="(x^{i},y^{i},\theta_{\text{yaw}}^{i})" class="ltx_Math" display="inline" id="S3.p3.6.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo>,</mo><msup><mi>y</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>θ</mi><mtext>yaw</mtext><mi>i</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{i},y^{i},\theta_{\text{yaw}}^{i})</annotation></semantics></math> denotes the <math alttext="i" class="ltx_Math" display="inline" id="S3.p3.7.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th future waypoint sampled at 10 Hz in the ego-vehicle’s coordinate frame at time <math alttext="T" class="ltx_Math" display="inline" id="S3.p3.8.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>.
As will be detailed in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS2.SSS2" title="3.2.2 Trajectory Decoding ‣ 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">3.2.2</span></a>, we adopt a control-based representation using unicycle dynamics with control inputs</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{a}=\{(a^{i},\kappa^{i})\}_{i=1}^{64}," class="ltx_Math" display="block" id="S3.E3.m1" intent=":literal"><semantics><mrow><mrow><mi>𝒂</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msup><mi>a</mi><mi>i</mi></msup><mo>,</mo><msup><mi>κ</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>64</mn></msubsup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{a}=\{(a^{i},\kappa^{i})\}_{i=1}^{64},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p3.12">where <math alttext="a^{i}" class="ltx_Math" display="inline" id="S3.p3.9.m1" intent=":literal"><semantics><msup><mi>a</mi><mi>i</mi></msup><annotation encoding="application/x-tex">a^{i}</annotation></semantics></math> and <math alttext="\kappa^{i}" class="ltx_Math" display="inline" id="S3.p3.10.m2" intent=":literal"><semantics><msup><mi>κ</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\kappa^{i}</annotation></semantics></math> denote the acceleration and curvature at timestep <math alttext="i" class="ltx_Math" display="inline" id="S3.p3.11.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, respectively.
Details of how <math alttext="\bm{\tau}" class="ltx_Math" display="inline" id="S3.p3.12.m4" intent=":literal"><semantics><mi>𝝉</mi><annotation encoding="application/x-tex">\bm{\tau}</annotation></semantics></math> is encoded and decoded are provided in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS2.SSS2" title="3.2.2 Trajectory Decoding ‣ 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">3.2.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="5.1 Action Modality Injection ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Overall Architecture.</span> <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.F1" title="In 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">1</span></a> presents the end-to-end architecture of AR1. The system processes multi-camera, multi-timestep observations as visual inputs, optionally augmented with textual inputs such as user commands and high-level navigation instructions. All inputs, including historical ego-motion data, are tokenized into a unified sequence of multimodal tokens following a predefined order. These tokens are then processed by the Cosmos-Reason <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib66" title="">2025a</a>)</cite> backbone, which produces output tokens representing reasoning traces, meta-actions, and predicted future trajectories. The model is trained in multiple stages, combining supervised fine-tuning (SFT) and RL, as will be described in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5" title="5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>VLM Backbone: Cosmos-Reason</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We adopt Cosmos-Reason <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib66" title="">2025a</a>)</cite> as the VLM backbone for Alpamayo-R1. Cosmos-Reason is a VLM specifically designed for Physical AI applications, post-trained on 3.7M Visual Question Answering (VQA) samples to develop physical common sense and embodied reasoning capabilities. The model incorporates 24.7K curated video VQA samples focused on driving scenarios, including scene descriptions, driving difficulty annotations, and reasoning traces distilled from DeepSeek-R1 <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib14" title="">2025</a>)</cite> to predict the next action.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Domain-Specific Supervised Fine-Tuning.</span> To further enhance Cosmos-Reason for autonomous driving deployment, we curate supplementary datasets spanning multiple Physical AI domains, including autonomous driving, robotics, healthcare, smart cities, manufacturing, retail, and logistics. This broad Physical AI pre-training enables the model to develop general physical common sense and embodied reasoning capabilities that transfer to driving scenarios. For autonomous driving specifically, we augment the training data with 100K new samples that include annotations for critical objects in the environment and reasoning for the next action.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Driving-Focused Data Curation.</span> We develop complementary labeling approaches to balance quality and scale for autonomous driving:</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Human-labeled data</span> includes comprehensive annotations covering the operational design domain (weather, lighting, road conditions), traffic regulations (traffic lights, signs), ego behaviors (interactive and non-interactive meta-actions), critical objects influencing ego behavior, and causal reasoning behind observed maneuvers. These labels improve the model’s understanding and reasoning in complex driving scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">Automatically labeled data</span> focuses on ego behavior reasoning and prediction, generated by prompting a teacher VLM (e.g., Qwen3-VL <cite class="ltx_cite ltx_citemacro_citep">(Qwen Team, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib77" title="">2025</a>)</cite>) with driving-specific priors that encode longitudinal, lateral, and lane-related meta-actions along with velocity information. This scalable approach strengthens the model’s predictive reasoning capabilities.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Domain-Specific Adaptations</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">While Cosmos-Reason provides a strong foundation, two critical gaps remain for real-world autonomous driving deployment: efficient vision encoding for multi-camera, multi-timestep inputs and precise trajectory decoding for real-time control. The following subsections detail our domain-specific components that address these challenges.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Vision Encoding</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The main role of vision encoders within VLMs is to convert input images into streams of tokens for later processing by the LLM backbone. However, as VLAs target onboard deployment, a critical requirement of their vision encoders is to produce as few tokens as possible while preserving relevant semantic information from the environment. To achieve this, there have been a variety of vision tokenization approaches proposed that primarily differ in how much information is encoded per inference step (i.e., how many images are compressed into how many tokens), as well as their associated architectural choices.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">In this section, we discuss the different vision encoders that AR1 can use as well as their tradeoffs, in addition to avenues for further token count compression towards enabling real-time onboard inference with larger backbone sizes.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p3.1.1">Single-Image Tokenization.</span>
Many vision tokenizers primarily focus on representing single images and either employ autoencoding architectures <cite class="ltx_cite ltx_citemacro_citep">(Sohn et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib82" title="">2015</a>; van den Oord et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib90" title="">2017</a>; Esser et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib19" title="">2021</a>)</cite> or directly encode patches of pixels <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib17" title="">2020</a>)</cite>. VLMs adopt the latter primarily and employ Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib17" title="">2020</a>)</cite> to partition images into patches that are encoded to form a 1D token sequence. We denote this paradigm as <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p3.1.2">single-image tokenization</span>, where a model encodes each input frame into a set of tokens.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.5">AR1’s default tokenizer (and the one used for all subsequent experiments) leverages this paradigm, employing the base VLM’s vision encoder (e.g., <cite class="ltx_cite ltx_citemacro_citet">Zhai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib107" title="">2023</a>); Tschannen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib88" title="">2025</a>)</cite>) to encode a <math alttext="W\times H" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.1.m1" intent=":literal"><semantics><mrow><mi>W</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">W\times H</annotation></semantics></math> px input image into patch features <math alttext="\mathbf{f}\in\mathbb{R}^{W/14\times H/14\times D}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.2.m2" intent=":literal"><semantics><mrow><mi>𝐟</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mrow><mrow><mi>W</mi><mo>/</mo><mn>14</mn></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>H</mi></mrow><mo>/</mo><mn>14</mn></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{f}\in\mathbb{R}^{W/14\times H/14\times D}</annotation></semantics></math> which are then <math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p4.3.m3" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math> bilinearly downsampled to <math alttext="\mathbf{f}^{\prime}\in\mathbb{R}^{W/28\times H/28\times D}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.4.m4" intent=":literal"><semantics><mrow><msup><mi>𝐟</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mrow><mrow><mi>W</mi><mo>/</mo><mn>28</mn></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>H</mi></mrow><mo>/</mo><mn>28</mn></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{f}^{\prime}\in\mathbb{R}^{W/28\times H/28\times D}</annotation></semantics></math> features per image. As an example, with <math alttext="W=448,H=280" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.5.m5" intent=":literal"><semantics><mrow><mrow><mi>W</mi><mo>=</mo><mn>448</mn></mrow><mo>,</mo><mrow><mi>H</mi><mo>=</mo><mn>280</mn></mrow></mrow><annotation encoding="application/x-tex">W=448,H=280</annotation></semantics></math> this process produces 160 tokens per image.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p5.1.1">Multi-Camera Tokenization.</span>
While single-image tokenization is simple to implement, it produces token counts that scale linearly with image resolution and the number of cameras <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib92" title="">2025</a>)</cite>.
To obtain a 360-degree view of their surroundings, AVs often use 6 to 10 cameras, the patch-based tokenization of which would yield thousands of tokens per timestep and preclude real-time inference. Accordingly, AR1 also supports the use of a new line of efficient multi-camera tokenizers that encode images from multiple cameras into an intermediate representation before tokenizing that representation.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p6">
<p class="ltx_p" id="S3.SS2.SSS1.p6.2">Specifically, AR1 can additionally use the efficient multi-camera tokenizer proposed in <cite class="ltx_cite ltx_citemacro_citet">Ivanovic et al. (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib29" title="">2025</a>)</cite>, which leverages triplanes as a 3D inductive bias, to simultaneously represent multiple camera images in an efficient manner.
Crucially, since the triplane sizes are fixed, the input number of cameras and their resolution are decoupled from the resulting number of tokens.
Formally, for a triplane with grid sizes <math alttext="S_{x},S_{y},S_{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.1.m1" intent=":literal"><semantics><mrow><msub><mi>S</mi><mi>x</mi></msub><mo>,</mo><msub><mi>S</mi><mi>y</mi></msub><mo>,</mo><msub><mi>S</mi><mi>z</mi></msub></mrow><annotation encoding="application/x-tex">S_{x},S_{y},S_{z}</annotation></semantics></math> and downstream patchification values of <math alttext="p_{x},p_{y},p_{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.2.m2" intent=":literal"><semantics><mrow><msub><mi>p</mi><mi>x</mi></msub><mo>,</mo><msub><mi>p</mi><mi>y</mi></msub><mo>,</mo><msub><mi>p</mi><mi>z</mi></msub></mrow><annotation encoding="application/x-tex">p_{x},p_{y},p_{z}</annotation></semantics></math>, the number of tokens produced by the tokenizer is</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\underbrace{\left(\frac{S_{x}-p_{x}}{p_{x}}+1\right)\left(\frac{S_{y}-p_{y}}{p_{y}}+1\right)}_{\text{\# of patches in the }xy\text{ plane}}+\underbrace{\left(\frac{S_{x}-p_{x}}{p_{x}}+1\right)\left(\frac{S_{z}-p_{z}}{p_{z}}+1\right)}_{\text{\# of patches in the }xz\text{ plane}}+\underbrace{\left(\frac{S_{y}-p_{y}}{p_{y}}+1\right)\left(\frac{S_{z}-p_{z}}{p_{z}}+1\right)}_{\text{\# of patches in the }yz\text{ plane}}." class="ltx_Math" display="block" id="S3.E4.m1" intent=":literal"><semantics><mrow><mrow><munder><munder accentunder="true"><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><msub><mi>S</mi><mi>x</mi></msub><mo>−</mo><msub><mi>p</mi><mi>x</mi></msub></mrow><msub><mi>p</mi><mi>x</mi></msub></mfrac><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><mrow><msub><mi>S</mi><mi>y</mi></msub><mo>−</mo><msub><mi>p</mi><mi>y</mi></msub></mrow><msub><mi>p</mi><mi>y</mi></msub></mfrac><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow><mo stretchy="true">⏟</mo></munder><mrow><mtext># of patches in the </mtext><mo lspace="0em" rspace="0em">​</mo><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mtext> plane</mtext></mrow></munder><mo>+</mo><munder><munder accentunder="true"><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><msub><mi>S</mi><mi>x</mi></msub><mo>−</mo><msub><mi>p</mi><mi>x</mi></msub></mrow><msub><mi>p</mi><mi>x</mi></msub></mfrac><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><mrow><msub><mi>S</mi><mi>z</mi></msub><mo>−</mo><msub><mi>p</mi><mi>z</mi></msub></mrow><msub><mi>p</mi><mi>z</mi></msub></mfrac><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow><mo stretchy="true">⏟</mo></munder><mrow><mtext># of patches in the </mtext><mo lspace="0em" rspace="0em">​</mo><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi><mo lspace="0em" rspace="0em">​</mo><mtext> plane</mtext></mrow></munder><mo>+</mo><munder><munder accentunder="true"><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><msub><mi>S</mi><mi>y</mi></msub><mo>−</mo><msub><mi>p</mi><mi>y</mi></msub></mrow><msub><mi>p</mi><mi>y</mi></msub></mfrac><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><mrow><msub><mi>S</mi><mi>z</mi></msub><mo>−</mo><msub><mi>p</mi><mi>z</mi></msub></mrow><msub><mi>p</mi><mi>z</mi></msub></mfrac><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow><mo stretchy="true">⏟</mo></munder><mrow><mtext># of patches in the </mtext><mo lspace="0em" rspace="0em">​</mo><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi><mo lspace="0em" rspace="0em">​</mo><mtext> plane</mtext></mrow></munder></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\underbrace{\left(\frac{S_{x}-p_{x}}{p_{x}}+1\right)\left(\frac{S_{y}-p_{y}}{p_{y}}+1\right)}_{\text{\# of patches in the }xy\text{ plane}}+\underbrace{\left(\frac{S_{x}-p_{x}}{p_{x}}+1\right)\left(\frac{S_{z}-p_{z}}{p_{z}}+1\right)}_{\text{\# of patches in the }xz\text{ plane}}+\underbrace{\left(\frac{S_{y}-p_{y}}{p_{y}}+1\right)\left(\frac{S_{z}-p_{z}}{p_{z}}+1\right)}_{\text{\# of patches in the }yz\text{ plane}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p6.6">As an example, for <math alttext="S_{x}=S_{y}=96,S_{z}=48" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>S</mi><mi>x</mi></msub><mo>=</mo><msub><mi>S</mi><mi>y</mi></msub><mo>=</mo><mn>96</mn></mrow><mo>,</mo><mrow><msub><mi>S</mi><mi>z</mi></msub><mo>=</mo><mn>48</mn></mrow></mrow><annotation encoding="application/x-tex">S_{x}=S_{y}=96,S_{z}=48</annotation></semantics></math>, and <math alttext="p_{x}=p_{y}=p_{z}=8" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.4.m2" intent=":literal"><semantics><mrow><msub><mi>p</mi><mi>x</mi></msub><mo>=</mo><msub><mi>p</mi><mi>y</mi></msub><mo>=</mo><msub><mi>p</mi><mi>z</mi></msub><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">p_{x}=p_{y}=p_{z}=8</annotation></semantics></math>, only 288 tokens are needed to represent one timestep of observations, irrespective of the number of cameras or their resolution. For a 7-camera vehicle setup, this equates to approximately <math alttext="41.1" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.5.m3" intent=":literal"><semantics><mn>41.1</mn><annotation encoding="application/x-tex">41.1</annotation></semantics></math> tokens per image (<math alttext="3.9\times" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p6.6.m4" intent=":literal"><semantics><mrow><mn>3.9</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">3.9\times</annotation></semantics></math> less than single-image tokenization). Further, as we will show in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS7" title="6.7 Ablation: Efficient Vision Encoding ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.7</span></a>, this efficiency is achieved without major compromises to end-to-end driving metrics.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p7">
<p class="ltx_p" id="S3.SS2.SSS1.p7.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p7.2.1">Multi-Camera Video Tokenization.</span> While the above already yields significant reductions in the number of tokens required to represent sensor observations, there are still two fundamental areas where additional efficiency can be achieved:</p>
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">accounting for temporal information (e.g., there can be redundancy in information across frames);</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">removing the potential performance ceiling that comes with using a structured feature representation.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS2.SSS1.p7.1">Accordingly, AR1 also supports using multi-camera video tokenizers that directly encode entire sequences of camera observations from multiple timesteps. One example is Flex <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib103" title="">2025</a>)</cite>, which compresses a set of image tokens from multiple cameras and timesteps via full self-attention layers and a fixed set of query vectors, providing an explicit mechanism to control the magnitude of the information bottleneck. As will be shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS7" title="6.7 Ablation: Efficient Vision Encoding ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.7</span></a>, this approach can achieve an up to <math alttext="20\times" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p7.1.m1" intent=":literal"><semantics><mrow><mn>20</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">20\times</annotation></semantics></math> token compression rate (compared to single-image tokenization) while maintaining or even improving downstream driving metrics.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p8">
<p class="ltx_p" id="S3.SS2.SSS1.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p8.1.1">Additional Avenues for Token Compression.</span>
Beyond the tokenization strategies described above, several complementary approaches can further reduce token counts. Post-training token pruning techniques, exemplified by SparseVILA <cite class="ltx_cite ltx_citemacro_citep">(Khaki et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib37" title="">2025</a>)</cite>, dynamically identify and remove redundant tokens during inference without retraining, offering a practical path to reduce computational costs on models already trained.
These methods represent promising directions for further scaling AR1 to even larger backbones while maintaining real-time performance constraints.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Trajectory Decoding</h4>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="328" id="S3.F2.g1" src="x2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.5.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.6.2" style="font-size:90%;">Examples of reasoning traces exhibiting common issues in existing datasets <cite class="ltx_cite ltx_citemacro_citep">(Malla et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib56" title="">2023</a>; Chi et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib8" title="">2025</a>; Arai et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib2" title="">2025</a>)</cite>. Text highlighted in <span class="ltx_text" id="S3.F2.6.2.1" style="--ltx-fg-color:#F5D23C;">yellow</span> indicates vague behavior descriptions that fail to specify concrete driving decisions correlated with the trajectories. Text highlighted in <span class="ltx_text" id="S3.F2.6.2.2" style="--ltx-fg-color:#2864DC;">blue</span> denotes superficial reasoning, such as contextual observations that do not directly inform the ego vehicle’s decision. <span class="ltx_text" id="S3.F2.6.2.3" style="--ltx-fg-color:#DC3C32;">Red</span> highlights indicate incorrect or causally inconsistent reasoning that contradicts the actual behavior of the ego vehicle.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To extend the capability of a VLM to operate effectively in the physical world, it is essential to incorporate <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">physical actions</span>, corresponding to future driving trajectories in the autonomous driving context, into the training of the VLA. However, embodiment introduces unique challenges in action decoding:</p>
<ol class="ltx_enumerate" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">the action representation must be accurate, preserving both fidelity and multi-modality;</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">the decoding process must be fast enough to support real-time inference;</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1">the decoding mechanism should integrate seamlessly into the VLA training pipeline.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.3">Initially, we found that training the model in raw position (i.e., <math alttext="x,y" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.1.m1" intent=":literal"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x,y</annotation></semantics></math>) waypoint space is susceptible to sensor noise, which often degrades model convergence.
Moreover, the downstream low-level vehicle controllers typically smooth trajectory outputs to ensure consistent and stable execution on-vehicle.
Thus, instead of directly learning <math alttext="\bm{\tau}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.2.m2" intent=":literal"><semantics><mi>𝝉</mi><annotation encoding="application/x-tex">\bm{\tau}</annotation></semantics></math> in the raw position waypoint space, we adopt an action representation governed by unicycle dynamics that leads to better closed-loop performance.
Specifically, we employ the following unicycle dynamics with control input <math alttext="\bm{a}=\{(a^{i},\kappa^{i})\}_{i=1}^{64}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.3.m3" intent=":literal"><semantics><mrow><mi>𝒂</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msup><mi>a</mi><mi>i</mi></msup><mo>,</mo><msup><mi>κ</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>64</mn></msubsup></mrow><annotation encoding="application/x-tex">\bm{a}=\{(a^{i},\kappa^{i})\}_{i=1}^{64}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Lynch and Park, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib55" title="">2017</a>)</cite> and apply Euler discretization:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{x}^{i+1}=\begin{pmatrix}x^{i+1}\\
y^{i+1}\\
\theta^{i+1}\\
v^{i+1}\\
\end{pmatrix}=\begin{pmatrix}x^{i}+\dfrac{\Delta T}{2}\!\left(v^{i}\cos\theta^{i}+v^{i+1}\cos\theta^{i+1}\right)\\[6.0pt]
y^{i}+\dfrac{\Delta T}{2}\!\left(v^{i}\sin\theta^{i}+v^{i+1}\sin\theta^{i+1}\right)\\[6.0pt]
\theta^{i}+\Delta T\,\kappa^{i}v^{i}+\dfrac{\Delta T^{2}}{2}\,\kappa^{i}a^{i}\\[6.0pt]
v^{i}+\Delta T\,a^{i}\end{pmatrix}," class="ltx_Math" display="block" id="S3.E5.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>𝐱</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mo>(</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></mtd></mtr><mtr><mtd><msup><mi>y</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></mtd></mtr><mtr><mtd><msup><mi>θ</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></mtd></mtr><mtr><mtd><msup><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></mtd></mtr></mtable><mo>)</mo></mrow><mo>=</mo><mrow><mo>(</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><msup><mi>x</mi><mi>i</mi></msup><mo>+</mo><mrow><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>v</mi><mi>i</mi></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>cos</mi><mo lspace="0.167em">⁡</mo><msup><mi>θ</mi><mi>i</mi></msup></mrow></mrow><mo>+</mo><mrow><msup><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>cos</mi><mo lspace="0.167em">⁡</mo><msup><mi>θ</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd><mrow><msup><mi>y</mi><mi>i</mi></msup><mo>+</mo><mrow><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>v</mi><mi>i</mi></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>sin</mi><mo lspace="0.167em">⁡</mo><msup><mi>θ</mi><mi>i</mi></msup></mrow></mrow><mo>+</mo><mrow><msup><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>sin</mi><mo lspace="0.167em">⁡</mo><msup><mi>θ</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd><mrow><msup><mi>θ</mi><mi>i</mi></msup><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi><mo lspace="0.170em" rspace="0em">​</mo><msup><mi>κ</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>v</mi><mi>i</mi></msup></mrow><mo>+</mo><mrow><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>T</mi><mn>2</mn></msup></mrow><mn>2</mn></mfrac><mo lspace="0.170em" rspace="0em">​</mo><msup><mi>κ</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>a</mi><mi>i</mi></msup></mrow></mrow></mtd></mtr><mtr><mtd><mrow><msup><mi>v</mi><mi>i</mi></msup><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi><mo lspace="0.170em" rspace="0em">​</mo><msup><mi>a</mi><mi>i</mi></msup></mrow></mrow></mtd></mtr></mtable><mo>)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{x}^{i+1}=\begin{pmatrix}x^{i+1}\\
y^{i+1}\\
\theta^{i+1}\\
v^{i+1}\\
\end{pmatrix}=\begin{pmatrix}x^{i}+\dfrac{\Delta T}{2}\!\left(v^{i}\cos\theta^{i}+v^{i+1}\cos\theta^{i+1}\right)\\[6.0pt]
y^{i}+\dfrac{\Delta T}{2}\!\left(v^{i}\sin\theta^{i}+v^{i+1}\sin\theta^{i+1}\right)\\[6.0pt]
\theta^{i}+\Delta T\,\kappa^{i}v^{i}+\dfrac{\Delta T^{2}}{2}\,\kappa^{i}a^{i}\\[6.0pt]
v^{i}+\Delta T\,a^{i}\end{pmatrix},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.p2.14">where <math alttext="\Delta T=0.1\text{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.4.m1" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow><mo>=</mo><mrow><mn>0.1</mn><mo lspace="0em" rspace="0em">​</mo><mtext>s</mtext></mrow></mrow><annotation encoding="application/x-tex">\Delta T=0.1\text{s}</annotation></semantics></math> in our setup, <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.5.m2" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.6.m3" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> denote positional waypoints in the bird’s-eye-view (BEV) plane, <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.7.m4" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> represents the yaw angle, <math alttext="v" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.8.m5" intent=":literal"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> the velocity, <math alttext="\kappa" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.9.m6" intent=":literal"><semantics><mi>κ</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math> the curvature, and <math alttext="a" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.10.m7" intent=":literal"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> the acceleration.
During training, the ground-truth control sequence <math alttext="\bm{a}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.11.m8" intent=":literal"><semantics><mi>𝒂</mi><annotation encoding="application/x-tex">\bm{a}</annotation></semantics></math> is derived from <math alttext="\bm{\tau}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.12.m9" intent=":literal"><semantics><mi>𝝉</mi><annotation encoding="application/x-tex">\bm{\tau}</annotation></semantics></math> through a least-squares formulation with Tikhonov regularization to attenuate high-frequency noise.
The model is trained to predict the control sequence <math alttext="\bm{a}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.13.m10" intent=":literal"><semantics><mi>𝒂</mi><annotation encoding="application/x-tex">\bm{a}</annotation></semantics></math> and, during inference, we apply <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.E5" title="In 3.2.2 Trajectory Decoding ‣ 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Eq.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a> to map it to <math alttext="\bm{\tau}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.14.m11" intent=":literal"><semantics><mi>𝝉</mi><annotation encoding="application/x-tex">\bm{\tau}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.4">Furthermore, to enable AR1 to understand and generate trajectories, we encode <math alttext="\bm{\tau}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.1.m1" intent=":literal"><semantics><mi>𝝉</mi><annotation encoding="application/x-tex">\bm{\tau}</annotation></semantics></math> either as discrete tokens or continuous embeddings. In the discrete representation, we uniformly quantize each continuous value in <math alttext="\bm{a}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.2.m2" intent=":literal"><semantics><mi>𝒂</mi><annotation encoding="application/x-tex">\bm{a}</annotation></semantics></math> within a predefined range into equally spaced bins and represent the resulting indices as special tokens. For the continuous representation, we map <math alttext="\bm{a}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.3.m3" intent=":literal"><semantics><mi>𝒂</mi><annotation encoding="application/x-tex">\bm{a}</annotation></semantics></math> into AR1’s embedding space using sinusoidal positional encoding followed by an MLP projection.
Specifically, we adopt a strategy inspired by <math alttext="\pi_{0.5}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.4.m4" intent=":literal"><semantics><msub><mi>π</mi><mn>0.5</mn></msub><annotation encoding="application/x-tex">\pi_{0.5}</annotation></semantics></math>-KI <cite class="ltx_cite ltx_citemacro_citep">(Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib18" title="">2025</a>)</cite>, combining <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p3.4.1">discrete</span> trajectory tokens learned within the VLM with an action-expert that decodes the same trajectories into <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p3.4.2">continuous</span> representations using a flow matching framework <cite class="ltx_cite ltx_citemacro_citep">(Lipman et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib48" title="">2023</a>)</cite>. This framework facilitates streamlined VLM training, accelerates trajectory decoding, and achieves better closed-loop performance.
Training details of the action modality injection are provided in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="5.1 Action Modality Injection ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p4.1.1">Summary.</span>
This section further detailed the two principal design dimensions (vision encoding and action decoding) through which VLMs can be systematically adapted into AV policy VLAs. In subsequent sections, we detail the construction of the data pipeline and the formulation of the training strategy, which together endow the model with enhanced reasoning and alignment capabilities, thereby improving its robustness in handling long-tail events.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="421" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.5.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.6.2" style="font-size:90%;">Overview of the proposed structured CoC labeling pipeline, composed of five steps: (1) <span class="ltx_text ltx_font_bold" id="S3.F3.6.2.1">Clip Selection</span>, where clips containing explicit driving decisions are selected, filtering out low-signal clips that offer limited causal information; (2) <span class="ltx_text ltx_font_bold" id="S3.F3.6.2.2">Keyframe Labeling</span>, where the decision-making moment within each video clip is identified, minimizing potential causal confusion; (3-5) <span class="ltx_text ltx_font_bold" id="S3.F3.6.2.3">Structured CoC Labeling</span>, to construct the final CoC and further mitigate causal confusion, we first annotate critical components from the observation while avoiding referring to causal factors in future frames, and then label the corresponding driving decision. We then compose a reasoning trace from driving decisions and causal factors in natural language.
</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To enable reasoning VLA models to explain the causes of driving actions and to improve their trajectory-level performance, reasoning data must be closely correlated with the ego trajectory. However, existing CoT reasoning datasets in the AV community often exhibit several limitations, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.F2" title="In 3.2.2 Trajectory Decoding ‣ 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">2</span></a>:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">Vague behavior descriptions</span>: free-form CoT annotations may fail to specify concrete driving actions or may choose words that weakly correlate with ego trajectories;</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">Superficial reasoning</span>: some reasoning traces primarily describe contextual observations or hypothetical factors that lack a direct causal link to the ego vehicle’s behavior, providing limited benefit for improving post-training driving performance;</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i3.p1.1.1">Causal confusion</span>: reasoning traces may include causal factors that occur in future time windows, which are not observable to the model during training. This arises because the labeling process often exposes the entire video without distinguishing between historical and future segments.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">To address these gaps, we introduce a labeling framework that enforces an explicit causal structure in the reasoning traces. We first define a comprehensive set of high-level driving decisions that directly correspond to low-level ego trajectories. Each reasoning trace is associated with an explicit driving decision and includes only the causal factors that motivate that driving decision. By carefully selecting keyframes to split historical and future video segments, we ensure that all causal factors originate within the observable history window, thereby preventing causal confusion. This design ensures that every reasoning trace is both <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">decision-grounded</span> and <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">causally linked</span>, capturing concise and interpretable cause–and–effect relationships rather than verbose descriptive narratives. The resulting dataset, termed the <span class="ltx_text ltx_font_bold" id="S4.p2.1.3">Chain of Causation (CoC) dataset</span>, provides clear supervision for learning decision causality, enabling reasoning VLAs to efficiently reason about the causes of specific driving actions during onboard inference. An overview of our labeling pipeline is shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.F3" title="In 3.2.2 Trajectory Decoding ‣ 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Structured Chain of Causation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To facilitate efficient annotation, our labeling framework decomposes each data sample into three structured components: the driving decision, the causal factors (critical components), and the composed CoC trace. Consequently, each data instance constitutes a structured CoC sample encompassing these three components.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Closed-set driving decisions (longitudinal and lateral) used to anchor reasoning traces to explicit control intent. Annotators select at most one decision per channel (or <em class="ltx_emph ltx_font_italic" id="S4.T1.15.1">None</em>), ensuring decision-grounded supervision.
Definitions emphasize operational intent and disambiguate visually or behaviorally similar maneuvers (e.g., <em class="ltx_emph ltx_font_italic" id="S4.T1.16.2">Lead obstacle following</em> vs. <em class="ltx_emph ltx_font_italic" id="S4.T1.17.3">Yield</em>, <em class="ltx_emph ltx_font_italic" id="S4.T1.18.4">Lane change</em> vs. <em class="ltx_emph ltx_font_italic" id="S4.T1.19.5">Merge / Split</em>). Each selected decision must be causally supported by evidence from the observed history window. LC denotes lane change.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1" style="font-size:90%;">Type</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.1" style="font-size:90%;">Driving decision</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T1.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.3.1">
<span class="ltx_p" id="S4.T1.1.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.3.1.1.1" style="font-size:90%;">Definition</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.3.1"><span class="ltx_text" id="S4.T1.1.3.1.1" style="font-size:90%;">Longitudinal</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.3.2"><span class="ltx_text" id="S4.T1.1.3.2.1" style="font-size:90%;">Set speed tracking</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T1.1.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.3.1">
<span class="ltx_p" id="S4.T1.1.3.3.1.1"><span class="ltx_text" id="S4.T1.1.3.3.1.1.1" style="font-size:90%;">Maintain or reach a target speed when unconstrained; excludes follow/yield/stop logic.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4">
<td class="ltx_td" id="S4.T1.1.4.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.4.2"><span class="ltx_text" id="S4.T1.1.4.2.1" style="font-size:90%;">Lead obstacle following</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.1">
<span class="ltx_p" id="S4.T1.1.4.3.1.1"><span class="ltx_text" id="S4.T1.1.4.3.1.1.1" style="font-size:90%;">Maintain a safe time gap to the lead entity (closest in-path entity moves in the same traffic flow); excludes geometry-based slowing, gap-matching, and yielding to non-lead entity.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td" id="S4.T1.1.5.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.2"><span class="ltx_text" id="S4.T1.1.5.2.1" style="font-size:90%;">Speed adaptation (road events)</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.3.1">
<span class="ltx_p" id="S4.T1.1.5.3.1.1"><span class="ltx_text" id="S4.T1.1.5.3.1.1.1" style="font-size:90%;">Adjust speed for roadway features (curves, grades, bumps, ramps, roundabouts, turns); independent of a lead.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6">
<td class="ltx_td" id="S4.T1.1.6.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.2"><span class="ltx_text" id="S4.T1.1.6.2.1" style="font-size:90%;">Gap-searching (for LC/merge/zipper)</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.6.3.1">
<span class="ltx_p" id="S4.T1.1.6.3.1.1"><span class="ltx_text" id="S4.T1.1.6.3.1.1.1" style="font-size:90%;">Adjust speed to match the target stream or create a usable gap to support a planned lateral maneuver.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7">
<td class="ltx_td" id="S4.T1.1.7.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.2"><span class="ltx_text" id="S4.T1.1.7.2.1" style="font-size:90%;">Acceleration for passing/overtaking</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.7.3.1">
<span class="ltx_p" id="S4.T1.1.7.3.1.1"><span class="ltx_text" id="S4.T1.1.7.3.1.1.1" style="font-size:90%;">Increase speed to pass a slower lead with an associated lateral plan.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8">
<td class="ltx_td" id="S4.T1.1.8.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.8.2"><span class="ltx_text" id="S4.T1.1.8.2.1" style="font-size:90%;">Yield (agent right-of-way)</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.8.3.1">
<span class="ltx_p" id="S4.T1.1.8.3.1.1"><span class="ltx_text" id="S4.T1.1.8.3.1.1.1" style="font-size:90%;">Slow/stop to concede priority to specific agents (pedestrians, cross-traffic, emergency vehicles, cut-ins).</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9">
<td class="ltx_td" id="S4.T1.1.9.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.9.2"><span class="ltx_text" id="S4.T1.1.9.2.1" style="font-size:90%;">Stop for static constraints</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.9.3.1">
<span class="ltx_p" id="S4.T1.1.9.3.1.1"><span class="ltx_text" id="S4.T1.1.9.3.1.1.1" style="font-size:90%;">Decelerate to—and hold at—control points (stop/yield lines, red light, school bus/rail rules); Sometimes a yield is necessary even when owning the right-of-way, to avoid a collision.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.10.1"><span class="ltx_text" id="S4.T1.1.10.1.1" style="font-size:90%;">Lateral</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.10.2"><span class="ltx_text" id="S4.T1.1.10.2.1" style="font-size:90%;">Lane keeping &amp; centering</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T1.1.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.10.3.1">
<span class="ltx_p" id="S4.T1.1.10.3.1.1"><span class="ltx_text" id="S4.T1.1.10.3.1.1.1" style="font-size:90%;">Maintain position within lane boundaries; minor in-lane offsets allowed; never cross lane lines.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td" id="S4.T1.1.1.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.3"><span class="ltx_text" id="S4.T1.1.1.3.1" style="font-size:90%;">Merge / Split (facility change)</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1"><span class="ltx_text" id="S4.T1.1.1.1.1.1.1" style="font-size:90%;">Transition between facilities (e.g., on-ramp </span><math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.m1" intent=":literal"><semantics><mo mathsize="0.900em" stretchy="false">↔</mo><annotation encoding="application/x-tex">\leftrightarrow</annotation></semantics></math><span class="ltx_text" id="S4.T1.1.1.1.1.1.2" style="font-size:90%;"> mainline, weave segments); not a same-road lane change.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11">
<td class="ltx_td" id="S4.T1.1.11.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.11.2"><span class="ltx_text" id="S4.T1.1.11.2.1" style="font-size:90%;">Out-of-lane nudge (straddle avoidance)</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.11.3.1">
<span class="ltx_p" id="S4.T1.1.11.3.1.1"><span class="ltx_text" id="S4.T1.1.11.3.1.1.1" style="font-size:90%;">Brief, intentional lane-line crossing to increase clearance around a blockage/hazard; return to original lane; specify left/right.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12">
<td class="ltx_td" id="S4.T1.1.12.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.12.2"><span class="ltx_text" id="S4.T1.1.12.2.1" style="font-size:90%;">In-lane nudge</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.12.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.12.3.1">
<span class="ltx_p" id="S4.T1.1.12.3.1.1"><span class="ltx_text" id="S4.T1.1.12.3.1.1.1" style="font-size:90%;">Temporary offset within the lane (no line crossing) to increase clearance around a blockage/hazard; specify left/right.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13">
<td class="ltx_td" id="S4.T1.1.13.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.13.2"><span class="ltx_text" id="S4.T1.1.13.2.1" style="font-size:90%;">Lane change (lateral push)</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.13.3.1">
<span class="ltx_p" id="S4.T1.1.13.3.1.1"><span class="ltx_text" id="S4.T1.1.13.3.1.1.1" style="font-size:90%;">Full adjacent-lane transition with gap negotiation; specify left/right in reasoning trace.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14">
<td class="ltx_td" id="S4.T1.1.14.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.14.2"><span class="ltx_text" id="S4.T1.1.14.2.1" style="font-size:90%;">Pull-over / curb approach</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.14.3.1">
<span class="ltx_p" id="S4.T1.1.14.3.1.1"><span class="ltx_text" id="S4.T1.1.14.3.1.1.1" style="font-size:90%;">Move toward edge/shoulder or a designated stop area (pickup, emergency stop, parking approach).</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.15">
<td class="ltx_td" id="S4.T1.1.15.1"></td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.15.2"><span class="ltx_text" id="S4.T1.1.15.2.1" style="font-size:90%;">Turn (intersection/roundabout/U-turn)</span></td>
<td class="ltx_td ltx_align_justify" id="S4.T1.1.15.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.15.3.1">
<span class="ltx_p" id="S4.T1.1.15.3.1.1"><span class="ltx_text" id="S4.T1.1.15.3.1.1.1" style="font-size:90%;">Planned path onto a different road segment with a significant heading change; specify left/right.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.16">
<td class="ltx_td ltx_border_bb" id="S4.T1.1.16.1"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.16.2"><span class="ltx_text" id="S4.T1.1.16.2.1" style="font-size:90%;">Lateral maneuver abort</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T1.1.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.16.3.1">
<span class="ltx_p" id="S4.T1.1.16.3.1.1"><span class="ltx_text" id="S4.T1.1.16.3.1.1.1" style="font-size:90%;">Cancel an ongoing lateral maneuver (nudge, lane change, merge/split, pull-over) and re-center when safe.</span></span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Driving Decision.</span> To ensure our CoC data is <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.2">decision-grounded</em>, we define a closed set of high-level driving decisions as in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T1" title="In 4.1 Structured Chain of Causation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">1</span></a>. Each clip is annotated with at most one longitudinal and one lateral decision (or <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.3">None</em> for either channel), corresponding to the first action taken by the ego vehicle immediately after the critical reasoning moment. This standardized inventory directly aligns with low-level trajectories and eliminates free-form, vague descriptions of driving behavior, ensuring that every reasoning trace unambiguously specifies <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.4">what</em> decision is taken. For linguistic consistency and diversity, the final CoC reasoning traces are constructed using a compact verb set aligned with these driving decisions.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Categories and example attributes of <span class="ltx_text ltx_font_italic" id="S4.T2.6.1">critical components</span> that may serve as causal factors for driving decisions. Only those directly influencing the driving decision are labeled. Use a Low/High uncertainty tag when forecasting object behavior or when signals are partially occluded. The list is open-ended, allowing additional critical components to be added as needed.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.7">
<tr class="ltx_tr" id="S4.T2.7.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.7.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.1.1.1">
<span class="ltx_p" id="S4.T2.7.1.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.1.1.1.1" style="font-size:90%;">Category</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.7.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.1.2.1">
<span class="ltx_p" id="S4.T2.7.1.2.1.1" style="width:286.2pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.2.1.1.1" style="font-size:90%;">Example attributes to record (if decision-relevant)</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.7.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.1.3.1">
<span class="ltx_p" id="S4.T2.7.1.3.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.1.3.1.1.1" style="font-size:90%;">Uncertainty</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.2.1.1">
<span class="ltx_p" id="S4.T2.7.2.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.2.1.1.1.1" style="font-size:90%;">Critical objects</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.2.2.1">
<span class="ltx_p" id="S4.T2.7.2.2.1.1" style="width:286.2pt;"><span class="ltx_text" id="S4.T2.7.2.2.1.1.1" style="font-size:90%;">Type (veh./ped./cyclist/VRU), relative pose to ego (in-path, left/right, oncoming, crosswalk), motion (stopped, slowing, crossing, cut-in risk)</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.2.3.1">
<span class="ltx_p" id="S4.T2.7.2.3.1.1" style="width:43.4pt;"><span class="ltx_text" id="S4.T2.7.2.3.1.1.1" style="font-size:90%;">Low / High</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.3.1.1">
<span class="ltx_p" id="S4.T2.7.3.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.3.1.1.1.1" style="font-size:90%;">Traffic lights</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.3.2.1">
<span class="ltx_p" id="S4.T2.7.3.2.1.1" style="width:286.2pt;"><span class="ltx_text" id="S4.T2.7.3.2.1.1.1" style="font-size:90%;">Current state (R/Y/G), arrow state, visibility/occlusion; presence of wait line</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.7.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.3.3.1">
<span class="ltx_p" id="S4.T2.7.3.3.1.1" style="width:43.4pt;"><span class="ltx_text" id="S4.T2.7.3.3.1.1.1" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.4.1.1">
<span class="ltx_p" id="S4.T2.7.4.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.4.1.1.1.1" style="font-size:90%;">Yield/Stop control</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.4.2.1">
<span class="ltx_p" id="S4.T2.7.4.2.1.1" style="width:286.2pt;"><span class="ltx_text" id="S4.T2.7.4.2.1.1.1" style="font-size:90%;">Presence of signs, all-way vs two-way, stop/yield line location</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.7.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.4.3.1">
<span class="ltx_p" id="S4.T2.7.4.3.1.1" style="width:43.4pt;"><span class="ltx_text" id="S4.T2.7.4.3.1.1.1" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.5.1.1">
<span class="ltx_p" id="S4.T2.7.5.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.5.1.1.1.1" style="font-size:90%;">Road events</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.5.2.1">
<span class="ltx_p" id="S4.T2.7.5.2.1.1" style="width:286.2pt;"><span class="ltx_text" id="S4.T2.7.5.2.1.1.1" style="font-size:90%;">Curvature/grade, speed bump, narrowing, roundabout, ramp/junction ahead</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.7.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.5.3.1">
<span class="ltx_p" id="S4.T2.7.5.3.1.1" style="width:43.4pt;"><span class="ltx_text" id="S4.T2.7.5.3.1.1.1" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.6.1.1">
<span class="ltx_p" id="S4.T2.7.6.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.6.1.1.1.1" style="font-size:90%;">Lane / lanelines</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.6.2.1">
<span class="ltx_p" id="S4.T2.7.6.2.1.1" style="width:286.2pt;"><span class="ltx_text" id="S4.T2.7.6.2.1.1.1" style="font-size:90%;">Lane count, laneline type (dashed/solid), shoulder/bike lane, usable width</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.6.3.1">
<span class="ltx_p" id="S4.T2.7.6.3.1.1" style="width:43.4pt;"><span class="ltx_text" id="S4.T2.7.6.3.1.1.1" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.1.1">
<span class="ltx_p" id="S4.T2.7.7.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.1.1.1.1" style="font-size:90%;">Routing intent</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.2.1">
<span class="ltx_p" id="S4.T2.7.7.2.1.1" style="width:286.2pt;"><span class="ltx_text" id="S4.T2.7.7.2.1.1.1" style="font-size:90%;">Target lane/turn (L/R/through), near-term split/merge, required lane for maneuver</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T2.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.3.1">
<span class="ltx_p" id="S4.T2.7.7.3.1.1" style="width:43.4pt;"><span class="ltx_text" id="S4.T2.7.7.3.1.1.1" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.7.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.8.1.1">
<span class="ltx_p" id="S4.T2.7.8.1.1.1" style="width:73.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.7.8.1.1.1.1" style="font-size:90%;">ODD constraints</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.7.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.8.2.1">
<span class="ltx_p" id="S4.T2.7.8.2.1.1" style="width:286.2pt;"><span class="ltx_text" id="S4.T2.7.8.2.1.1.1" style="font-size:90%;">Weather/visibility, construction, emergency vehicles, school bus/rail rules</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T2.7.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.8.3.1">
<span class="ltx_p" id="S4.T2.7.8.3.1.1" style="width:43.4pt;"><span class="ltx_text" id="S4.T2.7.8.3.1.1.1" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Critical Components.</span> In contrast to the closed-set driving decisions, causal factors are defined as an open-ended set, with categories and example attributes described in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T2" title="In 4.1 Structured Chain of Causation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">2</span></a>. This design allows human labelers or an auto-labeling pipeline to flexibly specify only the key elements that directly influence the driving decision, while maintaining a structured output.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Composed CoC Traces.</span> Once the driving decision and critical components are identified, they are linguistically organized into a coherent CoC reasoning trace that captures the causal rationale behind the chosen decision. As a result, the structured CoC protocol enforces:</p>
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.1">decision grounding</span>: each reasoning trace is anchored to a single, explicit decision at the critical moment;</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">causal locality</span>: all evidence must originate from the observed history window;</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i3.p1.1.1">annotation economy</span>: only decision-relevant factors are included.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Curation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Having defined the structured components of CoC (driving decisions, critical components, and composed CoC traces), the next step is to determine when these reasoning data should be labeled. Not every video clip warrants annotation; labeling is triggered only at moments where a clear causal link can be established between observable factors and the ego vehicle’s subsequent decision. Therefore, a key aspect of our data labeling framework is data curation, which involves identifying these critical reasoning moments.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Clip Selection.</span> We choose clips that contain an explicit driving decision to label the CoC dataset, thereby avoiding low-signal clips that provide limited causal information. These clips are categorized into two types of scenarios: (1) <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">Reactive</span> - where the ego vehicle must immediately adapt its behavior in response to a specific event, such as stopping for a lead vehicle or red light, or adjusting its lateral position to maintain clearance from a nearby obstacle or hazard; (2) <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">Proactive</span> - where the ego vehicle is not required to react instantly but must actively assess and anticipate potential maneuver adjustments due to upcoming road events or obstacles. For example, the ego may receive a routing command to change lanes but lacks sufficient space in the target lane, requiring continuous gap searching and space assessment in preparation for the lane change maneuver. We employ rule-based methods to identify clips corresponding to each scenario and balance the number of clips per scenario to ensure dataset diversity. Detailed definitions of the scenarios are provided in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T3" title="In 4.2 Data Curation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Scenarios used in clip selection, along with keyframe and keyframe range definitions for CoC annotation. The goal is to identify critical reasoning moments within each selected clip, where a clear causal link can be established between observable factors and the ego vehicle’s subsequent decision.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.4">
<tr class="ltx_tr" id="S4.T3.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.4.1.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.1.1.1">
<span class="ltx_p" id="S4.T3.4.1.1.1.1" style="width:39.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.1.1.1" style="font-size:90%;">Type</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.4.1.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.1.2.1">
<span class="ltx_p" id="S4.T3.4.1.2.1.1" style="width:102.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.2.1.1.1" style="font-size:90%;">Scenario name</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.4.1.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.1.3.1">
<span class="ltx_p" id="S4.T3.4.1.3.1.1" style="width:301.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.3.1.1.1" style="font-size:90%;">Keyframe Definition (Reactive) / Keyframe Range (Proactive)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.4.2.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.1.1">
<span class="ltx_p" id="S4.T3.4.2.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S4.T3.4.2.1.1.1.1" style="font-size:90%;">Reactive</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.4.2.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.2.1">
<span class="ltx_p" id="S4.T3.4.2.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.2.2.1.1.1" style="font-size:90%;">Slow for the lead vehicle</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.4.2.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.2.3.1">
<span class="ltx_p" id="S4.T3.4.2.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.2.3.1.1.1" style="font-size:90%;">0.5 seconds before the ego decelerates behind a lead vehicle.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.3">
<td class="ltx_td ltx_align_top" id="S4.T3.4.3.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.3.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.3.2.1">
<span class="ltx_p" id="S4.T3.4.3.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.3.2.1.1.1" style="font-size:90%;">Stop for the lead vehicle</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.3.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.3.3.1">
<span class="ltx_p" id="S4.T3.4.3.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.3.3.1.1.1" style="font-size:90%;">Same as above.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4">
<td class="ltx_td ltx_align_top" id="S4.T3.4.4.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.4.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.4.2.1">
<span class="ltx_p" id="S4.T3.4.4.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.4.2.1.1.1" style="font-size:90%;">Stop for traffic light (TL) / traffic sign (TS)</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.4.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.4.3.1">
<span class="ltx_p" id="S4.T3.4.4.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.4.3.1.1.1" style="font-size:90%;">Whichever occurs later: (1) 0.5 seconds before the ego begins to decelerate for a TL/TS; or (2) for a TL, the frame when it turns yellow/red.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.5">
<td class="ltx_td ltx_align_top" id="S4.T3.4.5.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.5.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.5.2.1">
<span class="ltx_p" id="S4.T3.4.5.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.5.2.1.1.1" style="font-size:90%;">Resume at TL / TS</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.5.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.5.3.1">
<span class="ltx_p" id="S4.T3.4.5.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.5.3.1.1.1" style="font-size:90%;">Whichever occurs later: (1) 0.5 seconds before the ego begins to accelerate from standstill due a TL/TS; or (2) for a TL, the frame when it turns green.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.6">
<td class="ltx_td ltx_align_top" id="S4.T3.4.6.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.6.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.6.2.1">
<span class="ltx_p" id="S4.T3.4.6.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.6.2.1.1.1" style="font-size:90%;">Lane change (LC)</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.6.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.6.3.1">
<span class="ltx_p" id="S4.T3.4.6.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.6.3.1.1.1" style="font-size:90%;">0.5 seconds before the ego starts to move off-center of its original lane.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.7">
<td class="ltx_td ltx_align_top" id="S4.T3.4.7.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.7.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.7.2.1">
<span class="ltx_p" id="S4.T3.4.7.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.7.2.1.1.1" style="font-size:90%;">Yield to VRUs</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.7.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.7.3.1">
<span class="ltx_p" id="S4.T3.4.7.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.7.3.1.1.1" style="font-size:90%;">0.5 seconds before the ego begins to decelerate or nudge for a VRU.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.8">
<td class="ltx_td ltx_align_top" id="S4.T3.4.8.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.8.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.8.2.1">
<span class="ltx_p" id="S4.T3.4.8.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.8.2.1.1.1" style="font-size:90%;">Vehicle cut-in</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.8.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.8.3.1">
<span class="ltx_p" id="S4.T3.4.8.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.8.3.1.1.1" style="font-size:90%;">Whichever occurs first: (1) when the contender signals a LC into ego’s lane; or (2) when the contender starts to move off-center of its original lane for the LC if no blinker signal is given.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.9">
<td class="ltx_td ltx_align_top" id="S4.T3.4.9.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.9.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.9.2.1">
<span class="ltx_p" id="S4.T3.4.9.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.9.2.1.1.1" style="font-size:90%;">Speed bump</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.9.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.9.3.1">
<span class="ltx_p" id="S4.T3.4.9.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.9.3.1.1.1" style="font-size:90%;">0.5 seconds before the ego decelerates for the speed bump ahead.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.10">
<td class="ltx_td ltx_align_top" id="S4.T3.4.10.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.10.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.10.2.1">
<span class="ltx_p" id="S4.T3.4.10.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.10.2.1.1.1" style="font-size:90%;">Nudge</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.10.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.10.3.1">
<span class="ltx_p" id="S4.T3.4.10.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.10.3.1.1.1" style="font-size:90%;">0.5 seconds before the ego moves away from the lane center to avoid or give space to an obstacle.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.11">
<td class="ltx_td ltx_align_top" id="S4.T3.4.11.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.11.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.11.2.1">
<span class="ltx_p" id="S4.T3.4.11.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.11.2.1.1.1" style="font-size:90%;">Bypass construction objects</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.11.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.11.3.1">
<span class="ltx_p" id="S4.T3.4.11.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.11.3.1.1.1" style="font-size:90%;">0.5 seconds before the ego decelerates or nudges to construction objects or changes lane in response to construction objects modifying the lane.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.12">
<td class="ltx_td ltx_align_top" id="S4.T3.4.12.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.12.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.12.2.1">
<span class="ltx_p" id="S4.T3.4.12.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.12.2.1.1.1" style="font-size:90%;">Risky driving</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.12.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.12.3.1">
<span class="ltx_p" id="S4.T3.4.12.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.12.3.1.1.1" style="font-size:90%;">0.5 seconds before the ego decelerates, nudges or moves backward for a risky event or obstacle, e.g., lane-weaving leading vehicle, parked vehicle backing out, or oncoming vehicle crossing into ego’s lane.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.4.13.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.13.1.1">
<span class="ltx_p" id="S4.T3.4.13.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S4.T3.4.13.1.1.1.1" style="font-size:90%;">Proactive</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.4.13.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.13.2.1">
<span class="ltx_p" id="S4.T3.4.13.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.13.2.1.1.1" style="font-size:90%;">Curvy road</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.4.13.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.13.3.1">
<span class="ltx_p" id="S4.T3.4.13.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.13.3.1.1.1" style="font-size:90%;">Start: whichever occurs first - (1) 0.5 seconds before the ego begins to decelerate for the curve; or (2) when the ego enters the curve at current speed. End: when the ego exits the curve.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.14">
<td class="ltx_td ltx_align_top" id="S4.T3.4.14.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.14.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.14.2.1">
<span class="ltx_p" id="S4.T3.4.14.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.14.2.1.1.1" style="font-size:90%;">Lane change (LC) preparation</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.14.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.14.3.1">
<span class="ltx_p" id="S4.T3.4.14.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.14.3.1.1.1" style="font-size:90%;">Start: ego receives a reason to perform a LC (e.g., route or passing a slow lead) but cannot do it immediately due to a blocked target lane.
End: Ego is ready to change lanes after gap searching or when traffic clears.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.15">
<td class="ltx_td ltx_align_top" id="S4.T3.4.15.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.15.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.15.2.1">
<span class="ltx_p" id="S4.T3.4.15.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.15.2.1.1.1" style="font-size:90%;">Nudge preparation</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.15.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.15.3.1">
<span class="ltx_p" id="S4.T3.4.15.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.15.3.1.1.1" style="font-size:90%;">Start: ego receives a reason to nudge for an obstacle, but cannot do it immediately due to traffic. End: Ego is ready to nudge once the traffic clears.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.16">
<td class="ltx_td ltx_align_top" id="S4.T3.4.16.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.4.16.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.16.2.1">
<span class="ltx_p" id="S4.T3.4.16.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.16.2.1.1.1" style="font-size:90%;">Passing intersection</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T3.4.16.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.16.3.1">
<span class="ltx_p" id="S4.T3.4.16.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.16.3.1.1.1" style="font-size:90%;">Start: ego enters the intersection when the front bumper crosses the stop line or crosswalk boundary. End: ego fully exits the intersection area.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.17">
<td class="ltx_td ltx_align_top ltx_border_bb" id="S4.T3.4.17.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.4.17.2" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.17.2.1">
<span class="ltx_p" id="S4.T3.4.17.2.1.1" style="width:102.4pt;"><span class="ltx_text" id="S4.T3.4.17.2.1.1.1" style="font-size:90%;">No yield to VRUs</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.4.17.3" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.4.17.3.1">
<span class="ltx_p" id="S4.T3.4.17.3.1.1" style="width:301.6pt;"><span class="ltx_text" id="S4.T3.4.17.3.1.1.1" style="font-size:90%;">Start: when VRUs appear with the intention to cross but are not yet crossing because (1) ego has the right of way, or (2) VRUs intentionally yield to ego. End: when the VRUs are no longer visible.</span></span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Keyframe Labeling.</span> Each raw clip contains 20 seconds of data and can generate multiple training samples, given the configuration of using a 2-second history to predict a 6-second future during both training and evaluation. Selecting keyframes for CoC annotation is therefore critical to maximizing the clarity of decision causality. For <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">reactive</span> scenarios, a keyframe is typically chosen by applying a short temporal buffer (approximately 0.5 seconds) before the ego vehicle initiates a behavior change corresponding to a driving decision. At this keyframe, the ego vehicle has accumulated sufficient observations within the preceding 2-second history to justify the forthcoming action, effectively avoiding casual confusion. Because the keyframe is positioned immediately prior to the decision-making moment, we ensure that a concrete driving decision is associated with the data sample, enabling the annotation of decision-grounded CoC traces. For <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">proactive</span> scenarios, we annotate a keyframe range: a time window during which the ego actively evaluates or prepares for a potential maneuver change. Detailed definitions of the keyframe or keyframe range for both reactive and proactive scenarios are provided in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T3" title="In 4.2 Data Curation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>. CoC reasoning traces are annotated only for samples corresponding to the keyframe timestamp or the keyframes sampled from the keyframe range.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Hybrid Labeling Procedure</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To ensure both quality and scalability, we develop a hybrid labeling procedure that combines human labeling and auto-labeling. While auto-labels are sufficient for generating large-scale training data for reasoning VLA models, high-quality and human-verified data, on the order of <math alttext="\sim 10\%" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mrow><mn>10</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\sim 10\%</annotation></semantics></math> of the total, is essential for further SFT, auto-label evaluation, and model evaluation. Our proposed hybrid labeling approach balances efficiency and accuracy, supporting both large-scale training and reliable model assessment.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Human Labeling</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1">Two-Stage Labeling Procedure.</span> Following the structured CoC described in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS1" title="4.1 Structured Chain of Causation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">4.1</span></a>, human annotators are required to complete a two-stage procedure designed to produce concise and causally grounded CoC write-ups.</p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.1">Stage I (0–2 s):</span> identify <em class="ltx_emph ltx_font_italic" id="S4.I3.i1.p1.1.2">critical components</em> from <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T2" title="In 4.1 Structured Chain of Causation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">2</span></a> within the observed history window (within 2s before the keyframe). This step helps prevent causal confusion by ensuring that only evidence available prior to the decision-making moment is considered. These critical components may influence the driving decision annotated in the next stage.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.p1.1.1">Stage II (0–8 s):</span> (a) apply a safety exclusion filter to remove invalid data with illegal or unsafe driving behavior, (b) select the first post-keyframe driving decision for each channel (longitudinal and lateral; or <em class="ltx_emph ltx_font_italic" id="S4.I3.i2.p1.1.2">None</em>), (c) write a CoC reasoning trace that references only the causal factors identified in Stage I that lead to the driving decision, along with routing or regulatory signals when applicable.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S4.SS3.SSS1.p1.2">To enforce a clear separation between Stage I and Stage II and minimize causal leakage, we designed a labeling tool that explicitly distinguishes historical video segments (0-2 s) from future segments (2-8 s). This tool also provides visual aids, including ego-dynamics plots (speed, acceleration, steering angle, and turn signals), BEV visualizations overlaid with lane topology, and obstacle bounding boxes in order to help annotators achieve a more accurate understanding of the driving scene.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p2.1.1">Quality Assurance (QA).</span> To maximize annotation quality and reduce potential bias, we implement a rigorous QA process. Each labeled instance first undergoes a quality check performed by a different annotator. Moreover, <math alttext="10\%-20\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.1.m1" intent=":literal"><semantics><mrow><mrow><mn>10</mn><mo>%</mo></mrow><mo>−</mo><mrow><mn>20</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">10\%-20\%</annotation></semantics></math> of labeled instances are selected, based on the performance of the assigned annotators, for an additional auditing process conducted by a dedicated team of experienced auditors. Both the quality check and auditing process follow the same QA guidelines, with key rules summarized in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T4" title="In 4.3.1 Human Labeling ‣ 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">4</span></a>. This QA process ensures that the desiderata of CoC are rigorously enforced while preserving flexibility for natural language expression. As a result, we generate high-quality CoC reasoning traces across diverse driving scenarios, with representative examples shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.F4" title="In 4.3.1 Human Labeling ‣ 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Quality assurance (QA) checklist for quality check and auditing process. Key rules tie closely to the desiderata of CoC: decision grounding, causal locality, and annotation economy.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.4">
<tr class="ltx_tr" id="S4.T4.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T4.4.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.1.1.1">
<span class="ltx_p" id="S4.T4.4.1.1.1.1" style="width:116.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.1.1.1.1" style="font-size:90%;">Rule</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T4.4.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.1.2.1">
<span class="ltx_p" id="S4.T4.4.1.2.1.1" style="width:267.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.2.1.1.1" style="font-size:90%;">Operational check</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.4.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.2.1.1">
<span class="ltx_p" id="S4.T4.4.2.1.1.1" style="width:116.7pt;"><span class="ltx_text" id="S4.T4.4.2.1.1.1.1" style="font-size:90%;">Causal coverage</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T4.4.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.2.2.1">
<span class="ltx_p" id="S4.T4.4.2.2.1.1" style="width:267.5pt;"><span class="ltx_text" id="S4.T4.4.2.2.1.1.1" style="font-size:90%;">Each selected decision references at least one Stage I component; otherwise mark </span><em class="ltx_emph ltx_font_italic" id="S4.T4.4.2.2.1.1.2" style="font-size:90%;">UNOBSERVED</em><span class="ltx_text" id="S4.T4.4.2.2.1.1.3" style="font-size:90%;"> with brief justification.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.3.1.1">
<span class="ltx_p" id="S4.T4.4.3.1.1.1" style="width:116.7pt;"><span class="ltx_text" id="S4.T4.4.3.1.1.1.1" style="font-size:90%;">Causal correctness</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.3.2.1">
<span class="ltx_p" id="S4.T4.4.3.2.1.1" style="width:267.5pt;"><span class="ltx_text" id="S4.T4.4.3.2.1.1.1" style="font-size:90%;">Reasoning trace must logically explain the selected decision based on valid cause–and–effect relationships. Circular reasoning, misattributed causes, or missing necessary conditions are flagged for rework</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.4.1.1">
<span class="ltx_p" id="S4.T4.4.4.1.1.1" style="width:116.7pt;"><span class="ltx_text" id="S4.T4.4.4.1.1.1.1" style="font-size:90%;">Proximate cause</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T4.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.4.2.1">
<span class="ltx_p" id="S4.T4.4.4.2.1.1" style="width:267.5pt;"><span class="ltx_text" id="S4.T4.4.4.2.1.1.1" style="font-size:90%;">Prefer the immediate driver (e.g., stopped lead) over background conditions (e.g., red light when not first in queue).</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T4.4.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.5.1.1">
<span class="ltx_p" id="S4.T4.4.5.1.1.1" style="width:116.7pt;"><span class="ltx_text" id="S4.T4.4.5.1.1.1.1" style="font-size:90%;">Decision minimality</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T4.4.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.4.5.2.1">
<span class="ltx_p" id="S4.T4.4.5.2.1.1" style="width:267.5pt;"><span class="ltx_text" id="S4.T4.4.5.2.1.1.1" style="font-size:90%;">If no change in decision, label </span><em class="ltx_emph ltx_font_italic" id="S4.T4.4.5.2.1.1.2" style="font-size:90%;">None</em><span class="ltx_text" id="S4.T4.4.5.2.1.1.3" style="font-size:90%;">.</span></span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="676" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.4.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.5.2" style="font-size:90%;">Examples of our labeled CoC reasoning traces, where <span class="ltx_text ltx_font_bold" id="S4.F4.5.2.1" style="--ltx-fg-color:#228B22;">driving decisions</span> and <span class="ltx_text ltx_font_bold" id="S4.F4.5.2.2" style="--ltx-fg-color:#4169E1;">critical components</span> are organized into CoC and highlighted correspondingly.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>List of atomic meta actions defined for longitudinal and lateral directions. These meta actions represent instantaneous kinematic changes in low-level trajectories at the frame level, in contrast to high-level driving decisions that are composed of multiple atomic actions over a video segment.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.4">
<tr class="ltx_tr" id="S4.T5.4.1">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2" id="S4.T5.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.1.1" style="font-size:90%;">Longitudinal</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2" id="S4.T5.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.4.1.2.1" style="font-size:90%;">Lateral</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.4.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.2.1.1">
<span class="ltx_p" id="S4.T5.4.2.1.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.2.1.1.1.1" style="font-size:90%;">Gentle accelerate</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.4.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.2.2.1">
<span class="ltx_p" id="S4.T5.4.2.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.2.2.1.1.1" style="font-size:90%;">Strong accelerate</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.4.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.2.3.1">
<span class="ltx_p" id="S4.T5.4.2.3.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.2.3.1.1.1" style="font-size:90%;">Steer left</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S4.T5.4.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.2.4.1">
<span class="ltx_p" id="S4.T5.4.2.4.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.2.4.1.1.1" style="font-size:90%;">Steer right</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.3.1.1">
<span class="ltx_p" id="S4.T5.4.3.1.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.3.1.1.1.1" style="font-size:90%;">Gentle decelerate</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.3.2.1">
<span class="ltx_p" id="S4.T5.4.3.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.3.2.1.1.1" style="font-size:90%;">Strong decelerate</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.3.3.1">
<span class="ltx_p" id="S4.T5.4.3.3.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.3.3.1.1.1" style="font-size:90%;">Sharp steer left</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T5.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.3.4.1">
<span class="ltx_p" id="S4.T5.4.3.4.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.3.4.1.1.1" style="font-size:90%;">Sharp steer right</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.4.1.1">
<span class="ltx_p" id="S4.T5.4.4.1.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.4.1.1.1.1" style="font-size:90%;">Maintain speed</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.4.2.1">
<span class="ltx_p" id="S4.T5.4.4.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.4.2.1.1.1" style="font-size:90%;">Stop</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T5.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.4.3.1">
<span class="ltx_p" id="S4.T5.4.4.3.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.4.3.1.1.1" style="font-size:90%;">Reverse left</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.T5.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.4.4.1">
<span class="ltx_p" id="S4.T5.4.4.4.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.4.4.1.1.1" style="font-size:90%;">Reverse right</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T5.4.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.5.1.1">
<span class="ltx_p" id="S4.T5.4.5.1.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.5.1.1.1.1" style="font-size:90%;">Reverse</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T5.4.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.5.2.1">
<span class="ltx_p" id="S4.T5.4.5.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.5.2.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T5.4.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.5.3.1">
<span class="ltx_p" id="S4.T5.4.5.3.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.5.3.1.1.1" style="font-size:90%;">Go straight</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T5.4.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T5.4.5.4.1">
<span class="ltx_p" id="S4.T5.4.5.4.1.1" style="width:99.6pt;"><span class="ltx_text" id="S4.T5.4.5.4.1.1.1" style="font-size:90%;">–</span></span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Auto-Labeling</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.1">Keyframe Selection for Auto-Labeling.</span> To efficiently scale up training data and enhance model generalization, we develop an auto-labeling pipeline for CoC annotation. To identify keyframes for auto-labeling, we first define a set of low-level meta actions and implement corresponding rule-based detectors to infer these meta actions at the frame level. Then, we treat the frame at which a meta action transition occurs as a decision-making moment, allowing us to determine the keyframe automatically and efficiently across large scale data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p2.1.1">Meta Actions.</span> The complete list of these meta actions is provided in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T5" title="In 4.3.1 Human Labeling ‣ 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a>. These low-level meta actions are <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.1.2">atomic</span>, representing instantaneous kinematic changes in the ego vehicle’s trajectory, and are therefore distinct from high-level driving decisions. A single high-level driving decision within a video segment typically consists of a sequence of such atomic meta actions across both longitudinal and lateral directions. For example, a left lane-change decision may comprise a sequence of <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.1.3">steer left</span>, followed by a brief <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.1.4">steer right</span> to stabilize the vehicle heading, and then <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.1.5">go straight</span>, often accompanied by a <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.1.6">gentle accelerate</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p2.1.7">maintain speed</span>. For each 8-second data sample, we annotate at most one longitudinal and one lateral high-level driving decision, while atomic meta actions are automatically labeled at 10Hz.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p3.1.1">Labeling Procedure.</span> Next, we employ state-of-the-art VLMs such as GPT-5 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib69" title="">2025</a>)</cite> to perform offline auto-labeling through a multi-step reasoning process. This approach distills world knowledge from large models into structured CoC annotations, while balancing efficiency and cost. Similar to the human labeling pipeline, VLMs generate structured reasoning traces consisting of the identified driving decision, critical components, and a concise reasoning trace that links the driving decision to its causal factors. To support the reasoning process, the auto-labeling pipeline provides the model with both raw video and auxiliary signals, including the ego vehicle’s trajectory, dynamic states, and meta actions. The video is sampled at 2 Hz to balance information density with the allowed input token budget within the auto-labeling model’s context window.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p4">
<p class="ltx_p" id="S4.SS3.SSS2.p4.1">To mitigate causal confusion, VLMs are prompted to use the 2-second historical video when identifying critical components. The subsequent 6-second future video, along with the ego’s trajectories and meta actions, is then used to resolve multi-modality and determine the corresponding driving decision. During this process, the model ranks the importance of the identified causal factors and retains only those that directly influence the driving decision in the final reasoning trace.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Evaluation</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">Assessing open-ended text, especially reasoning traces, remains an open challenge in the AV research community, and evaluating causal-effect relationships in CoC introduces an additional layer of complexity. Prior datasets have typically relied on one of the following approaches:</p>
<ol class="ltx_enumerate" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.1">Human evaluation</span> on a small subset of samples. While effective when labelers are properly guided, this approach is not scalable for large-scale evaluation or rapid iteration of labeling pipelines.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i2.p1.1.1">Heuristics-based metrics</span>, such as BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib72" title="">2002</a>)</cite>, METEOR <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Lavie, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib5" title="">2005</a>)</cite> and CIDEr <cite class="ltx_cite ltx_citemacro_citep">(Vedantam et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib91" title="">2015</a>)</cite>. These metrics focus on capturing only shallow text similarity and fail to reflect underlying causal reasoning, making them inadequate for evaluating our CoC dataset.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I4.i3.p1">
<p class="ltx_p" id="S4.I4.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I4.i3.p1.1.1">LLM-based auto-evaluation</span>, which leverages LLMs’ capacity to reason about causal relationships and scales effectively to large evaluation sets. However, LLMs are subject to hallucinations, particularly when assessing complex multi-step cause–and–effect chains.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S4.SS3.SSS3.p1.2">Due to these challenges, prior works often lack a reliable method for reasoning dataset evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p2.1.1">CoC Evaluation Procedure.</span> To address these challenges, we adopt a hybrid evaluation strategy that combines human verification with LLM-based auto-evaluation. Specifically, we use GPT-5 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib69" title="">2025</a>)</cite> as an LLM evaluator and construct a curated evaluation set of 2K samples spanning representative scenarios listed in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T3" title="In 4.2 Data Curation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>. To mitigate hallucination during LLM evaluation, we avoid using free-form text and grading results directly. Instead, we decompose the evaluation process into three structured subtasks covering driving decisions, presence of causal factors, and validity of the cause-and-effect relationship. By reformulating these aspects as a set of True/False questions, the evaluation process becomes more interpretable and better aligned with human judgment. To validate reliability, we compare LLM-based auto-evaluation against human evaluation on the same version of the auto-labeled dataset, and observe a 92% alignment rate, confirming the robustness of our LLM-based auto-evaluation. With this evaluation method, we find that the proposed structured CoC reasoning traces improve the causal relationship score by 132.8% relative to free-form reasoning traces, which do not enforce explicit driving decisions and critical components.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p3">
<p class="ltx_p" id="S4.SS3.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS3.p3.1.1">Effectiveness of Imperfect Auto-Labels.</span> It is important to note that attaining a perfect (100%) score in causal-effect evaluation, were it even possible, is not a necessary condition for the usefulness of auto-labeled data. Given the inherent ambiguity of causal reasoning in complex driving scenarios, as well as noise in both human-labeled ground truth and evaluation metrics, it is unclear whether 100% agreement is a reasonable or well-defined target. Instead, the primary value of CoC’s auto-labels lies in enabling large-scale SFT, which improves AR1’s generalization across diverse driving scenarios. Empirically, as will be shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6" title="6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6</span></a>, models trained on auto-labeled CoC traces already achieve significant improvements over baselines without reasoning supervision. Moreover, as will be described in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5" title="5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a>, our training pipeline incorporates subsequent RL-based post-training steps which further strengthen reasoning capability and causal consistency. In parallel, as our human annotation effort scales, we plan to introduce additional rounds of SFT using human-labeled CoC reasoning traces, progressively improving causal grounding and interpretability.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Training Strategy</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Building upon the Cosmos-Reason VLM backbone introduced in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3" title="3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>, which provides foundational physical reasoning capabilities through domain-specific SFT, we adopt a three-stage training strategy to transform the VLM into a reasoning-capable autonomous driving policy.
As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.F5" title="In 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a>, each stage progressively enhances distinct capabilities essential for robust and interpretable driving.
In <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="5.1 Action Modality Injection ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.1</span></a>, we inject the action modality into the VLM by training with discrete trajectory tokens and adding an action-expert based on flow matching, enabling the model to predict vehicle control outputs.
In <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS2" title="5.2 Eliciting Reasoning ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.2</span></a>, we improve the model’s reasoning capability through SFT on the CoC dataset (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4" title="4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">4</span></a>), teaching the model to generate causally grounded explanations for better driving decisions. Finally, in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3</span></a>, we employ RL with large reasoning model feedback to refine reasoning quality, align reasoning traces with executed actions, and optimize trajectory quality, producing interpretable and safe driving behavior.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="247" id="S5.F5.g1" src="figs/train_pipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">Overview of the Alpamayo-R1 model training pipeline, consisting of three key stages: (1) Action Modality Injection (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="5.1 Action Modality Injection ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.1</span></a>), (2) Eliciting Reasoning (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS2" title="5.2 Eliciting Reasoning ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.2</span></a>), and (3) RL-Based Post-Training (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3</span></a>).
</span></figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Action Modality Injection</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">During training, we inject the action modality to the VLM through discrete tokens (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS2.SSS2" title="3.2.2 Trajectory Decoding ‣ 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">3.2.2</span></a>) and train the VLM via cross-entropy loss over the training token sequence defined in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.E1" title="In 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Eq.</span>˜<span class="ltx_text ltx_ref_tag">1</span></a>.
Following the control-based representation in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.E3" title="In 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Eq.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>, each trajectory consists of 64 waypoints with 2 quantized values per waypoint (acceleration <math alttext="a^{i}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1" intent=":literal"><semantics><msup><mi>a</mi><mi>i</mi></msup><annotation encoding="application/x-tex">a^{i}</annotation></semantics></math> and curvature <math alttext="\kappa^{i}" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2" intent=":literal"><semantics><msup><mi>κ</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\kappa^{i}</annotation></semantics></math>), resulting in 128 discrete tokens per trajectory.
These are encoded with a set of special tokens dedicated to action representation.
However, we do not use discrete trajectory tokens for inference, as detailed below.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Motivation for Dual Representation.</span> The use of discrete tokens during training alongside a continuous flow-matching decoder at inference provides several key advantages.
First, discrete tokenization enables <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">unified</span> autoregressive training in which reasoning and trajectories share a common token space, allowing the VLM to tightly couple causal explanations with vehicle behaviors through standard next-token prediction.
Second, discrete representations facilitate RL optimization by allowing direct gradient flow during post-training (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3</span></a>), allowing policy gradient methods such as GRPO <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib80" title="">2024</a>)</cite> to jointly refine reasoning quality and reasoning-action consistency. Third, the discrete representation provides strong supervision for learning vehicle dynamics, while the flow-matching expert ensures physically feasible and multi-modal outputs. Finally, flow-matching decoding offers computational efficiency, generating continuous trajectories substantially faster than autoregressively sampling 128 discrete tokens, enabling real-time inference.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.7">Similar to <math alttext="\pi_{0.5}" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1" intent=":literal"><semantics><msub><mi>π</mi><mn>0.5</mn></msub><annotation encoding="application/x-tex">\pi_{0.5}</annotation></semantics></math>-KI <cite class="ltx_cite ltx_citemacro_citep">(Driess et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib18" title="">2025</a>)</cite>, we adopt a separate action-expert to decode actions via flow matching <cite class="ltx_cite ltx_citemacro_citep">(Janner et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib30" title="">2022</a>; Lipman et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib48" title="">2023</a>; Zhong et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib109" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib35" title="">2023b</a>)</cite>.
The action-expert follows the same Transformer architecture as the VLM, using the same number of attention heads and attention dimensions, but with a smaller hidden embedding size and MLP dimension for efficiency.
At each diffusion timestep <math alttext="t" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> in the diffusion schedule, the action-expert takes as input both the KV-cache from the sequence <math alttext="[\bm{o}_{\text{image}},\bm{o}_{\text{egomotion}},\textsc{Reason}]" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>𝒐</mi><mtext>image</mtext></msub><mo>,</mo><msub><mi>𝒐</mi><mtext>egomotion</mtext></msub><mo>,</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\bm{o}_{\text{image}},\bm{o}_{\text{egomotion}},\textsc{Reason}]</annotation></semantics></math> in the VLM and the embedded representation of the noisy control <math alttext="\bm{a}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p3.4.m4" intent=":literal"><semantics><msub><mi>𝒂</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{a}_{t}</annotation></semantics></math> (with the diffusion time <math alttext="t" class="ltx_Math" display="inline" id="S5.SS1.p3.5.m5" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> also embedded and added to the feature).
The expert then predicts the vector field <math alttext="\mathbf{v}_{\Theta}(\bm{a}_{t},\bm{o},\textsc{Reason})" class="ltx_Math" display="inline" id="S5.SS1.p3.6.m6" intent=":literal"><semantics><mrow><msub><mi>𝐯</mi><mi mathvariant="normal">Θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒂</mi><mi>t</mi></msub><mo>,</mo><mi>𝒐</mi><mo>,</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{v}_{\Theta}(\bm{a}_{t},\bm{o},\textsc{Reason})</annotation></semantics></math> by projecting the final layer feature through an MLP head, where <math alttext="\Theta" class="ltx_Math" display="inline" id="S5.SS1.p3.7.m7" intent=":literal"><semantics><mi mathvariant="normal">Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math> denotes the learnable parameters.
We train the action-expert using a vanilla conditional flow matching loss <cite class="ltx_cite ltx_citemacro_citep">(Lipman et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib48" title="">2023</a>)</cite>,</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{\text{cfm}}(\Theta)=\mathbb{E}_{t\in p_{\text{schedule}},(\bm{o},\textsc{Reason})\in\mathcal{D}_{\text{data}}}\|\mathbf{v}_{\Theta}(\bm{a}_{t},\bm{o},\textsc{Reason})-\mathbf{u}(\bm{a}_{t}|\bm{a})\|." class="ltx_math_unparsed" display="block" id="S5.E6.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mtext>cfm</mtext></msub><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><msub><mi>𝔼</mi><mrow><mrow><mi>t</mi><mo>∈</mo><msub><mi>p</mi><mtext>schedule</mtext></msub></mrow><mo>,</mo><mrow><mrow><mo stretchy="false">(</mo><mi>𝒐</mi><mo>,</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo stretchy="false">)</mo></mrow><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>data</mtext></msub></mrow></mrow></msub><mo lspace="0em" rspace="0.167em">∥</mo><msub><mi>𝐯</mi><mi mathvariant="normal">Θ</mi></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝒂</mi><mi>t</mi></msub><mo>,</mo><mi>𝒐</mi><mo>,</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo stretchy="false">)</mo></mrow><mo>−</mo><mi>𝐮</mi><mrow><mo stretchy="false">(</mo><msub><mi>𝒂</mi><mi>t</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>𝒂</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.0835em">∥</mo><mo lspace="0.0835em">.</mo></mrow><annotation encoding="application/x-tex">L_{\text{cfm}}(\Theta)=\mathbb{E}_{t\in p_{\text{schedule}},(\bm{o},\textsc{Reason})\in\mathcal{D}_{\text{data}}}\|\mathbf{v}_{\Theta}(\bm{a}_{t},\bm{o},\textsc{Reason})-\mathbf{u}(\bm{a}_{t}|\bm{a})\|.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p3.9">In practice, we adopt the Gaussian conditional optimal transport (OT) path and sample <math alttext="\bm{a}_{t}=t\bm{a}+(1-t)\bm{\epsilon}" class="ltx_Math" display="inline" id="S5.SS1.p3.8.m1" intent=":literal"><semantics><mrow><msub><mi>𝒂</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒂</mi></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{a}_{t}=t\bm{a}+(1-t)\bm{\epsilon}</annotation></semantics></math> with <math alttext="\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S5.SS1.p3.9.m2" intent=":literal"><semantics><mrow><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I})</annotation></semantics></math>, where the target vector field admits a closed-form expression:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{u}(\bm{a}_{t}|\bm{a})=\bm{a}-\bm{\epsilon}." class="ltx_Math" display="block" id="S5.E7.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>𝐮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒂</mi><mi>t</mi></msub><mo fence="false">|</mo><mi>𝒂</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝒂</mi><mo>−</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbf{u}(\bm{a}_{t}|\bm{a})=\bm{a}-\bm{\epsilon}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p3.10">During inference, starting with <math alttext="\bm{a}_{0}\in\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S5.SS1.p3.10.m1" intent=":literal"><semantics><mrow><msub><mi>𝒂</mi><mn>0</mn></msub><mo>∈</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{a}_{0}\in\mathcal{N}(\bm{0},\bm{I})</annotation></semantics></math>, we perform denoising through Euler integration:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{a}_{t+\delta_{t}}=\bm{a}_{t}+\delta_{t}\,\mathbf{v}_{\Theta}(\bm{a}_{t},\bm{o},\textsc{Reason})." class="ltx_Math" display="block" id="S5.E8.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>𝒂</mi><mrow><mi>t</mi><mo>+</mo><msub><mi>δ</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><mrow><msub><mi>𝒂</mi><mi>t</mi></msub><mo>+</mo><mrow><msub><mi>δ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐯</mi><mi mathvariant="normal">Θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒂</mi><mi>t</mi></msub><mo>,</mo><mi>𝒐</mi><mo>,</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{a}_{t+\delta_{t}}=\bm{a}_{t}+\delta_{t}\,\mathbf{v}_{\Theta}(\bm{a}_{t},\bm{o},\textsc{Reason}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p3.12">By default, we use <math alttext="\delta_{t}=0.1" class="ltx_Math" display="inline" id="S5.SS1.p3.11.m1" intent=":literal"><semantics><mrow><msub><mi>δ</mi><mi>t</mi></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\delta_{t}=0.1</annotation></semantics></math> during inference and set <math alttext="p_{\text{schedule}}" class="ltx_Math" display="inline" id="S5.SS1.p3.12.m2" intent=":literal"><semantics><msub><mi>p</mi><mtext>schedule</mtext></msub><annotation encoding="application/x-tex">p_{\text{schedule}}</annotation></semantics></math> to a shifted beta distribution during training, as suggested by <cite class="ltx_cite ltx_citemacro_citet">Physical Intelligence et al. (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib73" title="">2025</a>)</cite>. During training, we apply a stop-gradient to the KV-cache produced by the VLM to prevent gradients from the expert back-propagating into the VLM weights.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Eliciting Reasoning</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.3">Having established a VLA with action generation capabilities in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="5.1 Action Modality Injection ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.1</span></a>, the next challenge is to enable the model to perform structured and causally grounded reasoning that explains <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.3.1">why</span> specific driving decisions are made. This capability is critical for handling complex, safety-critical scenarios where pure pattern matching from imitation learning may fail <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib94" title="">2022</a>)</cite>. To achieve this, we leverage the structured CoC dataset introduced in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4" title="4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">4</span></a>, which provides decision-grounded and causally linked reasoning traces paired with expert trajectories. We perform SFT on the CoC dataset to teach the model to generate reasoning traces through imitation, where each reasoning trace is anchored to explicit driving decisions (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T1" title="In 4.1 Structured Chain of Causation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">1</span></a>) and grounded in critical scene components (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T2" title="In 4.1 Structured Chain of Causation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">2</span></a>). While SFT enables the model to scaffold basic reasoning capabilities, we further refine reasoning quality and enforce reasoning-action consistency through RL in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3</span></a>.
Formally, each training sample consists of a multi-camera driving scene observation <math alttext="\bm{o}=[\bm{o}_{\text{image}},\bm{o}_{\text{egomotion}}]" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1" intent=":literal"><semantics><mrow><mi>𝒐</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒐</mi><mtext>image</mtext></msub><mo>,</mo><msub><mi>𝒐</mi><mtext>egomotion</mtext></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{o}=[\bm{o}_{\text{image}},\bm{o}_{\text{egomotion}}]</annotation></semantics></math>, a structured CoC reasoning trace <span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S5.SS2.p1.3.2">Reason</span> that explains the causal factors behind the ego vehicle’s decision, and the corresponding ground-truth control-based trajectory representation <math alttext="\bm{a}" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3" intent=":literal"><semantics><mi>𝒂</mi><annotation encoding="application/x-tex">\bm{a}</annotation></semantics></math> defined in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.E3" title="In 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Eq.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>. Following the sequence formulation in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.E1" title="In 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Eq.</span>˜<span class="ltx_text ltx_ref_tag">1</span></a>, the SFT objective maximizes the conditional log-likelihood of the reasoning–action sequence:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{SFT}}(\theta)=-\mathbb{E}_{(\bm{o},\textsc{Reason},\bm{a})\sim\mathcal{D}_{\text{CoC}}}\left[\log\pi_{\theta}(\textsc{Reason},\,\bm{a}\mid\bm{o})\right]," class="ltx_Math" display="block" id="S5.E9.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>SFT</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><msub><mi>𝔼</mi><mrow><mrow><mo stretchy="false">(</mo><mi>𝒐</mi><mo>,</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo>,</mo><mi>𝒂</mi><mo stretchy="false">)</mo></mrow><mo>∼</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>CoC</mtext></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mtext class="ltx_font_smallcaps">Reason</mtext><mo rspace="0.337em">,</mo><mrow><mi>𝒂</mi><mo>∣</mo><mi>𝒐</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{SFT}}(\theta)=-\mathbb{E}_{(\bm{o},\textsc{Reason},\bm{a})\sim\mathcal{D}_{\text{CoC}}}\left[\log\pi_{\theta}(\textsc{Reason},\,\bm{a}\mid\bm{o})\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS2.p1.5">where <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> denotes the VLA policy parameterized by <math alttext="\theta" class="ltx_Math" display="inline" id="S5.SS2.p1.5.m2" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>, encompassing the vision encoder, language backbone, and corresponding embedding adapters.
In practice, we apply the cross-entropy loss over both the reasoning tokens and the discrete trajectory tokens (128 tokens per trajectory as described in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="5.1 Action Modality Injection ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.1</span></a>), enabling the model to learn the joint distribution of language-based reasoning and action prediction in a unified autoregressive framework.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Why SFT Alone is Insufficient.</span>
This imitation learning stage allows the model to internalize human-like reasoning patterns: learning not only <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.2">what</em> action to take, but also <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.3">why</em> such actions are appropriate given specific visual and contextual cues. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F8" title="In 6.1 Evaluation Protocol ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">8</span></a>, SFT on CoC data already yields measurable improvements in trajectory prediction accuracy compared to models trained without explicit reasoning supervision. However, while SFT enables the VLA model to scaffold reasoning traces, it remains inherently limited by several factors:</p>
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i1.p1.1.1">Data bias and annotation noise</span>: Auto-labeled data may contain imperfect causal relationships (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T5" title="In 4.3.1 Human Labeling ‣ 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a>), causing the model to overfit to annotation artifacts rather than learning robust causal reasoning.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i2.p1.1.1">Limited generalization</span>: The model may memorize common reasoning patterns without developing deeper causal understanding, failing to generalize to novel scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i3.p1.1.1">Weak visual grounding</span>: Next-token prediction does not enforce visual consistency; the model may hallucinate causal factors not present in the scene (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F10" title="In 6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i4.p1.1.1">Reasoning–action inconsistency</span>: Joint optimization does not explicitly enforce alignment between stated reasoning and predicted trajectories, potentially leading to contradictory explanations (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F11" title="In 6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">11</span></a>).</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">In the next section (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3</span></a>), we illustrate our approach to mitigate these limitations via RL-based post-training with large reasoning model feedback and explicit reasoning-action consistency rewards.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>RL-based Post-Training</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To address the limitations of SFT outlined in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS2" title="5.2 Eliciting Reasoning ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.2</span></a>, we introduce an RL-based post-training framework shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.F6" title="In 5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">6</span></a> that optimizes three complementary reward signals: reasoning quality (via large reasoning model feedback), reasoning-action consistency, and trajectory quality. Unlike SFT, which optimizes the likelihood of expert demonstrations under <em class="ltx_emph ltx_font_italic" id="S5.SS3.p1.1.1">teacher forcing</em> without feedback on the test-time inference errors, RL provides explicit inference feedback on the model’s own rollouts, aligning the optimization objective with how the system is actually deployed. This approach directly tackles the shortcomings of SFT by providing targeted feedback that evaluates both the causal correctness of reasoning and its alignment with executed actions, and yields disproportionately larger gains in robustness and generalization for the same compute budget.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S5.F6.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S5.F6.3.2" style="font-size:90%;">Overview of our RL-based post-training framework. We optimize three reward components: reasoning quality (via large reasoning model feedback), reasoning-action consistency, and trajectory quality, to align the model’s generated reasoning traces with its predicted actions.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Post-Training Algorithm</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">Large-scale foundation model post-training has emerged as a central strategy to enhance the reasoning capabilities and generation quality of large-scale foundation models <cite class="ltx_cite ltx_citemacro_citep">(Christiano et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib10" title="">2017</a>; DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib14" title="">2025</a>)</cite>.
Recently, these techniques have been extended to the embodied AI domain, encouraging VLA models to generate actions that better reflect human intent across diverse embodiments, including autonomous driving <cite class="ltx_cite ltx_citemacro_citep">(Tian and Goel, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib85" title="">2025</a>)</cite> and generalist robotic agents <cite class="ltx_cite ltx_citemacro_citep">(Tian et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib87" title="">2024b</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib108" title="">2025</a>)</cite>.
In our <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.1">reasoning</span> VLA context, the alignment stage extends beyond improving motion generation; it explicitly enhances reasoning quality grounded in embodied settings and enforces reasoning–action consistency, both of which are key properties for achieving interpretable and trustworthy autonomy.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p2">
<p class="ltx_p" id="S5.SS3.SSS1.p2.3">We adopt GRPO <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib80" title="">2024</a>)</cite> as our alignment algorithm. GRPO extends standard policy gradient methods by optimizing relative advantages within a group of sampled model rollouts rather than relying on absolute reward signals.
Specifically, given a group of model rollouts <math alttext="\{\tau_{i}\}_{i=1}^{K}" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p2.1.m1" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>τ</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">\{\tau_{i}\}_{i=1}^{K}</annotation></semantics></math> sampled from the current model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p2.2.m2" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math>, each with an associated scalar reward <math alttext="r_{i}" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p2.3.m3" intent=":literal"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_{i}</annotation></semantics></math>, the objective of GRPO is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{GRPO}}(\theta)=-\mathbb{E}{\tau_{i}\sim\pi_{\theta}}\left[\frac{\exp(\beta A_{i})}{\sum_{j}\exp(\beta A_{j})}\left(\log\pi_{\theta}(\tau_{i})-\lambda_{\mathrm{KL}}\mathrm{KL}[\pi_{\theta}(\tau_{i})\|\pi_{\text{ref}}(\tau_{i})]\right)\right],\quad A_{i}=r_{i}-\bar{r}." class="ltx_Math" display="block" id="S5.E10.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>GRPO</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>τ</mi><mi>i</mi></msub></mrow></mrow><mo>∼</mo><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>A</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mo>∑</mo><mi>j</mi></msub><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>A</mi><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>λ</mi><mi>KL</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>KL</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∥</mo><mrow><msub><mi>π</mi><mtext>ref</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>r</mi><mo>¯</mo></mover></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{GRPO}}(\theta)=-\mathbb{E}{\tau_{i}\sim\pi_{\theta}}\left[\frac{\exp(\beta A_{i})}{\sum_{j}\exp(\beta A_{j})}\left(\log\pi_{\theta}(\tau_{i})-\lambda_{\mathrm{KL}}\mathrm{KL}[\pi_{\theta}(\tau_{i})\|\pi_{\text{ref}}(\tau_{i})]\right)\right],\quad A_{i}=r_{i}-\bar{r}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p3">
<p class="ltx_p" id="S5.SS3.SSS1.p3.5">Here, <math alttext="A_{i}" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p3.1.m1" intent=":literal"><semantics><msub><mi>A</mi><mi>i</mi></msub><annotation encoding="application/x-tex">A_{i}</annotation></semantics></math> denotes the relative advantage of each trajectory within the group, <math alttext="\bar{r}" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p3.2.m2" intent=":literal"><semantics><mover accent="true"><mi>r</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{r}</annotation></semantics></math> is the group-average reward, and <math alttext="\beta" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p3.3.m3" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> controls the sharpness of the weighting distribution.
The KL regularization term with coefficient <math alttext="\lambda_{\mathrm{KL}}" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p3.4.m4" intent=":literal"><semantics><msub><mi>λ</mi><mi>KL</mi></msub><annotation encoding="application/x-tex">\lambda_{\mathrm{KL}}</annotation></semantics></math> penalizes deviations from the reference policy <math alttext="\pi_{\text{ref}}" class="ltx_Math" display="inline" id="S5.SS3.SSS1.p3.5.m5" intent=":literal"><semantics><msub><mi>π</mi><mtext>ref</mtext></msub><annotation encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics></math> (typically the SFT model), preventing over-optimization on noisy or biased reward signals and preserving linguistic and behavioral priors learned during pre-training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>Reward Model</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">Our reward model integrates three complementary signals that together evaluate both what the model reasons and how it acts. Specifically, the total reward <math alttext="r" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p1.1.m1" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> for each rollout is composed of three components: reasoning quality reward, reasoning-action consistency, and low-level trajectory quality.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p2">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS2.p2.1.1">Grading Reasoning with Large Reasoning Models.</span>
To mitigate the issue where reasoning traces can exhibit hallucinations that produce plausible yet unsafe or causally inconsistent plans, we employ large reasoning models (LRMs) as automatic evaluators to provide scalable, high-quality feedback on reasoning quality. Inspired by recent advances in LLM alignment, where expert models serve as judges to provide scalable feedback <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib4" title="">2022</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib40" title="">2023a</a>)</cite>, we leverage state-of-the-art LRMs (e.g., DeepSeek-R1 <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib14" title="">2025</a>)</cite>, Cosmos-Reason <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib66" title="">2025a</a>)</cite>) as <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS2.p2.1.2">reasoning critics</em> to evaluate the quality of reasoning traces generated by the VLA.
We choose an LRM as the critic because, although such models may struggle to generate driving-specific reasoning due to limited embodiment priors, they exhibit strong verification and evaluation capabilities. In other words, even when generation in this domain is imperfect, their ability to assess logical soundness, causal alignment, and contextual consistency remains highly reliable (also known as the generation–verification gap <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib83" title="">2025</a>)</cite>).
The resulting reward signal provides a continuous measure of reasoning quality, enabling RL to iteratively refine the model’s ability to generate grounded and logically consistent reasoning.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p3">
<p class="ltx_p" id="S5.SS3.SSS2.p3.6"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS2.p3.6.1">Reasoning Critic Design.</span>
For each training sample, the LRM critic takes as input the multi-camera visual observation <math alttext="\bm{o}_{\text{image}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p3.1.m1" intent=":literal"><semantics><msub><mi>𝒐</mi><mtext>image</mtext></msub><annotation encoding="application/x-tex">\bm{o}_{\text{image}}</annotation></semantics></math> at the last frame of the 2-second history window, the ground-truth CoC reasoning trace <math alttext="\textsc{Reason}_{\text{GT}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p3.2.m2" intent=":literal"><semantics><msub><mtext class="ltx_font_smallcaps">Reason</mtext><mtext>GT</mtext></msub><annotation encoding="application/x-tex">\textsc{Reason}_{\text{GT}}</annotation></semantics></math> from the dataset, and the model-generated reasoning trace <math alttext="\textsc{Reason}_{\text{pred}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p3.3.m3" intent=":literal"><semantics><msub><mtext class="ltx_font_smallcaps">Reason</mtext><mtext>pred</mtext></msub><annotation encoding="application/x-tex">\textsc{Reason}_{\text{pred}}</annotation></semantics></math> produced by the current policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p3.4.m4" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math>.
The critic evaluates how well <math alttext="\textsc{Reason}_{\text{pred}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p3.5.m5" intent=":literal"><semantics><msub><mtext class="ltx_font_smallcaps">Reason</mtext><mtext>pred</mtext></msub><annotation encoding="application/x-tex">\textsc{Reason}_{\text{pred}}</annotation></semantics></math> aligns with <math alttext="\textsc{Reason}_{\text{GT}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p3.6.m6" intent=":literal"><semantics><msub><mtext class="ltx_font_smallcaps">Reason</mtext><mtext>GT</mtext></msub><annotation encoding="application/x-tex">\textsc{Reason}_{\text{GT}}</annotation></semantics></math> along two dimensions: <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS2.p3.6.2">behavior consistency</span>, whether the predicted reasoning describes a driving decision consistent with ground truth; and <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS2.p3.6.3">causal reasoning quality</span>, whether it correctly identifies causal factors observable in the scene’s history according to CoC principles (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.SS1" title="4.1 Structured Chain of Causation ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">4.1</span></a>).
The critic grades the predicted reasoning according to a structured rubric focused on behavior consistency and causal reasoning consistency:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS2.p4">
<span class="ltx_inline-block"><svg class="ltx_picture" height="460.45" id="S5.SS3.SSS2.p4.pic1" overflow="visible" version="1.1" viewbox="0 0 600 460.45" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,460.45) matrix(1 0 0 -1 0 0)"><g fill="#B3B3B3" fill-opacity="1.0" style="--ltx-fill-color:#B3B3B3;"><path d="M 0 3.46 L 0 456.99 C 0 458.91 1.55 460.45 3.46 460.45 L 596.54 460.45 C 598.45 460.45 600 458.91 600 456.99 L 600 3.46 C 600 1.55 598.45 0 596.54 0 L 3.46 0 C 1.55 0 0 1.55 0 3.46 Z" style="stroke:none"></path></g><g fill="#F9F9F9" fill-opacity="1.0" style="--ltx-fill-color:#F9F9F9;"><path d="M 0.69 3.46 L 0.69 438.9 L 599.31 438.9 L 599.31 3.46 C 599.31 1.93 598.07 0.69 596.54 0.69 L 3.46 0.69 C 1.93 0.69 0.69 1.93 0.69 3.46 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.93 446.22)"><foreignobject color="#000000" height="12.3" overflow="visible" style="--ltx-fg-color:#000000;--ltx-fo-width:41.49em;--ltx-fo-height:0.69em;--ltx-fo-depth:0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="574.14"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SS3.SSS2.p4.pic1.1.1.1.1.1" style="width:36.08em;">
<span class="ltx_p" id="S5.SS3.SSS2.p4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS2.p4.pic1.1.1.1.1.1.1.1">Prompt : LLM Reasoning Grading Rubric</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.93 56.86)"><foreignobject color="#000000" height="413.73" overflow="visible" style="--ltx-fg-color:#000000;--ltx-fo-width:41.49em;--ltx-fo-height:26.72em;--ltx-fo-depth:3.17em;" transform="matrix(1 0 0 -1 0 369.79)" width="574.14"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1" style="width:44.86em;">
<span class="ltx_p" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1.1"><span class="ltx_text" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1.1.1" style="font-size:90%;">You are an expert evaluator for autonomous driving reasoning traces. The reasoning trace describes what the ego vehicle should be doing and the reasons and factors that lead to the behavior. Your task is to score how well a predicted reasoning trace (<span class="ltx_text ltx_font_typewriter" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1.1.1.1">PRED</span>) aligns with the ground truth (<span class="ltx_text ltx_font_typewriter" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1.1.1.2">GT</span>) in terms of behavior consistency and causal reasoning.</span></span>
<span class="ltx_p" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1.2.1" style="font-size:90%;">Scoring rubric (0–5):<span class="ltx_text ltx_font_medium" id="S5.SS3.SSS2.p4.pic1.2.2.2.1.1.2.1.1"></span></span></span>
<span class="ltx_itemize" id="S5.I2">
<span class="ltx_item" id="S5.I2.ix1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">5</span>
<span class="ltx_para" id="S5.I2.ix1.p1">
<span class="ltx_p" id="S5.I2.ix1.p1.1"><span class="ltx_text" id="S5.I2.ix1.p1.1.1" style="font-size:90%;">Behavior &amp; causal reasoning fully consistent.</span></span>
</span></span>
<span class="ltx_item" id="S5.I2.ix2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4</span>
<span class="ltx_para" id="S5.I2.ix2.p1">
<span class="ltx_p" id="S5.I2.ix2.p1.1"><span class="ltx_text" id="S5.I2.ix2.p1.1.1" style="font-size:90%;">Behavior correct; causal reasoning mostly consistent.</span></span>
</span></span>
<span class="ltx_item" id="S5.I2.ix3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3</span>
<span class="ltx_para" id="S5.I2.ix3.p1">
<span class="ltx_p" id="S5.I2.ix3.p1.1"><span class="ltx_text" id="S5.I2.ix3.p1.1.1" style="font-size:90%;">Behavior roughly correct, but incomplete or slightly incorrect reasoning.</span></span>
</span></span>
<span class="ltx_item" id="S5.I2.ix4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2</span>
<span class="ltx_para" id="S5.I2.ix4.p1">
<span class="ltx_p" id="S5.I2.ix4.p1.1"><span class="ltx_text" id="S5.I2.ix4.p1.1.1" style="font-size:90%;">Behavior partially incorrect or reasoning largely inconsistent.</span></span>
</span></span>
<span class="ltx_item" id="S5.I2.ix5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1</span>
<span class="ltx_para" id="S5.I2.ix5.p1">
<span class="ltx_p" id="S5.I2.ix5.p1.1"><span class="ltx_text" id="S5.I2.ix5.p1.1.1" style="font-size:90%;">Behavior is wrong or contradicts GT.</span></span>
</span></span>
<span class="ltx_item" id="S5.I2.ix6" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">0</span>
<span class="ltx_para" id="S5.I2.ix6.p1">
<span class="ltx_p" id="S5.I2.ix6.p1.1"><span class="ltx_text" id="S5.I2.ix6.p1.1.1" style="font-size:90%;">Completely unrelated or opposite.</span></span>
</span></span>
</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p5">
<p class="ltx_p" id="S5.SS3.SSS2.p5.1">The resulting scalar score <math alttext="r_{\text{reason}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p5.1.m1" intent=":literal"><semantics><msub><mi>r</mi><mtext>reason</mtext></msub><annotation encoding="application/x-tex">r_{\text{reason}}</annotation></semantics></math> is used as the reasoning reward. This signal encourages the model to generate reasoning traces that not only describe correct driving behaviors but also maintain causal fidelity, accurately explaining why an action is taken based on visual context and traffic cues.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p6">
<p class="ltx_p" id="S5.SS3.SSS2.p6.3"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS2.p6.3.1">CoC-Action Consistency</span>. To ensure that the model’s action generation faithfully follows its reasoning, we introduce a CoC–action consistency reward that measures behavioral alignment between the generated reasoning trace and the corresponding predicted ego trajectory.
Specifically, for each reasoning–action rollout, we convert the predicted motion trajectory into a sequence of meta-actions (interpretable motion primitives) described in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4.T5" title="In 4.3.1 Human Labeling ‣ 4.3 Hybrid Labeling Procedure ‣ 4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">5</span></a>.
These meta-actions encode the ego vehicle’s control behavior along both the longitudinal (acceleration/braking) and lateral (steering) directions. We then parse the generated reasoning trace to infer the ego’s intended behavior and compare it against the meta-actions derived from the predicted trajectory using rule-based matching. If the described behavior in the reasoning trace and the meta-action are consistent across both axes, we assign <math alttext="r_{\text{consistency}}=1" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p6.1.m1" intent=":literal"><semantics><mrow><msub><mi>r</mi><mtext>consistency</mtext></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_{\text{consistency}}=1</annotation></semantics></math>; otherwise, <math alttext="r_{\text{consistency}}=0" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p6.2.m2" intent=":literal"><semantics><mrow><msub><mi>r</mi><mtext>consistency</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">r_{\text{consistency}}=0</annotation></semantics></math>. In cases where the reasoning cannot be parsed into a valid driving decision (i.e., the intent is not recognized within the closed decision set used for auto-labeling), we conservatively assign <math alttext="r_{\text{consistency}}=0" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p6.3.m3" intent=":literal"><semantics><mrow><msub><mi>r</mi><mtext>consistency</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">r_{\text{consistency}}=0</annotation></semantics></math>.
Although based on simple rule-based logic, this binary reward plays a crucial role in improving the trustworthiness of the model’s reasoning–action coupling.
By explicitly penalizing inconsistencies and rewarding only correct matches, it encourages the model to generate reasoning that not only sounds plausible but also translates into coherent, physically consistent behavior.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p7">
<p class="ltx_p" id="S5.SS3.SSS2.p7.5"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS2.p7.5.1">Low-Level Trajectory Quality.</span> To ensure that the generated motion trajectories remain physically feasible, comfortable, and safe to execute, we include a low-level trajectory quality reward that evaluates the model’s motion outputs in continuous space.
This component complements the above reasoning- and consistency-level rewards by directly regularizing the trajectory’s physical properties.
The reward combines three terms:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{\text{traj}}=\lambda_{\text{L2}}\|x_{\text{pred}}-x_{\text{expert}}\|_{2}^{2}+\lambda_{\text{coll}}\mathbb{I}[\text{collision}(x_{\text{pred}})]+\lambda_{\text{jerk}}J(x_{\text{pred}})," class="ltx_Math" display="block" id="S5.E11.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>r</mi><mtext>traj</mtext></msub><mo>=</mo><mrow><mrow><msub><mi>λ</mi><mtext>L2</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>x</mi><mtext>pred</mtext></msub><mo>−</mo><msub><mi>x</mi><mtext>expert</mtext></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mtext>coll</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝕀</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mtext>collision</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>pred</mtext></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mtext>jerk</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mi>J</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>pred</mtext></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">r_{\text{traj}}=\lambda_{\text{L2}}\|x_{\text{pred}}-x_{\text{expert}}\|_{2}^{2}+\lambda_{\text{coll}}\mathbb{I}[\text{collision}(x_{\text{pred}})]+\lambda_{\text{jerk}}J(x_{\text{pred}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS3.SSS2.p7.4">where <math alttext="x_{\text{pred}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p7.1.m1" intent=":literal"><semantics><msub><mi>x</mi><mtext>pred</mtext></msub><annotation encoding="application/x-tex">x_{\text{pred}}</annotation></semantics></math> and <math alttext="x_{\text{expert}}" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p7.2.m2" intent=":literal"><semantics><msub><mi>x</mi><mtext>expert</mtext></msub><annotation encoding="application/x-tex">x_{\text{expert}}</annotation></semantics></math> denote the predicted and expert trajectories, respectively;
<math alttext="\mathbb{I}[\text{collision}(x_{\text{pred}})]" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p7.3.m3" intent=":literal"><semantics><mrow><mi>𝕀</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mtext>collision</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>pred</mtext></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{I}[\text{collision}(x_{\text{pred}})]</annotation></semantics></math> is a binary indicator that denotes whether the predicted motion leads to a collision with surrounding obstacles;
and <math alttext="J(x_{\text{pred}})" class="ltx_Math" display="inline" id="S5.SS3.SSS2.p7.4.m4" intent=":literal"><semantics><mrow><mi>J</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>pred</mtext></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(x_{\text{pred}})</annotation></semantics></math> measures the magnitude of the jerk to penalize abrupt or uncomfortable motion.
The L2 imitation term encourages proximity to expert demonstrations, promoting stable learning and smooth driving profiles. The collision penalty ensures safety, while the jerk regularization improves comfort and control smoothness.
Together, these terms anchor the learning of the model to human-like, safe, and comfortable motion, reinforcing the physical plausibility of the trajectories generated during the alignment process.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S5.F7.g1" src="x6.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S5.F7.3.2" style="font-size:90%;">Compared to models that only output trajectories or only output meta-actions and trajectories, Alpamayo-R1 achieves improvements in both nominal and challenging scenarios.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3 </span>Post-Training Data Curation for Cost-Effective Training</h4>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">RL–based post-training is computationally expensive due to its iterative nature: each policy update requires multiple model rollouts, reward evaluations, and gradient steps across large batches of reasoning and trajectory samples.
Moreover, unlike the SFT stage where the loss is directly computed from labeled data, our post-training procedure involves on-policy sampling and LRM-based reward function calls, which amplify both compute and data costs.
Consequently, scaling RL to the full pre-training data would be prohibitive in both training time and compute resources.
To address this, we curate a high-information-gain dataset for RL post-training.
The key idea is to prioritize samples where the model’s implicit reward signal (encoded in its logits) disagrees with the explicit reward model.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS3.p2">
<p class="ltx_p" id="S5.SS3.SSS3.p2.2">Specifically, for each sample rollout from the model (denoted as <math alttext="\tau_{i}" class="ltx_Math" display="inline" id="S5.SS3.SSS3.p2.1.m1" intent=":literal"><semantics><msub><mi>τ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\tau_{i}</annotation></semantics></math>), we compute the model’s predicted probability distribution derived from its logits, and the corresponding probability distribution implied by the rewards, which we obtain by transforming the reward into a Boltzmann distribution
<math alttext="p_{\text{reward}}(\tau_{i})=\frac{\exp(\beta\,r_{i})}{\sum_{j}\exp(\beta\,r_{j})}" class="ltx_Math" display="inline" id="S5.SS3.SSS3.p2.2.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>p</mi><mtext>reward</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>β</mi><mo lspace="0.170em" rspace="0em">​</mo><msub><mi>r</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mstyle displaystyle="false"><msub><mo>∑</mo><mi>j</mi></msub></mstyle><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>β</mi><mo lspace="0.170em" rspace="0em">​</mo><msub><mi>r</mi><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">p_{\text{reward}}(\tau_{i})=\frac{\exp(\beta\,r_{i})}{\sum_{j}\exp(\beta\,r_{j})}</annotation></semantics></math>.
A large divergence between these two distributions indicates that the model’s internal preference (its implicit reward) conflicts with the externally defined reward signal. Such disagreement reveals samples where the model’s learned reward is inaccurate, making them particularly valuable for alignment. We therefore prioritize these high-disagreement samples to construct a focused post-training dataset, while mixing in a similar proportion of randomly sampled data to preserve distributional diversity and stabilize training. By focusing RL updates on this hybrid set, we achieve both high alignment efficiency and robust learning dynamics compared to uniformly sampled data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.4 </span>Post-Training Infrastructure</h4>
<div class="ltx_para" id="S5.SS3.SSS4.p1">
<p class="ltx_p" id="S5.SS3.SSS4.p1.1">To conduct our RL experiments, we develop a customized version of the Cosmos-RL framework <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib62" title="">2025a</a>)</cite> that is specifically designed for AV reasoning tasks. This system provides a scalable, modular infrastructure for large-scale multimodal RL and fits directly with other parts of the Alpamayo-R1 system. It supports distributed data loading, mixed-parallelism training, vLLM-based rollout generation <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib39" title="">2023</a>)</cite>, and reward computation across multiple GPU nodes, enabling efficient, high-throughput policy optimization.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We conduct comprehensive evaluations of Alpamayo-R1 across multiple dimensions to assess its reasoning capabilities, trajectory prediction accuracy, and closed-loop driving performance. We first highlight in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.F7" title="In 5.3.2 Reward Model ‣ 5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">7</span></a> that the proposed Alpamayo-R1 significantly outperforms the trajectory-only baseline, particularly in challenging scenarios that intuitively require complex reasoning to make better driving decisions.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In the following sections, we first present the evaluation protocol in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS1" title="6.1 Evaluation Protocol ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.1</span></a>. Next, we illustrate how our reasoning-capable model contributes to an improved driving policy in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS2" title="6.2 Policy Improvements from Reasoning ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.2</span></a>. In <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS3" title="6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.3</span></a> we further demonstrate the improvements in behavioral alignment achieved through RL. From <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS5" title="6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.5</span></a> to <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS7" title="6.7 Ablation: Efficient Vision Encoding ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.7</span></a> we conduct a comprehensive ablation study on the backbone model, the trajectory expert model, and the vision encoder to gain deeper insight into the effectiveness of our proposed methodology. Finally, we present an on-vehicle demonstration showcasing the real-world performance of our model.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Evaluation Protocol</h3>
<figure class="ltx_table" id="S6.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T6.20.2.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S6.T6.2.1" style="font-size:90%;">Open-loop evaluation of models on the CoC dataset. The base model is pre-trained with <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.T6.2.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math> and all other models are finetuned on the CoC dataset, then evaluated on held-out CoC test data. Numbers with green background are the best under each setting.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T6.18">
<tr class="ltx_tr" id="S6.T6.6.4">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.6.4.5"><span class="ltx_text ltx_font_bold" id="S6.T6.6.4.5.1">ID</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T6.6.4.6"><span class="ltx_text ltx_font_bold" id="S6.T6.6.4.6.1">Model Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.6.4.7"><span class="ltx_text ltx_font_bold" id="S6.T6.6.4.7.1">Route</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.6.4.8"><span class="ltx_text ltx_font_bold" id="S6.T6.6.4.8.1">Parameters</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.4.2.2"><span class="ltx_text ltx_font_bold" id="S6.T6.4.2.2.2">minADE<sub class="ltx_sub" id="S6.T6.4.2.2.2.1"><span class="ltx_text ltx_font_medium" id="S6.T6.4.2.2.2.1.1">6</span></sub>@3s<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.4.2.2.2.m2" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.6.4.4"><span class="ltx_text ltx_font_bold" id="S6.T6.6.4.4.2">minADE<sub class="ltx_sub" id="S6.T6.6.4.4.2.1"><span class="ltx_text ltx_font_medium" id="S6.T6.6.4.4.2.1.1">6</span></sub>@6.4s<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.6.4.4.2.m2" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.7.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.7.5.2">1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T6.7.5.3">Base model (action modality)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.7.5.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.7.5.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.7.5.4">0.5B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.7.5.5">0.284</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.7.5.6">0.996</td>
</tr>
<tr class="ltx_tr" id="S6.T6.8.6">
<td class="ltx_td ltx_align_center" id="S6.T6.8.6.2">2</td>
<td class="ltx_td ltx_align_left" id="S6.T6.8.6.3">+ Ft. w/ Traj.</td>
<td class="ltx_td ltx_align_center" id="S6.T6.8.6.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.8.6.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.8.6.4">0.5B</td>
<td class="ltx_td ltx_align_center" id="S6.T6.8.6.5">0.282</td>
<td class="ltx_td ltx_align_center" id="S6.T6.8.6.6">0.971</td>
</tr>
<tr class="ltx_tr" id="S6.T6.9.7">
<td class="ltx_td ltx_align_center" id="S6.T6.9.7.2">3</td>
<td class="ltx_td ltx_align_left" id="S6.T6.9.7.3">+ Ft. w/ Meta-action &amp; Traj.</td>
<td class="ltx_td ltx_align_center" id="S6.T6.9.7.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.9.7.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.9.7.4">0.5B</td>
<td class="ltx_td ltx_align_center" id="S6.T6.9.7.5">0.291</td>
<td class="ltx_td ltx_align_center" id="S6.T6.9.7.6">0.988</td>
</tr>
<tr class="ltx_tr" id="S6.T6.10.8" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_center" id="S6.T6.10.8.2"><span class="ltx_text" id="S6.T6.10.8.2.1" style="--ltx-bg-color:#C5E096;">4</span></td>
<td class="ltx_td ltx_align_left" id="S6.T6.10.8.3"><span class="ltx_text" id="S6.T6.10.8.3.1" style="--ltx-bg-color:#C5E096;">+ Ft. w/ CoC &amp; Traj. (AR1)</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.10.8.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.10.8.1.m1" intent=":literal" style="--ltx-bg-color:#C5E096;"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.10.8.4"><span class="ltx_text" id="S6.T6.10.8.4.1" style="--ltx-bg-color:#C5E096;">0.5B</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.10.8.5"><span class="ltx_text" id="S6.T6.10.8.5.1" style="--ltx-bg-color:#C5E096;">0.279</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.10.8.6"><span class="ltx_text" id="S6.T6.10.8.6.1" style="--ltx-bg-color:#C5E096;">0.955</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.11.9">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.11.9.2">5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T6.11.9.3">Base model (action modality)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.11.9.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.11.9.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.11.9.4">3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.11.9.5">0.291</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.11.9.6">0.977</td>
</tr>
<tr class="ltx_tr" id="S6.T6.12.10">
<td class="ltx_td ltx_align_center" id="S6.T6.12.10.2">6</td>
<td class="ltx_td ltx_align_left" id="S6.T6.12.10.3">+ Ft. w/ Traj.</td>
<td class="ltx_td ltx_align_center" id="S6.T6.12.10.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.12.10.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.12.10.4">3B</td>
<td class="ltx_td ltx_align_center" id="S6.T6.12.10.5">0.293</td>
<td class="ltx_td ltx_align_center" id="S6.T6.12.10.6">0.976</td>
</tr>
<tr class="ltx_tr" id="S6.T6.13.11">
<td class="ltx_td ltx_align_center" id="S6.T6.13.11.2">7</td>
<td class="ltx_td ltx_align_left" id="S6.T6.13.11.3">+ Ft. w/ Meta-action &amp; Traj.</td>
<td class="ltx_td ltx_align_center" id="S6.T6.13.11.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.13.11.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.13.11.4">3B</td>
<td class="ltx_td ltx_align_center" id="S6.T6.13.11.5">0.280</td>
<td class="ltx_td ltx_align_center" id="S6.T6.13.11.6">0.927</td>
</tr>
<tr class="ltx_tr" id="S6.T6.14.12" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_center" id="S6.T6.14.12.2"><span class="ltx_text" id="S6.T6.14.12.2.1" style="--ltx-bg-color:#C5E096;">8</span></td>
<td class="ltx_td ltx_align_left" id="S6.T6.14.12.3"><span class="ltx_text" id="S6.T6.14.12.3.1" style="--ltx-bg-color:#C5E096;">+ Ft. w/ CoC &amp; Traj. (AR1)</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.12.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T6.14.12.1.m1" intent=":literal" style="--ltx-bg-color:#C5E096;"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.12.4"><span class="ltx_text" id="S6.T6.14.12.4.1" style="--ltx-bg-color:#C5E096;">3B</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.12.5"><span class="ltx_text" id="S6.T6.14.12.5.1" style="--ltx-bg-color:#C5E096;">0.275</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.14.12.6"><span class="ltx_text" id="S6.T6.14.12.6.1" style="--ltx-bg-color:#C5E096;">0.908</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.15.13">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.13.2">9</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T6.15.13.3">Base model (action modality)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.13.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T6.15.13.1.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.13.4">0.5B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.13.5">0.264</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.15.13.6">0.848</td>
</tr>
<tr class="ltx_tr" id="S6.T6.16.14">
<td class="ltx_td ltx_align_center" id="S6.T6.16.14.2">10</td>
<td class="ltx_td ltx_align_left" id="S6.T6.16.14.3">+ Ft. w/ Traj.</td>
<td class="ltx_td ltx_align_center" id="S6.T6.16.14.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T6.16.14.1.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.16.14.4">0.5B</td>
<td class="ltx_td ltx_align_center" id="S6.T6.16.14.5">0.262</td>
<td class="ltx_td ltx_align_center" id="S6.T6.16.14.6">0.834</td>
</tr>
<tr class="ltx_tr" id="S6.T6.17.15">
<td class="ltx_td ltx_align_center" id="S6.T6.17.15.2">11</td>
<td class="ltx_td ltx_align_left" id="S6.T6.17.15.3">+ Ft. w/ Meta-action &amp; Traj.</td>
<td class="ltx_td ltx_align_center" id="S6.T6.17.15.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T6.17.15.1.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T6.17.15.4">0.5B</td>
<td class="ltx_td ltx_align_center" id="S6.T6.17.15.5">0.264</td>
<td class="ltx_td ltx_align_center" id="S6.T6.17.15.6">0.821</td>
</tr>
<tr class="ltx_tr" id="S6.T6.18.16" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.18.16.2"><span class="ltx_text" id="S6.T6.18.16.2.1" style="--ltx-bg-color:#C5E096;">12</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T6.18.16.3"><span class="ltx_text" id="S6.T6.18.16.3.1" style="--ltx-bg-color:#C5E096;">+ Ft. w/ CoC &amp; Traj. (AR1)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.18.16.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T6.18.16.1.m1" intent=":literal" style="--ltx-bg-color:#C5E096;"><semantics><mi mathbackground="#C5E096" mathvariant="normal" style="--ltx-bg-color:#C5E096;">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.18.16.4"><span class="ltx_text" id="S6.T6.18.16.4.1" style="--ltx-bg-color:#C5E096;">0.5B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.18.16.5"><span class="ltx_text" id="S6.T6.18.16.5.1" style="--ltx-bg-color:#C5E096;">0.254</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.18.16.6"><span class="ltx_text" id="S6.T6.18.16.6.1" style="--ltx-bg-color:#C5E096;">0.794</span></td>
</tr>
</table>
</figure>
<figure class="ltx_table" id="S6.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T7.9.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S6.T7.10.2" style="font-size:90%;">Open-loop evaluation of models on the challenging dataset. All models are finetuned on the CoC dataset and evaluated on the challenging dataset.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T7.7">
<tr class="ltx_tr" id="S6.T7.4.4">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.4.4.5"><span class="ltx_text ltx_font_bold" id="S6.T7.4.4.5.1">ID</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T7.4.4.6"><span class="ltx_text ltx_font_bold" id="S6.T7.4.4.6.1">Model Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.4.4.7"><span class="ltx_text ltx_font_bold" id="S6.T7.4.4.7.1">Route</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.4.4.8"><span class="ltx_text ltx_font_bold" id="S6.T7.4.4.8.1">Parameters</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2">minADE<sub class="ltx_sub" id="S6.T7.2.2.2.2.1"><span class="ltx_text ltx_font_medium" id="S6.T7.2.2.2.2.1.1">6</span></sub>@3s<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T7.2.2.2.2.m2" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T7.4.4.4.2">minADE<sub class="ltx_sub" id="S6.T7.4.4.4.2.1"><span class="ltx_text ltx_font_medium" id="S6.T7.4.4.4.2.1.1">6</span></sub>@6.4s<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T7.4.4.4.2.m2" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.2">1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T7.5.5.3">Ft. w/ Traj.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T7.5.5.1.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.4">0.5B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5">0.315</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.6">0.994</td>
</tr>
<tr class="ltx_tr" id="S6.T7.6.6">
<td class="ltx_td ltx_align_center" id="S6.T7.6.6.2">2</td>
<td class="ltx_td ltx_align_left" id="S6.T7.6.6.3">Ft. w/ Meta-action &amp; Traj.</td>
<td class="ltx_td ltx_align_center" id="S6.T7.6.6.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T7.6.6.1.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S6.T7.6.6.4">0.5B</td>
<td class="ltx_td ltx_align_center" id="S6.T7.6.6.5">0.301</td>
<td class="ltx_td ltx_align_center" id="S6.T7.6.6.6">0.928</td>
</tr>
<tr class="ltx_tr" id="S6.T7.7.7" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.7.7.2"><span class="ltx_text" id="S6.T7.7.7.2.1" style="--ltx-bg-color:#C5E096;">3</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T7.7.7.3"><span class="ltx_text" id="S6.T7.7.7.3.1" style="--ltx-bg-color:#C5E096;">Ft. w/ CoC &amp; Traj. (AR1)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.7.7.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T7.7.7.1.m1" intent=":literal" style="--ltx-bg-color:#C5E096;"><semantics><mi mathbackground="#C5E096" mathvariant="normal" style="--ltx-bg-color:#C5E096;">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.7.7.4"><span class="ltx_text" id="S6.T7.7.7.4.1" style="--ltx-bg-color:#C5E096;">0.5B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.7.7.5"><span class="ltx_text" id="S6.T7.7.7.5.1" style="--ltx-bg-color:#C5E096;">0.290</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.7.7.6"><span class="ltx_text" id="S6.T7.7.7.6.1" style="--ltx-bg-color:#C5E096;">0.868</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="S6.F8.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S6.F8.3.2" style="font-size:90%;">Policy improvements via eliciting reasoning: Alpamayo-R1 generates a correct reasoning trace at an all-way stop sign intersection and yields to other vehicles that enter the intersection earlier than ego.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Our evaluation strategy consists of four complementary components:</p>
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I1.i1.p1.1.1">open-loop trajectory prediction</span> on both nominal and long-tail driving scenarios to measure planning accuracy;</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I1.i2.p1.1.1">closed-loop simulation</span> using AlpaSim <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib67" title="">2025b</a>)</cite> to assess safety and robustness when the model controls the vehicle in realistic scenarios;</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I1.i3.p1.1.1">ablation studies</span> examining the impact of key architectural choices, including vision-language model scaling, vision encoding strategies, reasoning integration, and action decoding strategies;</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I1.i4.p1.1.1">on-vehicle road tests</span> to validate real-world deployment of the model in autonomous driving scenarios.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.4"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.4.1">Dataset.</span> We train and evaluate models on internal driving data collected across diverse geographic regions in the US and EU, with all evaluation data strictly geo-fenced and held out from training regions to prevent information leakage.
Our evaluation encompasses both nominal driving scenarios in dataset <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.SS1.p2.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math> and challenging long-tail cases in <math alttext="\mathcal{D}_{\text{hard}}" class="ltx_Math" display="inline" id="S6.SS1.p2.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>hard</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{hard}}</annotation></semantics></math> to thoroughly test the model’s ability to handle rare, safety-critical events. In detail, the full training and evaluation dataset comprises 80,000 hours of driving data collected from multiple ego-vehicles operating in more than 2,500 cities in 25 countries. It encompasses diverse driving scenarios, including highway and urban environments, under various weather conditions, times of day, and traffic densities. The raw sensory inputs consist of video recordings from a surround-view seven-camera setup, accompanied by precise camera calibration parameters and ego-motion data. In this work, we focus on using two front-facing cameras as input: a front wide-angle camera with 120<sup class="ltx_sup" id="S6.SS1.p2.4.2"><span class="ltx_text ltx_font_italic" id="S6.SS1.p2.4.2.1">∘</span></sup> field of view and a front telephoto camera with 30<sup class="ltx_sup" id="S6.SS1.p2.4.3"><span class="ltx_text ltx_font_italic" id="S6.SS1.p2.4.3.1">∘</span></sup> field of view, providing complementary perspectives for both near-field and far-field scene understanding.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">In addition to the general driving dataset <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math>, we construct the CoC dataset (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S4" title="4 Chain of Causation Dataset: Learning Causally Grounded Reasoning VLAs ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">4</span></a>) consisting of 700K video segments with structured CoC. This dataset is used for fine-tuning models to elicit reasoning capabilities (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS2" title="6.2 Policy Improvements from Reasoning ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.2</span></a>) and for RL-based post-training alignment (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS3" title="6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.3</span></a>).</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Open-Loop Evaluation.</span>
For open-loop trajectory prediction, we evaluate models over a prediction horizon of 6.4 seconds, corresponding to the ego-vehicle’s planned waypoints. We use minADE and ADE as the evaluation metric. minADE is computed over 6 samples (<math alttext="\text{minADE}_{6}" class="ltx_Math" display="inline" id="S6.SS1.p4.1.m1" intent=":literal"><semantics><msub><mtext>minADE</mtext><mn>6</mn></msub><annotation encoding="application/x-tex">\text{minADE}_{6}</annotation></semantics></math>) and is defined as the minimum distance between the ground-truth future trajectory and the best-matching trajectory among 6 predictions generated by the model. ADE (Average Displacement Error) is the average distance between the predicted trajectory and the ground-truth trajectory across all future timesteps.</p>
</div>
<div class="ltx_para" id="S6.SS1.p5">
<p class="ltx_p" id="S6.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p5.1.1">Closed-Loop Evaluation.</span>
It is well established that strong open-loop results do not necessarily translate into reliable closed-loop driving performance <cite class="ltx_cite ltx_citemacro_citep">(Dauner et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib13" title="">2023</a>)</cite>. To address this gap, we further evaluate our models within <span class="ltx_text ltx_font_italic" id="S6.SS1.p5.1.2">AlpaSim</span> <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib67" title="">2025b</a>)</cite>, an open-source closed-loop end-to-end simulator based on state-of-the-art neural reconstruction technology <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib98" title="">2025b</a>)</cite>. AlpaSim leverages a temporal 3D Gaussian Splatting representation from recorded real-world driving logs and, during closed-loop evaluation, uses it to synthesize novel viewpoints when the ego vehicle deviates from the recorded trajectory. During evaluation, predicted trajectories are tracked by a model predictive controller (MPC), and vehicle dynamics follow a dynamically extended bicycle model. Traffic agents, including vehicles and pedestrians, follow their recorded trajectories.</p>
</div>
<div class="ltx_para" id="S6.SS1.p6">
<p class="ltx_p" id="S6.SS1.p6.1">We evaluate models in 75 challenging 20-second scenarios, selected for their dense ego–agent and agent–agent interactions. While this may appear as a limited set, these scenarios are specifically curated to represent the most demanding safety-critical situations requiring complex reasoning and interactive decision-making.
We report the following AlpaSim metrics:</p>
<ol class="ltx_enumerate" id="S6.I2">
<li class="ltx_item" id="S6.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S6.I2.i1.p1">
<p class="ltx_p" id="S6.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i1.p1.1.1">close encounter rate (all)</span>: percentage of scenarios where the ego vehicle experiences a close encounter with any other traffic agent;</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S6.I2.i2.p1">
<p class="ltx_p" id="S6.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i2.p1.1.1">close encounter rate (at-fault)</span>: same as close encounter rate but considering only close encounters where the ego vehicle is deemed responsible, i.e., excluding rear-end close encounters.</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S6.I2.i3.p1">
<p class="ltx_p" id="S6.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i3.p1.1.1">offroad rate</span>: percentage of scenarios where the ego vehicle drives outside of the drivable area;</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S6.I2.i4.p1">
<p class="ltx_p" id="S6.I2.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i4.p1.1.1">AlpaSim score (all)</span>: average distance driven in km between events, where events correspond to offroad or close encounter occurrences;</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span>
<div class="ltx_para" id="S6.I2.i5.p1">
<p class="ltx_p" id="S6.I2.i5.p1.1"><span class="ltx_text ltx_font_italic" id="S6.I2.i5.p1.1.1">AlpaSim score (at-fault)</span>: same as AlpaSim score but considering only close encounters where the ego vehicle is deemed responsible, i.e., excluding rear-end close encounters.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S6.SS1.p6.2">The simulation ends after the first close encounter or off-road event. To mitigate rendering artifacts, events in which the ego deviates more than 4 m from the original recorded trajectory are excluded from all metric computations.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Policy Improvements from Reasoning</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">One of the key contributions of this work is the use of the proposed CoC data to improve driving policies.
To evaluate the impact of reasoning on driving performance, we start with a base model pre-trained on <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math> with action modality injection (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS1" title="5.1 Action Modality Injection ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.1</span></a>), then fine-tune it on the CoC dataset with different reasoning modalities: meta-action descriptions and full chain-of-causation reasoning traces. During inference, models trained with CoC reasoning generate explicit reasoning outputs alongside trajectory predictions, enabling them to better handle challenging scenarios that require multi-step decision making.
We compare three fine-tuning strategies: (1) trajectory prediction only, (2) meta-action and trajectory prediction, and (3) chain-of-causation reasoning and trajectory prediction (Alpamayo-R1). All models are evaluated on held-out CoC test data in two settings: with and without route information provided to the model.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Open-Loop Improvements.</span> As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T6" title="In 6.1 Evaluation Protocol ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">6</span></a> (nominal scenarios) and <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T7" title="In 6.1 Evaluation Protocol ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">7</span></a> (challenging scenarios), incorporating CoC reasoning yields substantial improvements in open-loop trajectory prediction in both settings. Without route information, AR1 achieves a minADE<sub class="ltx_sub" id="S6.SS2.p2.1.2">6</sub> of 0.955m at 6.4s, a 4.1% improvement over the base model and outperforming both trajectory-only (0.971m) and meta-action (0.988m) baselines.
With route information, the gains are more pronounced: AR1 achieves 0.794m, representing 4.8% improvement over the trajectory-only baseline (0.834m). Scaling to 3B parameters further improves performance, with AR1-3B achieving 0.908m (without route), demonstrating the benefits of increased model capacity for complex reasoning tasks. In challenging scenarios, the improvements are even larger, with AR1 achieving 0.868m, a <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.3">12% improvement</span> over the trajectory-only baseline (0.994m).</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">These results demonstrate that explicit reasoning capabilities enable the model to more effectively leverage contextual information such as route guidance and handle complex driving scenarios that require anticipating future interactions. <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F8" title="In 6.1 Evaluation Protocol ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">8</span></a> illustrates qualitative examples where the CoC-enabled model successfully generates correct reasoning traces and yields to vehicles in challenging scenarios, while baseline models fail to anticipate these interactions.</p>
</div>
<figure class="ltx_figure" id="S6.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="452" id="S6.F9.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S6.F9.3.2" style="font-size:90%;">Examples of closed-loop evaluation in AlpaSim <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib67" title="">2025b</a>)</cite>. The top row presents an intersection scenario, whereas the bottom row illustrates a construction scenario.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p4.1.1">Closed-Loop Improvements.</span>
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T8" title="In 6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">8</span></a>, AR1 achieves a 35% reduction in close encounter rate (11% vs 17%) compared to the trajectory-only baseline, and a comparable off-road rate (4% vs 3%). The overall AlpaSim score improves from 0.38 to 0.50, demonstrating that reasoning-based decision making improves safety in dynamic closed-loop scenarios. <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F9" title="In 6.2 Policy Improvements from Reasoning ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">9</span></a> presents two qualitative examples that demonstrate that our model can successfully perform closed-loop driving in challenging scenarios within AlpaSim.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Improvements of Reasoning, Consistency, and Safety via RL Post-Training</h3>
<figure class="ltx_table" id="S6.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T8.17.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S6.T8.18.2" style="font-size:90%;">Closed-loop evaluation results in AlpaSim <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib67" title="">2025b</a>)</cite>. All models are evaluated without route information across 75 challenging scenarios. Baseline refers to the trajectory-only model fine-tuned on CoC training data without reasoning.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T8.15" style="width:433.6pt;height:36.9pt;vertical-align:-16.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-72.0pt,6.1pt) scale(0.750648879985334,0.750648879985334) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T8.15.15">
<tr class="ltx_tr" id="S6.T8.15.15.16">
<td class="ltx_td ltx_border_tt" id="S6.T8.15.15.16.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T8.15.15.16.2"><span class="ltx_text ltx_font_bold" id="S6.T8.15.15.16.2.1">Close Encounter Rate</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T8.15.15.16.3"><span class="ltx_text ltx_font_bold" id="S6.T8.15.15.16.3.1">Close Encounter Rate</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T8.15.15.16.4"><span class="ltx_text ltx_font_bold" id="S6.T8.15.15.16.4.1">Off-Road Rate</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T8.15.15.16.5"><span class="ltx_text ltx_font_bold" id="S6.T8.15.15.16.5.1">AlpaSim Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T8.15.15.16.6"><span class="ltx_text ltx_font_bold" id="S6.T8.15.15.16.6.1">AlpaSim Score</span></td>
</tr>
<tr class="ltx_tr" id="S6.T8.5.5.5">
<td class="ltx_td ltx_align_left" id="S6.T8.5.5.5.6"><span class="ltx_text ltx_font_bold" id="S6.T8.5.5.5.6.1">Model</span></td>
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.1">all (%) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T8.1.1.1.1.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S6.T8.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T8.2.2.2.2.1">at-fault (%) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T8.2.2.2.2.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S6.T8.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S6.T8.3.3.3.3.1">(%) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T8.3.3.3.3.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S6.T8.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T8.4.4.4.4.1">all <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T8.4.4.4.4.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S6.T8.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S6.T8.5.5.5.5.1">at-fault <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T8.5.5.5.5.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S6.T8.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.10.10.10.6">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.6.6.6.1">17.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.6.6.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>3.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.7.7.7.2">6.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.7.7.7.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>1.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.8.8.8.3">3.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.8.8.8.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>2.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.9.9.9.4">0.38<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.9.9.9.4.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T8.10.10.10.5">0.86<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.10.10.10.5.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.11</td>
</tr>
<tr class="ltx_tr" id="S6.T8.15.15.15" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T8.15.15.15.6"><span class="ltx_text" id="S6.T8.15.15.15.6.1" style="--ltx-bg-color:#C5E096;">Alpamayo-R1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.11.11.11.1"><span class="ltx_text" id="S6.T8.11.11.11.1.1" style="--ltx-bg-color:#C5E096;">11.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.11.11.11.1.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>2.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.12.12.12.2"><span class="ltx_text" id="S6.T8.12.12.12.2.1" style="--ltx-bg-color:#C5E096;">5.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.12.12.12.2.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>3.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.13.13.13.3"><span class="ltx_text" id="S6.T8.13.13.13.3.1" style="--ltx-bg-color:#C5E096;">4.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.13.13.13.3.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>3.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.14.14.14.4"><span class="ltx_text" id="S6.T8.14.14.14.4.1" style="--ltx-bg-color:#C5E096;">0.50<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.14.14.14.4.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T8.15.15.15.5"><span class="ltx_text" id="S6.T8.15.15.15.5.1" style="--ltx-bg-color:#C5E096;">0.87<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T8.15.15.15.5.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.18</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">While SFT on CoC data enables the model to jointly generate reasoning traces and actions, it does not guarantee that these traces are causally grounded or that the resulting actions faithfully reflect the reasoning or align with human driving norms.
To address this gap, we apply RL-based post-training to simultaneously improve reasoning quality, reasoning-action consistency, and trajectory quality (see <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3</span></a> for methodology details).
In this section, we post-train a 0.5B AR1 model fine-tuned on CoC data, and demonstrate the impact of different reward components on model behavior.</p>
</div>
<figure class="ltx_figure" id="S6.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="377" id="S6.F10.g1" src="x9.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F10.4.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S6.F10.5.2" style="font-size:90%;">Post-training with the reasoning reward improves causal understanding and contextual reasoning in driving scenarios. <span class="ltx_text ltx_font_bold" id="S6.F10.5.2.1">Left</span>: The base model overlooks construction barriers and fails to initiate evasive action, while the post-trained model correctly reasons that the ego should nudge right to avoid obstacles. <span class="ltx_text ltx_font_bold" id="S6.F10.5.2.2">Right</span>: The base model misses that pedestrians are clearing the path, whereas the post-trained model correctly reasons that it is safe for the ego vehicle to accelerate.</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">The Value of Learning from LRM Feedback</span>. To ensure that the model’s reasoning traces are not only fluent but also causally grounded and contextually accurate, we introduce a reasoning reward derived from LRM feedback (more details are in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3" title="5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3</span></a>). This reward provides a continuous evaluation signal that measures the logical consistency and causal correctness of each generated reasoning trace with respect to the driving scene. Specifically, the average reasoning score of the most-likely rollout among six generations improves by approximately 45% (3.1<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS3.p2.1.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>4.5) when the reasoning reward is applied.
In <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F10" title="In 6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">10</span></a>, we illustrate two qualitative examples showcasing the model’s behavioral differences before and after post-training. In the left scenario, the ego vehicle approaches a construction site. The most-likely mode generated by the SFT-pretrained model overlooks the construction barriers and describes the scene as a normal driving situation, failing to recognize the need for evasive behavior. After post-training, however, the model’s reasoning correctly attends to the construction area and explains that the ego vehicle should nudge right to avoid obstacles. Similarly, in the right scenario, two pedestrians are about to clear the path. The most-likely mode generated by the SFT-pretrained model overlooks this contextual cue and fails to anticipate that the ego vehicle should prepare to accelerate. After post-training, the model correctly recognizes that the pedestrians are exiting the drivable area and reasons that it is safe for the ego vehicle to resume motion.</p>
</div>
<figure class="ltx_figure" id="S6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="S6.F11.g1" src="x10.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F11.4.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S6.F11.5.2" style="font-size:90%;">Post-training with the reasoning–action consistency reward improves motion fidelity.
Grey motion denotes the most-likely rollout from the SFT-pretrained base model, and green motion denotes the most-likely rollout from the post-trained model. The orange motions denote the obstacles’ motion replay.
<span class="ltx_text ltx_font_bold" id="S6.F11.5.2.1">Left</span>: The base model (grey) stops halfway and fails to resume motion, even though its reasoning trace correctly instructs the ego vehicle to accelerate after stopping. The post-trained model (green) executes the full causal sequence: decelerating, stopping, and accelerating once the intersection is clear. <span class="ltx_text ltx_font_bold" id="S6.F11.5.2.2">Right</span>: When the reasoning instructs the ego vehicle to follow a lead vehicle, the post-trained model’s generated motion maintains appropriate speed and lane position in accordance with its reasoning trace (“accelerating and keeping lane”), whereas the base model’s generated motion changes the lane, drifting from the intended plan.</span></figcaption>
</figure>
<figure class="ltx_table" id="S6.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T9.13.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S6.T9.14.2" style="font-size:90%;">Improvements from RL-based post-training. We evaluate the impact of RL-based post-training on the model’s reasoning, consistency, and motion quality. Metrics are computed from the most-likely rollout among six generated rollouts to assess how RL alignment influences the model’s generation distribution. We measure ADE, reasoning quality graded by the large reasoning critic (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S5.SS3.SSS2" title="5.3.2 Reward Model ‣ 5.3 RL-based Post-Training ‣ 5 Training Strategy ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">5.3.2</span></a>), reasoning–action consistency, and close encounter rate. Evaluations are conducted on the full CoC dataset introduced in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS2" title="6.2 Policy Improvements from Reasoning ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.2</span></a>. We compare four configurations: the SFT-pretrained base model and three RL post-training variants incorporating different combinations of reasoning, consistency, and safety rewards.
</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T9.11" style="width:345.0pt;height:29.4pt;vertical-align:-13.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-186.2pt,15.9pt) scale(0.480945380726623,0.480945380726623) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T9.11.11">
<tr class="ltx_tr" id="S6.T9.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T9.5.5.5.6"><span class="ltx_text ltx_font_bold" id="S6.T9.5.5.5.6.1">Training strategy</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T9.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S6.T9.1.1.1.1.1">ADE</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T9.1.1.1.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T9.2.2.2.2">
<span class="ltx_text ltx_font_bold" id="S6.T9.2.2.2.2.1">Reasoning Grading</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T9.2.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T9.3.3.3.3">
<span class="ltx_text ltx_font_bold" id="S6.T9.3.3.3.3.1">Reasoning–Action Consistency Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T9.3.3.3.3.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T9.5.5.5.5">
<span class="ltx_text ltx_font_bold" id="S6.T9.4.4.4.4.1">Close Encounter Rate (<math alttext="\%" class="ltx_Math" display="inline" id="S6.T9.4.4.4.4.1.m1" intent=":literal"><semantics><mo>%</mo><annotation encoding="application/x-tex">\%</annotation></semantics></math>)</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T9.5.5.5.5.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.11.11.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T9.11.11.12.1">SFT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T9.11.11.12.2">2.12m</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T9.11.11.12.3">3.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T9.11.11.12.4">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T9.11.11.12.5">6.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.6.6.6">
<td class="ltx_td ltx_align_left" id="S6.T9.6.6.6.1">SFT + RL (<math alttext="r_{\text{reason}}" class="ltx_Math" display="inline" id="S6.T9.6.6.6.1.m1" intent=":literal"><semantics><msub><mi>r</mi><mtext>reason</mtext></msub><annotation encoding="application/x-tex">r_{\text{reason}}</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S6.T9.6.6.6.2">2.19m</td>
<td class="ltx_td ltx_align_center" id="S6.T9.6.6.6.3">4.5</td>
<td class="ltx_td ltx_align_center" id="S6.T9.6.6.6.4">0.53</td>
<td class="ltx_td ltx_align_center" id="S6.T9.6.6.6.5">5.8</td>
</tr>
<tr class="ltx_tr" id="S6.T9.8.8.8">
<td class="ltx_td ltx_align_left" id="S6.T9.8.8.8.2">SFT + RL (<math alttext="r_{\text{reason}}" class="ltx_Math" display="inline" id="S6.T9.7.7.7.1.m1" intent=":literal"><semantics><msub><mi>r</mi><mtext>reason</mtext></msub><annotation encoding="application/x-tex">r_{\text{reason}}</annotation></semantics></math> + <math alttext="r_{\text{consistency}}" class="ltx_Math" display="inline" id="S6.T9.8.8.8.2.m2" intent=":literal"><semantics><msub><mi>r</mi><mtext>consistency</mtext></msub><annotation encoding="application/x-tex">r_{\text{consistency}}</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S6.T9.8.8.8.3">1.92m</td>
<td class="ltx_td ltx_align_center" id="S6.T9.8.8.8.4">4.5</td>
<td class="ltx_td ltx_align_center" id="S6.T9.8.8.8.5">0.85</td>
<td class="ltx_td ltx_align_center" id="S6.T9.8.8.8.6">6.2</td>
</tr>
<tr class="ltx_tr" id="S6.T9.11.11.11" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T9.11.11.11.3"><span class="ltx_text" id="S6.T9.11.11.11.3.3" style="--ltx-bg-color:#C5E096;">SFT + RL (<math alttext="r_{\text{reason}}" class="ltx_Math" display="inline" id="S6.T9.9.9.9.1.1.m1" intent=":literal"><semantics><msub><mi mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">r</mi><mtext mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">reason</mtext></msub><annotation encoding="application/x-tex">r_{\text{reason}}</annotation></semantics></math> + <math alttext="r_{\text{consistency}}" class="ltx_Math" display="inline" id="S6.T9.10.10.10.2.2.m2" intent=":literal"><semantics><msub><mi mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">r</mi><mtext mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">consistency</mtext></msub><annotation encoding="application/x-tex">r_{\text{consistency}}</annotation></semantics></math> + <math alttext="r_{\text{safety}}" class="ltx_Math" display="inline" id="S6.T9.11.11.11.3.3.m3" intent=":literal"><semantics><msub><mi mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">r</mi><mtext mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">safety</mtext></msub><annotation encoding="application/x-tex">r_{\text{safety}}</annotation></semantics></math>)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T9.11.11.11.4"><span class="ltx_text" id="S6.T9.11.11.11.4.1" style="--ltx-bg-color:#C5E096;">1.94m</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T9.11.11.11.5"><span class="ltx_text" id="S6.T9.11.11.11.5.1" style="--ltx-bg-color:#C5E096;">4.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T9.11.11.11.6"><span class="ltx_text" id="S6.T9.11.11.11.6.1" style="--ltx-bg-color:#C5E096;">0.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T9.11.11.11.7"><span class="ltx_text" id="S6.T9.11.11.11.7.1" style="--ltx-bg-color:#C5E096;">3.7</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.3"><span class="ltx_text ltx_font_bold" id="S6.SS3.p3.3.1">The Value of Enforcing Reasoning-Action Consistency</span>. Interestingly, when the post-training stage optimizes solely for the reasoning reward, the reasoning score indeed improves; however, both the ADE metric and reasoning–action consistency degrade compared to the base model. This indicates that optimizing for reasoning quality alone can lead to ungrounded or overconfident reasoning, where the model produces fluent but causally disconnected explanations that fail to translate into coherent actions. The consistency reward is therefore crucial for anchoring reasoning to physically realizable behaviors, ensuring that improvements in interpretability do not come at the expense of control fidelity. Specifically, when jointly optimizing both the reasoning and consistency rewards, the post-trained model achieves a 9.4% reduction in most-likely mode ADE (2.12m<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS3.p3.1.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>1.92m), a 45% improvement in the reasoning score (3.1<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS3.p3.2.m2" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>4.5), and a 37% increase in reasoning–action consistency (0.62<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS3.p3.3.m3" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>0.85).
These results demonstrate that the two reward components are complementary: the reasoning reward enhances interpretability and causal grounding, while the consistency reward ensures that the generated reasoning translates into faithful and more accurate motion behaviors.
In <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F11" title="In 6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">11</span></a>, we present two qualitative examples illustrating how post-training improves the model’s motion fidelity. When the model reasons “decelerate, stop, and then accelerate at a stop sign,” the aligned model produces actions that faithfully follow this causal sequence (decelerating smoothly, coming to a complete stop, and accelerating only once the intersection is clear), whereas the SFT-pretrained model tends to stop halfway and never resume motion.</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p4.1.1">The Value of Imposing a Safety Reward</span>. While reasoning and consistency rewards improve interpretability and causal grounding, they do not explicitly constrain the model to produce safe motion trajectories. To ensure physical safety, we introduce a safety reward that penalizes unsafe or physically implausible trajectories during post-training. Empirically, adding the safety reward further reduces the close encounter rate and stabilizes trajectory generation without compromising reasoning quality. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T9" title="In 6.3 Improvements of Reasoning, Consistency, and Safety via RL Post-Training ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">9</span></a>, the full reward configuration achieves the lowest close encounter rate while maintaining improvements in ADE and reasoning–action consistency.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Public Benchmark Evaluation</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">To enable reproducible evaluation and community comparison, we evaluate the publicly released Alpamayo-R1 model on the PhysicalAI-AV dataset <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib63" title="">2025b</a>)</cite> and the AlpaSim public scenario set <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib67" title="">2025b</a>)</cite>. Alpamayo-R1-10B leverages Cosmos-Reason <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib66" title="">2025a</a>)</cite> as the VLM backbone with a 2B parameter diffusion-based trajectory decoder, while Alpamayo-R1-0.5B uses a smaller backbone for comparison. Both models generate Chain-of-Causation reasoning traces alongside trajectory predictions.</p>
</div>
<figure class="ltx_table" id="S6.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T10.13.1.1" style="font-size:90%;">Table 10</span>: </span><span class="ltx_text" id="S6.T10.14.2" style="font-size:90%;">Evaluation of Alpamayo-R1 on the public PhysicalAI-AV dataset <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib63" title="">2025b</a>)</cite>. Open-loop results are evaluated on 644 examples from the PhysicalAI-AV evaluation set. Closed-loop results are evaluated on 920 scenarios from the PhysicalAI-AV NuRec dataset <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib64" title="">2025c</a>)</cite> using AlpaSim <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib67" title="">2025b</a>)</cite>. All models predict CoC reasoning traces and trajectories. Closed-loop metrics are at-fault.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T10.11" style="width:433.6pt;height:38.9pt;vertical-align:-17.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.0pt,5.4pt) scale(0.783285539526651,0.783285539526651) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T10.11.11">
<tr class="ltx_tr" id="S6.T10.11.11.12">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T10.11.11.12.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S6.T10.11.11.12.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.11.11.12.2"><span class="ltx_text ltx_font_bold" id="S6.T10.11.11.12.2.1">Open-Loop</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T10.11.11.12.3"><span class="ltx_text ltx_font_bold" id="S6.T10.11.11.12.3.1">Closed-Loop (AlpaSim)</span></td>
</tr>
<tr class="ltx_tr" id="S6.T10.5.5.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T10.2.2.2.2.2">minADE<sub class="ltx_sub" id="S6.T10.2.2.2.2.2.1"><span class="ltx_text ltx_font_medium" id="S6.T10.2.2.2.2.2.1.1">6</span></sub>@6.4s<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T10.2.2.2.2.2.m2" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S6.T10.3.3.3.3.1">Close Encounter Rate <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T10.3.3.3.3.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math> (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T10.4.4.4.4.1">Off-Road Rate <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T10.4.4.4.4.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math> (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S6.T10.5.5.5.5.1">AlpaSim Score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T10.5.5.5.5.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S6.T10.8.8.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.8.8.8.4">Alpamayo-R1-0.5B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.8.8.8.5">0.913</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.6.6.6.1">9.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T10.6.6.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>1.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.7.7.7.2">19.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T10.7.7.7.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.8.8.8.3">0.35<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T10.8.8.8.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.01</td>
</tr>
<tr class="ltx_tr" id="S6.T10.11.11.11" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S6.T10.11.11.11.4"><span class="ltx_text" id="S6.T10.11.11.11.4.1" style="--ltx-bg-color:#C5E096;">Alpamayo-R1-10B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T10.11.11.11.5"><span class="ltx_text" id="S6.T10.11.11.11.5.1" style="--ltx-bg-color:#C5E096;">0.849</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.9.9.9.1"><span class="ltx_text" id="S6.T10.9.9.9.1.1" style="--ltx-bg-color:#C5E096;">4.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T10.9.9.9.1.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.10.10.10.2"><span class="ltx_text" id="S6.T10.10.10.10.2.1" style="--ltx-bg-color:#C5E096;">16.0<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T10.10.10.10.2.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T10.11.11.11.3"><span class="ltx_text" id="S6.T10.11.11.11.3.1" style="--ltx-bg-color:#C5E096;">0.72<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T10.11.11.11.3.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math>0.02</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS4.p2.1.1">Open-Loop Results.</span> We evaluate both models on 644 held-out examples from the PhysicalAI-AV dataset. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T10" title="In 6.4 Public Benchmark Evaluation ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">10</span></a>, Alpamayo-R1-10B achieves a minADE<sub class="ltx_sub" id="S6.SS4.p2.1.2">6</sub> of 0.849m at 6.4s, a 7.0% improvement over Alpamayo-R1-0.5B (0.913m), demonstrating the benefits of scaling the VLM backbone.</p>
</div>
<div class="ltx_para" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS4.p3.1.1">Closed-Loop Results.</span> We further evaluate both models on the AlpaSim public scenario set comprising 920 challenging driving scenarios. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T10" title="In 6.4 Public Benchmark Evaluation ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">10</span></a>, Alpamayo-R1-10B achieves substantial improvements across all closed-loop metrics: a 16% reduction in off-road rate (16% vs 19%),
a 55% reduction in close encounter rate (4% vs 9%),
and more than 2<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS4.p3.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> improvement in AlpaSim score (0.72 vs 0.35). These results demonstrate that scaling model capacity significantly enhances the model’s ability to handle complex driving scenarios in closed-loop simulation.</p>
</div>
<figure class="ltx_figure" id="S6.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="285" id="S6.F12.g1" src="x11.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F12.4.2.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S6.F12.2.1" style="font-size:90%;">Impact of VLM backbone size on open-loop driving performance. All models are evaluated on <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.F12.2.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math> with the same training data and hyperparameters.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Ablation: VLM Backbone Selection</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">The choice of VLM backbone is critical for Alpamayo-R1’s performance. In this section, we investigate two complementary aspects: the impact of model scale and the benefits of Physical-AI-focused pre-training. Together, these ablations demonstrate that both model capacity and domain-relevant pre-training are essential for strong driving performance.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.1 </span>Model Size Ablation</h4>
<div class="ltx_para" id="S6.SS5.SSS1.p1">
<p class="ltx_p" id="S6.SS5.SSS1.p1.2">To investigate the impact of model capacity on driving performance, we first conduct baseline scaling experiments using general-purpose VLMs. Specifically, we evaluate three variants of our architecture with different backbone sizes: 0.5B, 3B, and 7B parameters. The 0.5B model uses a DINOv2 <cite class="ltx_cite ltx_citemacro_citep">(Oquab et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib70" title="">2023</a>)</cite> vision encoder combined with the Qwen2.5-0.5B <cite class="ltx_cite ltx_citemacro_citep">(Qwen Team, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib76" title="">2024</a>)</cite> language model, while the 3B and 7B models leverage Qwen2.5-VL-3B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib3" title="">2025</a>)</cite> and Qwen2.5-VL-7B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib3" title="">2025</a>)</cite>, respectively. For this ablation study, all variants are trained on identical data with a reduced training budget compared to our main models, and evaluated on <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.SS5.SSS1.p1.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math> held-out test set without route information, using the minADE<sub class="ltx_sub" id="S6.SS5.SSS1.p1.2.1">6</sub> metric over a 6.4s horizon.</p>
</div>
<div class="ltx_para" id="S6.SS5.SSS1.p2">
<p class="ltx_p" id="S6.SS5.SSS1.p2.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F12" title="In 6.4 Public Benchmark Evaluation ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">12</span></a>, we observe consistent improvements in open-loop performance as model size increases. The 7B model achieves a reduction of 11% in minADE<sub class="ltx_sub" id="S6.SS5.SSS1.p2.1.1">6</sub> compared to the baseline of 0.5B, demonstrating that scaling the vision-language backbone enables better scene understanding and trajectory prediction. While these results confirm the importance of model capacity, they are based on general-purpose VLMs without domain-specific pre-training. As we demonstrate in  <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS5.SSS3" title="6.5.3 Cosmos-Reason Physical AI Capabilities ‣ 6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.5.3</span></a>, incorporating Physical AI-focused pre-training (via Cosmos-Reason, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.SS5.SSS3" title="6.5.3 Cosmos-Reason Physical AI Capabilities ‣ 6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">6.5.3</span></a>) yields substantial further improvements, which is why our final Alpamayo-R1 models adopt Cosmos-Reason as their backbone.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.2 </span>Data Scaling</h4>
<div class="ltx_para" id="S6.SS5.SSS2.p1">
<p class="ltx_p" id="S6.SS5.SSS2.p1.1">Complementary to model scaling, we investigate how training data scale affects driving performance when model architecture and training budget are held constant. We train the 0.5B model on varying amounts of data: 100k, 200k, 500k, 1M, and 2M video segments, keeping the total number of training steps fixed across all experiments.</p>
</div>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S6.F13.g1" src="x12.png" width="580"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F13.4.2.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="S6.F13.2.1" style="font-size:90%;">Impact of training data scale on open-loop driving performance. All models use the 0.5B architecture with identical hyperparameters and fixed total training steps. Models are evaluated on <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.F13.2.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math> held-out test set.
</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS5.SSS2.p2">
<p class="ltx_p" id="S6.SS5.SSS2.p2.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F13" title="In 6.5.2 Data Scaling ‣ 6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">13</span></a>, performance consistently improves with increased data scale, demonstrating the value of data diversity for autonomous driving. The 100k model exhibits clear overfitting (1.111m without early stopping; 1.016m with early stopping). Scaling to 500k achieves 0.880m (13.4% improvement over 100k), while 2M achieves the best performance at 0.874m (14.0% improvement). These results, together with the model size ablation in the previous subsection, demonstrate that both model capacity and data scale are effective dimensions for improving driving performance, underscoring their complementary roles in achieving robust autonomous driving systems.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.3 </span>Cosmos-Reason Physical AI Capabilities</h4>
<div class="ltx_para" id="S6.SS5.SSS3.p1">
<p class="ltx_p" id="S6.SS5.SSS3.p1.1">While the scaling experiments above demonstrate the importance of model capacity, they do not address a critical question: given a fixed model size, does domain-specific pre-training matter? As described in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3" title="3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>, Alpamayo-R1 adopts Cosmos-Reason <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib66" title="">2025a</a>)</cite> as its VLM backbone, specifically post-trained on Physical AI data including driving scenarios. To validate this architectural choice and demonstrate that Physical-AI-focused pre-training enhances driving-specific understanding beyond what scale alone provides, we evaluate Cosmos-Reason against comparable 7B-scale general-purpose VLMs on public driving benchmarks.</p>
</div>
<div class="ltx_para" id="S6.SS5.SSS3.p2">
<p class="ltx_p" id="S6.SS5.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS5.SSS3.p2.1.1">LingoQA Benchmark.</span> <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T11" title="In 6.5.3 Cosmos-Reason Physical AI Capabilities ‣ 6.5 Ablation: VLM Backbone Selection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">11</span></a> presents zero-shot evaluation results on the LingoQA benchmark <cite class="ltx_cite ltx_citemacro_citep">(Marcu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib59" title="">2024</a>)</cite>, which assesses vision-language models on driving scene understanding. Our Cosmos-Reason-7B model achieves 66.2% accuracy, outperforming various VLMs including GPT-4V (59.6%), Qwen2-VL-7B (52.6%), Qwen2.5-VL-7B (62.2%), InternVL3.5-8B (58.6%), and DeepSeek-VL-7B (46.4%). This improvement over the baselines demonstrates that Physical-AI-focused SFT significantly improves scene understanding capabilities for autonomous driving contexts, complementing the benefits of model scaling shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F12" title="In 6.4 Public Benchmark Evaluation ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure class="ltx_table" id="S6.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T11.2.1.1" style="font-size:90%;">Table 11</span>: </span><span class="ltx_text" id="S6.T11.3.2" style="font-size:90%;">Zero-shot accuracy of various VLMs on the LingoQA benchmark <cite class="ltx_cite ltx_citemacro_citep">(Marcu et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib59" title="">2024</a>)</cite>. Our Cosmos-Reason-7B model outperforms all baselines.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T11.4" style="width:411.9pt;height:22pt;vertical-align:-8.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.9pt,1.6pt) scale(0.873338975998594,0.873338975998594) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T11.4.1">
<tr class="ltx_tr" id="S6.T11.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T11.4.1.1.1">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T11.4.1.1.2">GPT-4V</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T11.4.1.1.3">Qwen2-VL-7B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T11.4.1.1.4">Qwen2.5-VL-7B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T11.4.1.1.5">InternVL3.5-8B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T11.4.1.1.6">DeepSeek-VL-7B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T11.4.1.1.7">Ours</td>
</tr>
<tr class="ltx_tr" id="S6.T11.4.1.2">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S6.T11.4.1.2.1">Lingo-Judge</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T11.4.1.2.2">59.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T11.4.1.2.3">52.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T11.4.1.2.4">62.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T11.4.1.2.5">58.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T11.4.1.2.6">46.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T11.4.1.2.7"><span class="ltx_text ltx_font_bold" id="S6.T11.4.1.2.7.1">66.2</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S6.SS5.SSS3.p3">
<p class="ltx_p" id="S6.SS5.SSS3.p3.1">These results confirm that <span class="ltx_text ltx_font_italic" id="S6.SS5.SSS3.p3.1.1">both</span> model capacity <span class="ltx_text ltx_font_italic" id="S6.SS5.SSS3.p3.1.2">and</span> domain-specific pre-training are essential for strong driving performance. This motivates our choice of Cosmos-Reason as the backbone for Alpamayo-R1, providing a strong foundation with Physical AI capabilities that general-purpose VLMs may otherwise not have.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Ablation: Action Modality Injection</h3>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">We demonstrate the effectiveness of adopting a continuous action representation governed by unicycle dynamics with flow matching in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T12" title="In 6.6 Ablation: Action Modality Injection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">12</span></a>.
Specifically, we compare a baseline model trained to auto-regressively predict 6 discrete trajectory tokens against a model of identical size and training data that decodes trajectories via flow matching.
The discrete trajectory tokenizer in the baseline auto-regressive model is pre-trained via VQGAN <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib19" title="">2021</a>)</cite>, which minimizes the number of output discrete tokens to reduce the auto-regressive decoding latency while maintaining low reconstruction error.
During inference, we set <math alttext="\delta_{t}=0.2" class="ltx_Math" display="inline" id="S6.SS6.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>δ</mi><mi>t</mi></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">\delta_{t}=0.2</annotation></semantics></math>, i.e., 5 steps, in flow matching to reduce latency with negligible performance degradation.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T12" title="In 6.6 Ablation: Action Modality Injection ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">12</span></a>, leveraging a dynamically governed continuous action space through flow-matching yields substantial improvements in both open-loop and closed-loop metrics, enhancing comfort and achieving faster inference speed.</p>
</div>
<figure class="ltx_table" id="S6.T12">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T12.13.2.1" style="font-size:90%;">Table 12</span>: </span><span class="ltx_text" id="S6.T12.2.1" style="font-size:90%;">Comparison on trajectory decoding strategies. The models are trained and evaluated with route signals.
The evaluation is on <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.T12.2.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math> to show overall gains.
Comfort (Accel) metric measures the percentage of predicted trajectories that are within a comfort range.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T12.11" style="width:433.6pt;height:29.8pt;vertical-align:-12.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.2pt,3.7pt) scale(0.800150334462644,0.800150334462644) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T12.11.9">
<tr class="ltx_tr" id="S6.T12.7.5.5">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T12.7.5.5.6"><span class="ltx_text ltx_font_bold" id="S6.T12.7.5.5.6.1">Strategy</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T12.4.2.2.2">
<math alttext="\text{minADE}_{6}" class="ltx_Math" display="inline" id="S6.T12.3.1.1.1.m1" intent=":literal"><semantics><msub><mtext class="ltx_mathvariant_bold">minADE</mtext><mn>6</mn></msub><annotation encoding="application/x-tex">\text{minADE}_{6}</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.T12.4.2.2.2.1">@6.4s</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T12.4.2.2.2.m2" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T12.5.3.3.3">
<span class="ltx_text ltx_font_bold" id="S6.T12.5.3.3.3.1">AlpaSim Score (at fault)</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T12.5.3.3.3.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T12.6.4.4.4">
<span class="ltx_text ltx_font_bold" id="S6.T12.6.4.4.4.1">Comfort (Accel)</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T12.6.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T12.7.5.5.5"><span class="ltx_text ltx_font_bold" id="S6.T12.7.5.5.5.1">Rel. Decode Speed<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T12.7.5.5.5.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S6.T12.9.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T12.9.7.7.3">Auto-Regressive</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.9.7.7.4">0.6811</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.8.6.6.1">0.59 <math alttext="\pm" class="ltx_Math" display="inline" id="S6.T12.8.6.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.9.7.7.5">44.05%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.9.7.7.2">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S6.T12.9.7.7.2.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S6.T12.11.9.9" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T12.11.9.9.3"><span class="ltx_text" id="S6.T12.11.9.9.3.1" style="--ltx-bg-color:#C5E096;">Flow Matching</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.11.9.9.4"><span class="ltx_text" id="S6.T12.11.9.9.4.1" style="--ltx-bg-color:#C5E096;">0.6440</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.10.8.8.1"><span class="ltx_text" id="S6.T12.10.8.8.1.1" style="--ltx-bg-color:#C5E096;">1.27 <math alttext="\pm" class="ltx_Math" display="inline" id="S6.T12.10.8.8.1.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.11.9.9.5"><span class="ltx_text" id="S6.T12.11.9.9.5.1" style="--ltx-bg-color:#C5E096;">97.38%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T12.11.9.9.2"><span class="ltx_text" id="S6.T12.11.9.9.2.1" style="--ltx-bg-color:#C5E096;">1.16<math alttext="\times" class="ltx_Math" display="inline" id="S6.T12.11.9.9.2.1.m1" intent=":literal"><semantics><mo mathbackground="#C5E096" style="--ltx-bg-color:#C5E096;">×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.7 </span>Ablation: Efficient Vision Encoding</h3>
<div class="ltx_para" id="S6.SS7.p1">
<p class="ltx_p" id="S6.SS7.p1.1">As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S3.SS2.SSS1" title="3.2.1 Vision Encoding ‣ 3.2 Domain-Specific Adaptations ‣ 3 Building a Reasoning VLA Architecture ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Sec.</span>˜<span class="ltx_text ltx_ref_tag">3.2.1</span></a>, there are alternative methods for vision encoding that can be more efficient than the default single-image tokenizer in terms of tokens needed to represent multi-camera video inputs. To compare approaches, we choose a 4-camera setup, vary the vision encoder, and compare the resulting end-to-end model’s open-loop driving quality via minADE<sub class="ltx_sub" id="S6.SS7.p1.1.1">6</sub> relative to the baseline.</p>
</div>
<div class="ltx_para" id="S6.SS7.p2">
<p class="ltx_p" id="S6.SS7.p2.3">As can be seen in <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T13" title="In 6.7 Ablation: Efficient Vision Encoding ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">13</span></a>, the triplane-based multi-camera tokenizer from <cite class="ltx_cite ltx_citemacro_citet">Ivanovic et al. (<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib29" title="">2025</a>)</cite> achieves nearly identical minADE<sub class="ltx_sub" id="S6.SS7.p2.3.1">6</sub> values as the baseline, while only adding 6.3M parameters and reducing sensor token counts by <math alttext="3.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS7.p2.2.m2" intent=":literal"><semantics><mrow><mn>3.6</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">3.6\times</annotation></semantics></math>. Flex <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib103" title="">2025</a>)</cite> is able to achieve more drastic improvements, with a token compression of up to <math alttext="20\times" class="ltx_math_unparsed" display="inline" id="S6.SS7.p2.3.m3" intent=":literal"><semantics><mrow><mn>20</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">20\times</annotation></semantics></math> while only adding 61.6M parameters to the overall driving model and matching the driving quality of the baseline.</p>
</div>
<div class="ltx_para" id="S6.SS7.p3">
<p class="ltx_p" id="S6.SS7.p3.1">AR1 adopts single-image tokenization by default, as the optimal strategy can vary with the number of cameras, temporal frames, and camera resolutions. For example, a small number of cameras and short histories will favor single-image tokenization, more cameras and short histories will favor triplanes <cite class="ltx_cite ltx_citemacro_citep">(Ivanovic et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib29" title="">2025</a>)</cite>, and more cameras and long history sequences will favor Flex <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib103" title="">2025</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S6.T13">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T13.29.2.1" style="font-size:90%;">Table 13</span>: </span><span class="ltx_text" id="S6.T13.2.1" style="font-size:90%;">Relative comparison of different efficient vision encoding strategies on <math alttext="\mathcal{D}_{\text{overall}}" class="ltx_Math" display="inline" id="S6.T13.2.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>overall</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{overall}}</annotation></semantics></math>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T13.27">
<tr class="ltx_tr" id="S6.T13.6.4">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T13.6.4.5"><span class="ltx_text ltx_font_bold" id="S6.T13.6.4.5.1">Vision Encoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T13.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T13.3.1.1.1">Added Parameters <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T13.3.1.1.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T13.4.2.2"><span class="ltx_text ltx_font_bold" id="S6.T13.4.2.2.1">Tokens per Image <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T13.4.2.2.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T13.6.4.4">
<span class="ltx_text ltx_font_bold" id="S6.T13.5.3.3.1">Rel. minADE<sub class="ltx_sub" id="S6.T13.5.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S6.T13.5.3.3.1.1.1">6</span></sub></span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T13.6.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S6.T13.9.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T13.9.7.4">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.9.7.5">0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T13.8.6.2">
<math alttext="160" class="ltx_Math" display="inline" id="S6.T13.7.5.1.m1" intent=":literal"><semantics><mn>160</mn><annotation encoding="application/x-tex">160</annotation></semantics></math> <math alttext="(1.0\times)" class="ltx_math_unparsed" display="inline" id="S6.T13.8.6.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mn>1.0</mn><mo lspace="0.222em" rspace="0em">×</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1.0\times)</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.9.7.3"><math alttext="0\%" class="ltx_Math" display="inline" id="S6.T13.9.7.3.m1" intent=":literal"><semantics><mrow><mn>0</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">0\%</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T13.12.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T13.12.10.4" rowspan="2"><span class="ltx_text" id="S6.T13.12.10.4.1">
<span class="ltx_inline-block ltx_align_left" id="S6.T13.12.10.4.1.1">
<span class="ltx_p" id="S6.T13.12.10.4.1.1.1">Triplane</span>
<span class="ltx_p" id="S6.T13.12.10.4.1.1.2"><cite class="ltx_cite ltx_citemacro_citep">(Ivanovic et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib29" title="">2025</a>)</cite></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.12.10.5" rowspan="2"><span class="ltx_text" id="S6.T13.12.10.5.1">6.3M</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T13.11.9.2">
<math alttext="104" class="ltx_Math" display="inline" id="S6.T13.10.8.1.m1" intent=":literal"><semantics><mn>104</mn><annotation encoding="application/x-tex">104</annotation></semantics></math> <math alttext="(1.5\times)" class="ltx_math_unparsed" display="inline" id="S6.T13.11.9.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mn>1.5</mn><mo lspace="0.222em" rspace="0em">×</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1.5\times)</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.12.10.3"><math alttext="-3\%" class="ltx_Math" display="inline" id="S6.T13.12.10.3.m1" intent=":literal"><semantics><mrow><mo>−</mo><mrow><mn>3</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">-3\%</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T13.15.13">
<td class="ltx_td ltx_align_right" id="S6.T13.14.12.2">
<math alttext="45" class="ltx_Math" display="inline" id="S6.T13.13.11.1.m1" intent=":literal"><semantics><mn>45</mn><annotation encoding="application/x-tex">45</annotation></semantics></math> <math alttext="(3.6\times)" class="ltx_math_unparsed" display="inline" id="S6.T13.14.12.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mn>3.6</mn><mo lspace="0.222em" rspace="0em">×</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(3.6\times)</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S6.T13.15.13.3"><math alttext="+4\%" class="ltx_Math" display="inline" id="S6.T13.15.13.3.m1" intent=":literal"><semantics><mrow><mo>+</mo><mrow><mn>4</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">+4\%</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T13.18.16">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S6.T13.18.16.4" rowspan="4"><span class="ltx_text" id="S6.T13.18.16.4.1">
<span class="ltx_inline-block ltx_align_left" id="S6.T13.18.16.4.1.1">
<span class="ltx_p" id="S6.T13.18.16.4.1.1.1">Flex</span>
<span class="ltx_p" id="S6.T13.18.16.4.1.1.2"><cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib103" title="">2025</a>)</cite></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T13.18.16.5" rowspan="4"><span class="ltx_text" id="S6.T13.18.16.5.1">61.6M</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T13.17.15.2">
<math alttext="50" class="ltx_Math" display="inline" id="S6.T13.16.14.1.m1" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> <math alttext="(3.2\times)" class="ltx_math_unparsed" display="inline" id="S6.T13.17.15.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mn>3.2</mn><mo lspace="0.222em" rspace="0em">×</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(3.2\times)</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.18.16.3"><math alttext="-3\%" class="ltx_Math" display="inline" id="S6.T13.18.16.3.m1" intent=":literal"><semantics><mrow><mo>−</mo><mrow><mn>3</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">-3\%</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T13.21.19">
<td class="ltx_td ltx_align_right" id="S6.T13.20.18.2">
<math alttext="32" class="ltx_Math" display="inline" id="S6.T13.19.17.1.m1" intent=":literal"><semantics><mn>32</mn><annotation encoding="application/x-tex">32</annotation></semantics></math> <math alttext="(5.0\times)" class="ltx_math_unparsed" display="inline" id="S6.T13.20.18.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mn>5.0</mn><mo lspace="0.222em" rspace="0em">×</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(5.0\times)</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S6.T13.21.19.3"><math alttext="-3\%" class="ltx_Math" display="inline" id="S6.T13.21.19.3.m1" intent=":literal"><semantics><mrow><mo>−</mo><mrow><mn>3</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">-3\%</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T13.24.22">
<td class="ltx_td ltx_align_right" id="S6.T13.23.21.2">
<math alttext="16" class="ltx_Math" display="inline" id="S6.T13.22.20.1.m1" intent=":literal"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation></semantics></math> <math alttext="(10\times)" class="ltx_math_unparsed" display="inline" id="S6.T13.23.21.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mn>10</mn><mo lspace="0.222em" rspace="0em">×</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(10\times)</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S6.T13.24.22.3"><math alttext="-2\%" class="ltx_Math" display="inline" id="S6.T13.24.22.3.m1" intent=":literal"><semantics><mrow><mo>−</mo><mrow><mn>2</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">-2\%</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T13.27.25">
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T13.26.24.2">
<math alttext="8" class="ltx_Math" display="inline" id="S6.T13.25.23.1.m1" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math> <math alttext="(20\times)" class="ltx_math_unparsed" display="inline" id="S6.T13.26.24.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mn>20</mn><mo lspace="0.222em" rspace="0em">×</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(20\times)</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T13.27.25.3"><math alttext="-2\%" class="ltx_Math" display="inline" id="S6.T13.27.25.3.m1" intent=":literal"><semantics><mrow><mo>−</mo><mrow><mn>2</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">-2\%</annotation></semantics></math></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.8 </span>On-Vehicle Road Tests</h3>
<div class="ltx_para" id="S6.SS8.p1">
<p class="ltx_p" id="S6.SS8.p1.1">To validate the real-world deployment capability of AR1, we deployed the model in a test vehicle and conducted road testing in urban driving environments. The vehicle successfully navigated complex urban scenarios without human intervention, demonstrating the model’s ability to handle real-world driving conditions beyond simulation.
<a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.F14" title="In 6.8 On-Vehicle Road Tests ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Fig.</span>˜<span class="ltx_text ltx_ref_tag">14</span></a> shows an intersection where AR1 accurately identifies the traffic situation and produces clear and concise reasoning traces that lead to appropriate driving actions. These tests confirm that simulation improvements are transferred successfully to real-world autonomous driving scenarios.</p>
</div>
<div class="ltx_para" id="S6.SS8.p2">
<p class="ltx_p" id="S6.SS8.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS8.p2.1.1">Real-Time Inference Performance.</span> A critical requirement for on-vehicle deployment is real-time inference capability. We benchmark AR1 on an NVIDIA RTX 6000 Pro Blackwell platform, achieving an end-to-end inference latency of 99ms, within the real-time requirements for autonomous driving (typically 100ms). <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#S6.T14" title="In 6.8 On-Vehicle Road Tests ‣ 6 Experiments ‣ Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail"><span class="ltx_text ltx_ref_tag">Tab.</span>˜<span class="ltx_text ltx_ref_tag">14</span></a> provides a detailed breakdown of the inference pipeline, comparing our approach against alternative design choices. The prefilling stage processes the visual tokens and route information through the transformer layers to generate the key-value cache, which is then used during both reasoning and trajectory decoding.</p>
</div>
<figure class="ltx_table" id="S6.T14">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T14.2.1.1" style="font-size:90%;">Table 14</span>: </span><span class="ltx_text" id="S6.T14.3.2" style="font-size:90%;">Inference runtime breakdown on an NVIDIA RTX 6000 Pro Blackwell. Alpamayo-R1 achieves real-time performance (99ms) by combining flow-matching-based trajectory decoding with efficient vision encoding.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T14.4" style="width:345.0pt;height:27.9pt;vertical-align:-12.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-131.5pt,10.6pt) scale(0.567523570886003,0.567523570886003) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T14.4.1">
<tr class="ltx_tr" id="S6.T14.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T14.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T14.4.1.1.1.1">Model Configuration</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T14.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T14.4.1.1.2.1">Vision Encoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T14.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T14.4.1.1.3.1">Prefilling</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T14.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T14.4.1.1.4.1">Reasoning Decoding</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T14.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T14.4.1.1.5.1">Trajectory Decoding</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T14.4.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T14.4.1.1.6.1">Total</span></td>
</tr>
<tr class="ltx_tr" id="S6.T14.4.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T14.4.1.2.1">Baseline (trajectory-only, flow matching)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T14.4.1.2.2">3.43ms</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T14.4.1.2.3">16.54ms</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T14.4.1.2.4">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T14.4.1.2.5">8.75ms (5 steps)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T14.4.1.2.6">29ms</td>
</tr>
<tr class="ltx_tr" id="S6.T14.4.1.3" style="--ltx-bg-color:#C5E096;">
<td class="ltx_td ltx_align_left" id="S6.T14.4.1.3.1"><span class="ltx_text" id="S6.T14.4.1.3.1.1" style="--ltx-bg-color:#C5E096;">Alpamayo-R1 (ours, flow matching)</span></td>
<td class="ltx_td ltx_align_center" id="S6.T14.4.1.3.2"><span class="ltx_text" id="S6.T14.4.1.3.2.1" style="--ltx-bg-color:#C5E096;">3.43ms</span></td>
<td class="ltx_td ltx_align_center" id="S6.T14.4.1.3.3"><span class="ltx_text" id="S6.T14.4.1.3.3.1" style="--ltx-bg-color:#C5E096;">16.54ms</span></td>
<td class="ltx_td ltx_align_center" id="S6.T14.4.1.3.4"><span class="ltx_text" id="S6.T14.4.1.3.4.1" style="--ltx-bg-color:#C5E096;">70ms (40 tokens)</span></td>
<td class="ltx_td ltx_align_center" id="S6.T14.4.1.3.5"><span class="ltx_text" id="S6.T14.4.1.3.5.1" style="--ltx-bg-color:#C5E096;">8.75ms (5 steps)</span></td>
<td class="ltx_td ltx_align_center" id="S6.T14.4.1.3.6"><span class="ltx_text ltx_font_bold" id="S6.T14.4.1.3.6.1" style="--ltx-bg-color:#C5E096;">99ms</span></td>
</tr>
<tr class="ltx_tr" id="S6.T14.4.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T14.4.1.4.1">Alpamayo-R1 (auto-regressive traj)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T14.4.1.4.2">3.43ms</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T14.4.1.4.3">16.54ms</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T14.4.1.4.4">70ms (40 tokens)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T14.4.1.4.5">222ms (127 tokens)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T14.4.1.4.6">312ms</td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S6.F14.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F14.2.1.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="S6.F14.3.2" style="font-size:90%;">On-vehicle road test showing that AR1 generates a reasoning trace in an intersection scenario. The ego vehicle first decelerates to stop due to the red light, then waits for the traffic signal and finally resumes when the light turns green and takes the turn.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we present <span class="ltx_text ltx_font_bold" id="S7.p1.1.1">Alpamayo-R1 (AR1)</span>, a vision-language-action model that integrates structured chain-of-thought reasoning capabilities with trajectory prediction to enhance autonomous driving performance, particularly in long-tail, safety-critical scenarios. To enable the model to generate causally-grounded reasoning, we introduce the <span class="ltx_text ltx_font_bold" id="S7.p1.1.2">Chain of Causation (CoC)</span> dataset, constructed through a hybrid labeling pipeline that combines large-scale auto-labeling with humans in the loop. We further align reasoning with action through RL, ensuring that the generated reasoning traces are consistent with the executed driving behaviors. Our comprehensive evaluations across open-loop metrics, closed-loop simulation, and ablation studies demonstrate that AR1 achieves consistent improvements over end-to-end baselines, with particularly pronounced gains on challenging scenarios involving complex agent interactions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Future Work.</span> Several promising research directions remain open. First, <span class="ltx_text ltx_font_italic" id="S7.p2.1.2">policy structuring</span>: while our flow-matching-based trajectory decoder provides kinematically feasible outputs, exploring hierarchical policy architectures that decompose high-level meta-actions into structured motion primitives could further improve interpretability and efficiency. Second, <span class="ltx_text ltx_font_italic" id="S7.p2.1.3">reasoning on demand</span>: our current architecture generates reasoning traces for every input; future work could investigate adaptive mechanisms that selectively invoke reasoning only for safety-critical or ambiguous scenarios, enabling more efficient inference-time computation allocation similar to recent advances in test-time scaling <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib104" title="">2023</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib68" title="">2024</a>)</cite>; Third, <span class="ltx_text ltx_font_italic" id="S7.p2.1.4">auxiliary task integration</span>: while AR1 focuses on trajectory prediction and causal reasoning, incorporating complementary self-supervised objectives, such as depth estimation, scene flow prediction, or 3D Gaussian Splatting representations, could improve the visual backbone’s semantic understanding; Fourth, <span class="ltx_text ltx_font_italic" id="S7.p2.1.5">world model integration</span>: our current approach predicts actions from observed states; incorporating learned world models could enable forward simulation and counterfactual reasoning, improving robustness in dynamic scenarios.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">Open Source Release.</span> We release Alpamayo-R1-10B model weights at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/nvidia/Alpamayo-R1-10B" title="">https://huggingface.co/nvidia/Alpamayo-R1-10B</a> and inference code at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVlabs/alpamayo" title="">https://github.com/NVlabs/alpamayo</a>. The model is evaluated on the PhysicalAI-AV dataset <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib63" title="">2025b</a>)</cite> and the AlpaSim public scenario set <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA et al., <a class="ltx_ref" href="https://arxiv.org/html/2511.00088v2#bib.bib67" title="">2025b</a>)</cite>, enabling reproducible benchmarking by the research community.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Contributors and Acknowledgments</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Core Contributors</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">Yulong Cao,
Tong Che,
Yuxiao Chen,
Wenhao Ding,
Boris Ivanovic,
Peter Karkus,
Boyi Li,
Tsung-Yi Lin,
Patrick Langechuan Liu,
Zhijian Liu,
Jason Lu,
Wenjie Luo,
Marco Pavone,
Ran Tian,
Yan Wang,
Xinshuo Weng,
Tianjun Xiao,
Xiaodong Yang,
Yurong You,
Xiaohui Zeng.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.1">Data &amp; Benchmarks:</span> TX, XW, YC, WD, YW curated autonomous driving datasets and benchmarks.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.2">Labeling Pipeline:</span> XW, YC, WD, BL, XY, YW developed the reasoning trace labeling pipeline and the infrastructure. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.3">Training Infrastructure:</span> YY, WL, YW, WD built the supervised fine-tuning infrastructure; TC, RT, WL built the reinforcement learning infrastructure. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.4">Vision Encoding:</span> BI, YW developed the vision encoder. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.5">Action Decoding:</span> YY, YC built the flow-matching trajectory decoder. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.6">Model Training:</span> YY, WL, YW, WD, JL, ZL, PLL trained the VLA models with supervised fine-tuning; YW, WL, YY, XY, TL, XZ trained the Cosmos-Reason VLM backbone; RT, TC, YW, WL, YY, WD designed the post-training strategy and post-trained models with reinforcement learning; WD, YC designed the data mixture strategy.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.7">Project Leads:</span> YW, WL drove the project from concept to completion.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.8">Program Architect and Project Manager:</span> MP conceived, coordinated, and guided the overall effort. BI supported MP in coordination and guidance.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Contributors</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Junjie Bai,
Ke Chen,
Jenna Diamond,
Yifan Ding,
Liang Feng,
Greg Heinrich,
Jack Huang,
Pinyi Li,
Dongran Liu,
Ming-Yu Liu,
Leo Yunxiang Mao,
Pavlo Molchanov,
Lindsey Pavao,
Zhenghao Peng,
Mike Ranzinger,
Ed Schmerling,
Shida Shen,
Yunfei Shi,
Sarah Tariq,
Tilman Wekel,
Eric Yang,
Wenyuan Zhang.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.1.1">Contributions.</span>
ST led the end-to-end development on the production side and provided key input on the data pipeline and model architecture. LP, JD led the human annotation effort. PM, GH, MR trained the vision encoder.
ML provided Cosmos-Reason model support.
YD processed cosmos AV data into training format.
ZP improved the large-scale SFT training workflow.
FL, JB provided support for the large-scale RL training infrastructure.
ES curated and preprocessed driving data.
KC, WZ, JH improved the CoC auto-labeling pipeline.
SS developed the LLM-based evaluator for CoC reasoning traces.
YS, EY, TW built the CoC labeling tools for human labeling. DL, PL and LM were instrumental in conducting the on-vehicle tests and model profiling.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Acknowledgments</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">We thank Xinzhou Wu and Ali Kani for leadership and strategic support; Sachin Patil for general support in AV model training and deployment; Zhiding Yu, Guilin Liu, Max Li, Song Han, Hongxu Yin, Sifei Liu, and Yu-Wei Chao for valuable discussions on vision-language model training; Jesse Hong for running the CoC labeling pipeline; Richard Lin, Zi Wang, Walter Yu for improvements to the CoC auto-labeling pipeline; Anton Mitrokhin, Jacob Kern for improvements to the CoC human labeling pipeline; Martin Peng, Steve Hu, Andy Martin for dataset management and releases; Di Chen, Hanson Xu for help with model deployment; Chao Fang, Shuaijun Chen, and Niral Pathak for on-vehicle deployment support; Charles Vorbach, Zhenyi Zhang, Rachit Shah, Ritaank Tiwari for help with onboard vehicle deployment; Parixit Aghera, Ratin Kumar, Parag Mehendale, Niranjan Avadhanam, Rajath Shetty, Ronan LeToquin, Suraj Das and Ashley Hu for vehicle testing; Sachit Kadle, Annie Feng, and Zheng Lian for closed-loop simulation support; Maximilian Igl, Michael Watson, and Apoorva Sharma for closed-loop experimentation and metric implementations.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arai et al. (2025)</span>
<span class="ltx_bibblock">
Hidehisa Arai, Keita Miwa, Kento Sasaki, Kohei Watanabe, Yu Yamaguchi, Shunsuke
Aoki, and Issei Yamamoto.

</span>
<span class="ltx_bibblock">CoVLA: Comprehensive vision-language-action dataset for autonomous
driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2025 IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV)</em>, pages 1933–1943. IEEE, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2025)</span>
<span class="ltx_bibblock">
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai
Dang, Peng Wang, Shijie Wang, Jun Tang, et al.

</span>
<span class="ltx_bibblock">Qwen2.5-VL technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2502.13923</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2022)</span>
<span class="ltx_bibblock">
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
et al.

</span>
<span class="ltx_bibblock">Constitutional AI: Harmlessness from AI feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2212.08073</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie.

</span>
<span class="ltx_bibblock">METEOR: An automatic metric for mt evaluation with improved
correlation with human judgments.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ACL Workshop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization</em>, pages 65–72, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojarski et al. (2016)</span>
<span class="ltx_bibblock">
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, et al.

</span>
<span class="ltx_bibblock">End-to-End Learning for Self-Driving Cars.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:1604.07316</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caesar et al. (2020)</span>
<span class="ltx_bibblock">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">nuScenes: A multimodal dataset for autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 11621–11631, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi et al. (2025)</span>
<span class="ltx_bibblock">
Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li,
Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, et al.

</span>
<span class="ltx_bibblock">Impromptu VLA: Open weights and open data for driving
vision-language-action models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2505.23757</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2025)</span>
<span class="ltx_bibblock">
Jang Hyun Cho, Boris Ivanovic, Yulong Cao, Edward Schmerling, Yue Wang, Xinshuo
Weng, Boyi Li, Yurong You, Philipp Krähenbühl, Yan Wang, et al.

</span>
<span class="ltx_bibblock">Language-image models with 3D understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Conference on Learning Representations</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al. (2017)</span>
<span class="ltx_bibblock">
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Comanici et al. (2025)</span>
<span class="ltx_bibblock">
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen
Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen,
et al.

</span>
<span class="ltx_bibblock">Gemini 2.5: Pushing the frontier with advanced reasoning,
multimodality, long context, and next generation agentic capabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2507.06261</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corbière et al. (2025)</span>
<span class="ltx_bibblock">
Charles Corbière, Simon Roburin, Syrielle Montariol, Antoine Bosselut, and
Alexandre Alahi.

</span>
<span class="ltx_bibblock">Retrieval-based interleaved visual chain-of-thought in real-world
driving scenarios.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2501.04671</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dauner et al. (2023)</span>
<span class="ltx_bibblock">
Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta.

</span>
<span class="ltx_bibblock">Parting with misconceptions about learning-based vehicle motion
planning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Conference on Robot Learning</em>, pages 1268–1281. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI (2025)</span>
<span class="ltx_bibblock">
DeepSeek-AI.

</span>
<span class="ltx_bibblock">DeepSeek-R1: Incentivizing reasoning capability in LLMs via
reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2501.12948</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2024)</span>
<span class="ltx_bibblock">
Xinpeng Ding, Jianhua Han, Hang Xu, Xiaodan Liang, Wei Zhang, and Xiaomeng Li.

</span>
<span class="ltx_bibblock">Holistic autonomous driving understanding by bird’s-eye-view injected
multi-modal large models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 13668–13677, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2017)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun.

</span>
<span class="ltx_bibblock">CARLA: An open urban driving simulator.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Conference on Robot Learning</em>. PMLR, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driess et al. (2025)</span>
<span class="ltx_bibblock">
Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell,
Karl Pertsch, Allen Z Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al.

</span>
<span class="ltx_bibblock">Knowledge insulating vision-language-action models: Train fast, run
fast, generalize better.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2505.23705</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Esser, Robin Rombach, and Bjorn Ommer.

</span>
<span class="ltx_bibblock">Taming transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 12873–12883, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ettinger et al. (2021)</span>
<span class="ltx_bibblock">
Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek
Pradhan, Yuning Chai, Ben Sapp, Charles Qi, Yin Zhou, Zoey Yang, Aurélien
Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon
Shlens, and Dragomir Anguelov.

</span>
<span class="ltx_bibblock">Large scale interactive motion forecasting for autonomous driving:
The waymo open motion dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">IEEE International Conference on Computer Vision</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2018)</span>
<span class="ltx_bibblock">
Haoyang Fan, Fan Zhu, Changchun Liu, Liangliang Zhang, Li Zhuang, Dong Li,
Weicheng Zhu, Jiangtao Hu, Hongye Li, and Qi Kong.

</span>
<span class="ltx_bibblock">Baidu Apollo EM motion planner.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:1807.08048</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2025)</span>
<span class="ltx_bibblock">
Shiyu Fang, Yiming Cui, Haoyang Liang, Chen Lv, Peng Hang, and Jian Sun.

</span>
<span class="ltx_bibblock">CoReVLA: A dual-stage end-to-end autonomous driving framework for
long-tail scenarios via collect-and-refine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2509.15968</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. (2025)</span>
<span class="ltx_bibblock">
Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song, Caihong
Qin, Mofan Zhou, Yifei Zhan, Peng Jia, et al.

</span>
<span class="ltx_bibblock">DriveAction: A benchmark for exploring human-like driving decisions
in vla models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2506.05667</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hegde et al. (2025)</span>
<span class="ltx_bibblock">
Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya,
Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M Patel, and Fatih
Porikli.

</span>
<span class="ltx_bibblock">Distilling multi-modal large language models for autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 27575–27585, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al. (2025)</span>
<span class="ltx_bibblock">
Xinmeng Hou, Wuqi Wang, Long Yang, Hao Lin, Jinglun Feng, Haigen Min, and
Xiangmo Zhao.

</span>
<span class="ltx_bibblock">DriveAgent: Multi-agent structured reasoning with LLM and
multimodal sensor fusion for autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2505.02123</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai,
Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu,
Jifeng Dai, Yu Qiao, and Hongyang Li.

</span>
<span class="ltx_bibblock">Planning-oriented autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 17853–17862, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al. (2024)</span>
<span class="ltx_bibblock">
Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy
Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, et al.

</span>
<span class="ltx_bibblock">EMMA: End-to-end multimodal model for autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2410.23262</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ishaq et al. (2025)</span>
<span class="ltx_bibblock">
Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar, Dinura
Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham Cholakkal,
et al.

</span>
<span class="ltx_bibblock">DriveLMM-o1: A step-by-step reasoning dataset and large multimodal
model for driving scenario understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2503.10621</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivanovic et al. (2025)</span>
<span class="ltx_bibblock">
Boris Ivanovic, Cristiano Saltori, Yurong You, Yan Wang, Wenjie Luo, and Marco
Pavone.

</span>
<span class="ltx_bibblock">Efficient multi-camera tokenization with triplanes for end-to-end
driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">IEEE Robotics and Automation Letters</em>, 10(11):11713–11720, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Janner et al. (2022)</span>
<span class="ltx_bibblock">
Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine.

</span>
<span class="ltx_bibblock">Planning with diffusion for flexible behavior synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">International Conference on Machine Learning</em>, pages
9902–9915, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2024)</span>
<span class="ltx_bibblock">
Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, and Junchi Yan.

</span>
<span class="ltx_bibblock">Bench2Drive: Towards multi-ability benchmarking of closed-loop
end-to-end autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>,
37:819–844, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2025)</span>
<span class="ltx_bibblock">
Anqing Jiang, Yu Gao, Yiru Wang, Zhigang Sun, Shuo Wang, Yuwen Heng, Hao Sun,
Shichen Tang, Lijuan Zhu, Jinhao Chai, et al.

</span>
<span class="ltx_bibblock">IRL-VLA: Training an vision-language-action policy via reward world
model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2508.06571</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023a)</span>
<span class="ltx_bibblock">
Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian
Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang.

</span>
<span class="ltx_bibblock">VAD: Vectorized scene representation for efficient autonomous
driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 8340–8350, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang
Huang, Wenyu Liu, and Xinggang Wang.

</span>
<span class="ltx_bibblock">Senna: Bridging large vision-language models and end-to-end
autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2410.22313</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023b)</span>
<span class="ltx_bibblock">
Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir
Anguelov, et al.

</span>
<span class="ltx_bibblock">MotionDiffuser: Controllable multi-agent motion prediction using
diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 9644–9653, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khaki et al. (2025)</span>
<span class="ltx_bibblock">
Samir Khaki, Junxian Guo, Jiaming Tang, Shang Yang, Yukang Chen,
Konstantinos N. Plataniotis, Yao Lu, Song Han, and Zhijian Liu.

</span>
<span class="ltx_bibblock">SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ICCV</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2018)</span>
<span class="ltx_bibblock">
Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata.

</span>
<span class="ltx_bibblock">Textual explanations for self-driving vehicles.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">European Conference on Computer Vision</em>, pages 563–578,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
Joseph E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with
pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ACM SIGOPS 29th Symposium on Operating Systems Principles</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023a)</span>
<span class="ltx_bibblock">
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret,
Kellie Ren Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi,
et al.

</span>
<span class="ltx_bibblock">RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback
with AI Feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">International Conference on Machine Learning</em>,
2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023b)</span>
<span class="ltx_bibblock">
Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier,
Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu.

</span>
<span class="ltx_bibblock">Aligning text-to-image models using human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2302.12192</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lefèvre et al. (2014)</span>
<span class="ltx_bibblock">
Sébastien Lefèvre, David Vasquez, and Christian Laugier.

</span>
<span class="ltx_bibblock">A survey on motion prediction and risk assessment for intelligent
vehicles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">ROBOMECH Journal</em>, 1(1):1–14, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025a)</span>
<span class="ltx_bibblock">
Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui,
Sushant Veer, Max Ehrlich, Jonah Philion, et al.

</span>
<span class="ltx_bibblock">Wolf: Dense video captioning with a world summarization framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Transactions on Machine Learning Research</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Yiheng Li, Cunxin Fan, Chongjian Ge, Zhihao Zhao, Chenran Li, Chenfeng Xu,
Huaxiu Yao, Masayoshi Tomizuka, Bolei Zhou, Chen Tang, et al.

</span>
<span class="ltx_bibblock">WOMD-Reasoning: A large-scale dataset for interaction reasoning in
driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2407.04281</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025b)</span>
<span class="ltx_bibblock">
Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun
Zhou, Long Chen, Haiyang Sun, Bing Wang, et al.

</span>
<span class="ltx_bibblock">ReCogDrive: A reinforced cognitive framework for end-to-end
autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2506.08052</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025c)</span>
<span class="ltx_bibblock">
Yue Li, Meng Tian, Dechang Zhu, Jiangtong Zhu, Zhenyu Lin, Zhiwei Xiong, and
Xinhai Zhao.

</span>
<span class="ltx_bibblock">Drive-R1: Bridging reasoning and planning in VLMs for autonomous
driving with reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2506.18234</em>, 2025c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2025)</span>
<span class="ltx_bibblock">
Haicheng Liao, Hanlin Kong, Bonan Wang, Chengyue Wang, Wang Ye, Zhengbing He,
Chengzhong Xu, and Zhenning Li.

</span>
<span class="ltx_bibblock">CoT-Drive: Efficient motion forecasting for autonomous driving with
LLMs and chain-of-thought prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">IEEE Transactions on Artificial Intelligence</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipman et al. (2023)</span>
<span class="ltx_bibblock">
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew
Le.

</span>
<span class="ltx_bibblock">Flow matching for generative modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2025a)</span>
<span class="ltx_bibblock">
Wenru Liu, Pei Liu, and Jun Ma.

</span>
<span class="ltx_bibblock">DSDrive: Distilling large language model for lightweight end-to-end
autonomous driving with unified reasoning and planning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2505.05360</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2025b)</span>
<span class="ltx_bibblock">
Xueyi Liu, Zuodong Zhong, Yuxin Guo, Yun-Fu Liu, Zhiguo Su, Qichao Zhang, Junli
Wang, Yinfeng Gao, Yupeng Zheng, Qiao Lin, et al.

</span>
<span class="ltx_bibblock">ReasonPlan: Unified scene prediction and decision reasoning for
closed-loop autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2505.20024</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2023)</span>
<span class="ltx_bibblock">
Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bronstein, Rebecca Roelofs,
Benjamin Sapp, Brandyn White, Aleksandra Faust, Shimon Whiteson, et al.

</span>
<span class="ltx_bibblock">Imitation is not enough: Robustifying imitation with reinforcement
learning for challenging driving scenarios.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">IEEE/RSJ International Conference on Intelligent Robots and
Systems</em>, pages 7553–7560, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2025)</span>
<span class="ltx_bibblock">
Yuhang Lu, Jiadong Tu, Yuexin Ma, and Xinge Zhu.

</span>
<span class="ltx_bibblock">Real-ad: Towards human-like reasoning in end-to-end autonomous
driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 27783–27793, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2025a)</span>
<span class="ltx_bibblock">
Yuechen Luo, Fang Li, Shaoqing Xu, Zhiyi Lai, Lei Yang, Qimao Chen, Ziang Luo,
Zixun Xie, Shengyin Jiang, Jiaxin Liu, et al.

</span>
<span class="ltx_bibblock">AdaThinkDrive: Adaptive thinking via reinforcement learning for
autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2509.13769</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2025b)</span>
<span class="ltx_bibblock">
Ziang Luo, Kangan Qian, Jiahua Wang, Yuechen Luo, Jinyu Miao, Zheng Fu, Yunlong
Wang, Sicong Jiang, Zilin Huang, Yifei Hu, et al.

</span>
<span class="ltx_bibblock">MTRDrive: Memory-tool synergistic reasoning for robust autonomous
driving in corner cases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2509.20843</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lynch and Park (2017)</span>
<span class="ltx_bibblock">
Kevin M Lynch and Frank C Park.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Modern Robotics</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malla et al. (2023)</span>
<span class="ltx_bibblock">
Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li.

</span>
<span class="ltx_bibblock">DRAMA: Joint risk localization and captioning in driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Winter Conference on Applications of Computer Vision</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2023)</span>
<span class="ltx_bibblock">
Jiageng Mao, Yuxi Qian, Junjie Ye, Hang Zhao, and Yue Wang.

</span>
<span class="ltx_bibblock">GPT-Driver: Learning to drive with GPT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2310.01415</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2024)</span>
<span class="ltx_bibblock">
Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang.

</span>
<span class="ltx_bibblock">A language agent for autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Conference on Language Modeling</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcu et al. (2024)</span>
<span class="ltx_bibblock">
Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte,
Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie
Shotton, et al.

</span>
<span class="ltx_bibblock">LingoQA: Visual question answering for autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">European Conference on Computer Vision</em>, pages 252–269,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al. (2024)</span>
<span class="ltx_bibblock">
Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian
Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng.

</span>
<span class="ltx_bibblock">Rule based rewards for language model safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in Neural Information Processing Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2024)</span>
<span class="ltx_bibblock">
Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, and
Li Zhang.

</span>
<span class="ltx_bibblock">Reason2Drive: Towards interpretable and chain-based reasoning for
autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">European Conference on Computer Vision</em>, pages 292–308,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2025a)</span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">Cosmos-RL: A flexible and scalable reinforcement learning
framework.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nvidia-cosmos.github.io/cosmos-rl/" title="">https://nvidia-cosmos.github.io/cosmos-rl/</a>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2025b)</span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">Physical AI autonomous vehicles dataset.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles" title="">https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles</a>,
October 2025b.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles" title="">https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2025c)</span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">Physical AI autonomous vehicles NuRec dataset.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles-NuRec" title="">https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles-NuRec</a>,
October 2025c.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles-NuRec" title="">https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles-NuRec</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2026)</span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">NVIDIA Announces Alpamayo Family of Open-Source AI Models and Tools
to Accelerate Safe, Reasoning-Based Autonomous Vehicle Development.

</span>
<span class="ltx_bibblock">2026.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nvidianews.nvidia.com/news/alpamayo-autonomous-vehicle-development" title="">https://nvidianews.nvidia.com/news/alpamayo-autonomous-vehicle-development</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA et al. (2025a)</span>
<span class="ltx_bibblock">
NVIDIA, Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit
Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding,
Liang Feng, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth
Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan
Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li,
Xuan Li, Maosheng Liao, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Xiangyu Lu,
Alice Luo, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero,
Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang,
Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Dinghao Yang, Xiaodong Yang,
Zhuolin Yang, Jingxu Zhang, Xiaohui Zeng, and Zhe Zhang.

</span>
<span class="ltx_bibblock">Cosmos-Reason1: From physical common sense to embodied reasoning,
2025a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2503.15558" title="">https://arxiv.org/abs/2503.15558</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA et al. (2025b)</span>
<span class="ltx_bibblock">
NVIDIA, Yulong Cao, Riccardo de Lutio, Sanja Fidler, Guillermo Garcia Cobo, Zan
Gojcic, Maximilian Igl, Boris Ivanovic, Peter Karkus, Janick Martinez Esturo,
Marco Pavone, Aaron Smith, Ellie Tanimura, Michal Tyszkiewicz, Michael
Watson, Qi Wu, and Le Zhang.

</span>
<span class="ltx_bibblock">AlpaSim: A modular, lightweight, and data-driven research simulator
for autonomous driving, October 2025b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVlabs/alpasim" title="">https://github.com/NVlabs/alpasim</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Learning to reason with LLMs, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/learning-to-reason-with-llms/" title="">https://openai.com/index/learning-to-reason-with-llms/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2025)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-5 system card.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/gpt-5-system-card/" title="">https://openai.com/index/gpt-5-system-card/</a>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab et al. (2023)</span>
<span class="ltx_bibblock">
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V Vo, Marc
Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa,
Alaaeldin El-Nouby, et al.

</span>
<span class="ltx_bibblock">DINOv2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paden et al. (2016)</span>
<span class="ltx_bibblock">
Brian Paden, Michal Čáp, Sze Zheng Yong, Dmitry Yershov, and Emilio
Frazzoli.

</span>
<span class="ltx_bibblock">A survey of motion planning and control techniques for self-driving
urban vehicles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">IEEE Transactions on Intelligent Vehicles</em>, 1(1):33–55, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">BLEU: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Association for Computational Linguistics</em>, pages 311–318,
2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Physical Intelligence et al. (2025)</span>
<span class="ltx_bibblock">
Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan
Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo
Fusai, et al.

</span>
<span class="ltx_bibblock"><math alttext="\pi_{0.5}" class="ltx_Math" display="inline" id="bib.bib73.1.m1" intent=":literal"><semantics><msub><mi>π</mi><mn>0.5</mn></msub><annotation encoding="application/x-tex">\pi_{0.5}</annotation></semantics></math>: a vision-language-action model with open-world
generalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.2.1">arXiv preprint arXiv:2504.16054</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al. (2025)</span>
<span class="ltx_bibblock">
Kangan Qian, Sicong Jiang, Yang Zhong, Ziang Luo, Zilin Huang, Tianze Zhu, Kun
Jiang, Mengmeng Yang, Zheng Fu, Jinyu Miao, et al.

</span>
<span class="ltx_bibblock">AgentThink: A unified framework for tool-augmented chain-of-thought
reasoning in vision-language models for autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2505.15298</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al. (2024)</span>
<span class="ltx_bibblock">
Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang.

</span>
<span class="ltx_bibblock">NuScenes-QA: A multi-modal visual question answering benchmark for
autonomous driving scenario.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">AAAI Conference on Artificial Intelligence</em>, pages
4542–4550, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen Team (2024)</span>
<span class="ltx_bibblock">
Qwen Team.

</span>
<span class="ltx_bibblock">Qwen2.5: A party of foundation models, September 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://qwenlm.github.io/blog/qwen2.5/" title="">https://qwenlm.github.io/blog/qwen2.5/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen Team (2025)</span>
<span class="ltx_bibblock">
Qwen Team.

</span>
<span class="ltx_bibblock">Qwen3-VL: Sharper vision, deeper thought, broader action.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list" title="">https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list</a>,
2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Renz et al. (2025)</span>
<span class="ltx_bibblock">
Katrin Renz, Long Chen, Elahe Arani, and Oleg Sinavski.

</span>
<span class="ltx_bibblock">SimLingo: Vision-only closed-loop autonomous driving with
language-action alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">IEEE/CVF Computer Vision and Pattern Recognition
Conference</em>, pages 11993–12003, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rowe et al. (2025)</span>
<span class="ltx_bibblock">
Luke Rowe, Rodrigue de Schaetzen, Roger Girgis, Christopher Pal, and Liam
Paull.

</span>
<span class="ltx_bibblock">Poutine: Vision-language-trajectory pre-training and reinforcement
learning post-training enable robust end-to-end autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2506.11234</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2024)</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei
Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al.

</span>
<span class="ltx_bibblock">DeepSeekMath: Pushing the limits of mathematical reasoning in open
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:2402.03300</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sima et al. (2024)</span>
<span class="ltx_bibblock">
Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie,
Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li.

</span>
<span class="ltx_bibblock">DriveLM: Driving with graph visual question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">European conference on computer vision</em>, pages 256–274.
Springer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohn et al. (2015)</span>
<span class="ltx_bibblock">
Kihyuk Sohn, Honglak Lee, and Xinchen Yan.

</span>
<span class="ltx_bibblock">Learning structured output representation using deep conditional
generative models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Advances in Neural Information Processing Systems</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2025)</span>
<span class="ltx_bibblock">
Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, and Udaya
Ghai.

</span>
<span class="ltx_bibblock">Mind the gap: Examining the self-improvement capabilities of large
language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">ICLR</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2025)</span>
<span class="ltx_bibblock">
Kexin Tian, Jingrui Mao, Yunlong Zhang, Jiwan Jiang, Yang Zhou, and Zhengzhong
Tu.

</span>
<span class="ltx_bibblock">NuScenes-SpatialQA: A spatial understanding and reasoning benchmark
for vision-language models in autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2504.03164</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian and Goel (2025)</span>
<span class="ltx_bibblock">
Ran Tian and Kratarth Goel.

</span>
<span class="ltx_bibblock">Direct post-training preference alignment for multi-agent motion
generation models using implicit feedback from pre-training demonstrations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2503.20105</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2024a)</span>
<span class="ltx_bibblock">
Ran Tian, Boyi Li, Xinshuo Weng, Yuxiao Chen, Edward Schmerling, Yue Wang,
Boris Ivanovic, and Marco Pavone.

</span>
<span class="ltx_bibblock">Tokenize the world into object-level knowledge to address long-tail
events in autonomous driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Conference on Robot Learning</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2024b)</span>
<span class="ltx_bibblock">
Ran Tian, Yilin Wu, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, and Andrea
Bajcsy.

</span>
<span class="ltx_bibblock">Maximizing alignment with minimal feedback: Efficiently learning
rewards for visuomotor robot policy alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:2412.04835</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tschannen et al. (2025)</span>
<span class="ltx_bibblock">
Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim
Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil
Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua
Zhai.

</span>
<span class="ltx_bibblock">SigLIP 2: Multilingual vision-language encoders with improved
semantic understanding, localization, and dense features.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2502.14786</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Urmson et al. (2008)</span>
<span class="ltx_bibblock">
Chris Urmson, John Anhalt, J. Andrew Bagnell, Christopher Baker, Robert
Bittner, Michael N. Clark, John Dolan, Daniel Duggins, Todd Galatali,
Christopher Geyer, et al.

</span>
<span class="ltx_bibblock">Autonomous driving in urban environments: Boss and the urban
challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Journal of Field Robotics</em>, 25(8):425–466,
2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van den Oord et al. (2017)</span>
<span class="ltx_bibblock">
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu.

</span>
<span class="ltx_bibblock">Neural discrete representation learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Advances in Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam et al. (2015)</span>
<span class="ltx_bibblock">
Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">CIDEr: Consensus-based image description evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 4566–4575, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025)</span>
<span class="ltx_bibblock">
Feng Wang, Yaodong Yu, Guoyizhe Wei, Wei Shao, Yuyin Zhou, Alan Yuille, and
Cihang Xie.

</span>
<span class="ltx_bibblock">Scaling laws in patchification: An image is worth 50,176 tokens and
more.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">arXiv preprint arXiv:2502.03738</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Tianqi Wang, Enze Xie, Ruihang Chu, Zhenguo Li, and Ping Luo.

</span>
<span class="ltx_bibblock">DriveCoT: Integrating chain-of-thought reasoning with end-to-end
driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">arXiv preprint arXiv:2403.16996</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2025)</span>
<span class="ltx_bibblock">
Maolin Wei, Wanzhou Liu, and Eshed Ohn-Bar.

</span>
<span class="ltx_bibblock">DriveQA: Passing the driving knowledge test.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">arXiv preprint arXiv:2508.21824</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng et al. (2024)</span>
<span class="ltx_bibblock">
Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone.

</span>
<span class="ltx_bibblock">PARA-Drive: Parallelized Architecture for Real-Time Autonomous
Driving.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 15449–15458, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2025a)</span>
<span class="ltx_bibblock">
Dongming Wu, Wencheng Han, Yingfei Liu, Tiancai Wang, Cheng-Zhong Xu, Xiangyu
Zhang, and Jianbing Shen.

</span>
<span class="ltx_bibblock">Language prompt for autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">The Association for the Advancement of Artificial
Intelligence</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2025b)</span>
<span class="ltx_bibblock">
Qi Wu, Janick Martinez Esturo, Ashkan Mirzaei, Nicolas Moenne-Loccoz, and Zan
Gojcic.

</span>
<span class="ltx_bibblock">3DGUT: Enabling distorted cameras and secondary rays in gaussian
splatting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 26036–26046, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu (2025)</span>
<span class="ltx_bibblock">
Xinzhou Wu.

</span>
<span class="ltx_bibblock">Accelerate the future of AI-defined vehicles and autonomous
driving, 2025.

</span>
<span class="ltx_bibblock">Available at
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nvidia.com/en-us/on-demand/session/gtc25-dd40000/" title="">https://www.nvidia.com/en-us/on-demand/session/gtc25-dd40000/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2025)</span>
<span class="ltx_bibblock">
Shaoyuan Xie, Lingdong Kong, Yuhao Dong, Chonghao Sima, Wenwei Zhang, Qi Alfred
Chen, Ziwei Liu, and Liang Pan.

</span>
<span class="ltx_bibblock">Are VLMs ready for autonomous driving? an empirical study from the
reliability, data, and metric perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">arXiv preprint arXiv:2501.04003</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024a)</span>
<span class="ltx_bibblock">
Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory P Meyer, Siva Karthik Mustikovela,
Siddhartha Srinivasa, Eric M Wolff, and Xin Huang.

</span>
<span class="ltx_bibblock">VLM-AD: End-to-end autonomous driving through vision-language model
supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2412.14446</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024b)</span>
<span class="ltx_bibblock">
Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee K Wong,
Zhenguo Li, and Hengshuang Zhao.

</span>
<span class="ltx_bibblock">DriveGPT4: Interpretable end-to-end autonomous driving via large
language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">IEEE Robotics and Automation Letters</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2025)</span>
<span class="ltx_bibblock">
Jiawei Yang, Ziyu Chen, Yurong You, Yan Wang, Yiming Li, Yuxiao Chen, Boyi Li,
Boris Ivanovic, Marco Pavone, and Yue Wang.

</span>
<span class="ltx_bibblock">Towards efficient and effective multi-camera encoding for end-to-end
driving, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2512.10947" title="">https://arxiv.org/abs/2512.10947</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and
Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">Advances in Neural Information Processing Systems</em>, pages
11809–11822, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2025)</span>
<span class="ltx_bibblock">
Zhenlong Yuan, Jing Tang, Jinguo Luo, Rui Chen, Chengxuan Qian, Lei Sun,
Xiangxiang Chu, Yujun Cai, Dapeng Zhang, and Shuo Li.

</span>
<span class="ltx_bibblock">AutoDrive-R<sup class="ltx_sup" id="bib.bib105.2.1">2</sup>: Incentivizing reasoning and self-reflection
capacity for VLA model in autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">arXiv preprint arXiv:2509.01944</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2025)</span>
<span class="ltx_bibblock">
Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan,
Mu Xu, and Xing Wei.

</span>
<span class="ltx_bibblock">FutureSightDrive: Thinking visually with spatio-temporal cot for
autonomous driving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:2505.17685</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">IEEE International Conference on Computer Vision</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2025)</span>
<span class="ltx_bibblock">
Jiahui Zhang, Yusen Luo, Abrar Anwar, Sumedh Anand Sontakke, Joseph J Lim,
Jesse Thomason, Erdem Biyik, and Jesse Zhang.

</span>
<span class="ltx_bibblock">ReWiND: Language-guided rewards teach robot policies without new
demonstrations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Conference on Robot Learning</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2023)</span>
<span class="ltx_bibblock">
Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che,
Baishakhi Ray, and Marco Pavone.

</span>
<span class="ltx_bibblock">Guided conditional diffusion for controllable traffic simulation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">IEEE International Conference on Robotics and Automation</em>,
pages 3560–3566, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2025a)</span>
<span class="ltx_bibblock">
Xingcheng Zhou, Xuyuan Han, Feng Yang, Yunpu Ma, and Alois C Knoll.

</span>
<span class="ltx_bibblock">OpenDriveVLA: Towards end-to-end autonomous driving with large
vision language action model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2503.23463</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2025b)</span>
<span class="ltx_bibblock">
Zewei Zhou, Tianhui Cai, Seth Z Zhao, Yun Zhang, Zhiyu Huang, Bolei Zhou, and
Jiaqi Ma.

</span>
<span class="ltx_bibblock">AutoVLA: A vision-language-action model for end-to-end autonomous
driving with adaptive reasoning and reinforcement fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">arXiv preprint arXiv:2506.13757</em>, 2025b.

</span>
</li>
</ul>
</section><div about="" class="ltx_rdf" content="{}" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="{Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail}" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jan  7 09:04:59 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
